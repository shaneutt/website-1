{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A Rust client for Kubernetes in the style of a more generic client-go , a runtime abstraction inspired by controller-runtime , and a derive macro for CRDs inspired by kubebuilder . Hosted by CNCF as a Sandbox Project These crates build upon Kubernetes apimachinery + api concepts to enable generic abstractions. These abstractions allow Rust reinterpretations of reflectors, controllers, and custom resource interfaces, so that you can write applications easily. Getting Started Community Crates Github","title":"Home"},{"location":"adopters/","text":"Adopters # Open Source # linkerd-policy-controller - the policy controllers for the Linkerd service mesh krustlet - a complete WASM running kubelet stackable operators - ( kafka , zookeeper , and more) bottlerocket-update-operator vector kdash tui - terminal dashboard for kubernetes logdna agent kubeapps pinniped kubectl-view-allocations - kubectl plugin to list resource allocations krator - kubernetes operators using state machines hahaha - an operator that cleans up sidecars after Jobs kubectl-watch - a kubectl plugin to provide a pretty delta change view of being watched kubernetes resources Companies # AWS Buoyant Deis Labs Stackable Datadog logdna Bitnami Materialize Qualified TrueLayer ViacomCBS nais If you're using kube-rs in production and are not on this list, please submit a pull request ! Reverse Dependencies # Open source users of kube are additionally viewable through reverse dependency listings on both github and crates.io (for published resources). These will contain a more comprehensive/up-to-date list of adopters, with the caveat that some of these can be more experimental.","title":"Adopters"},{"location":"adopters/#adopters","text":"","title":"Adopters"},{"location":"adopters/#open-source","text":"linkerd-policy-controller - the policy controllers for the Linkerd service mesh krustlet - a complete WASM running kubelet stackable operators - ( kafka , zookeeper , and more) bottlerocket-update-operator vector kdash tui - terminal dashboard for kubernetes logdna agent kubeapps pinniped kubectl-view-allocations - kubectl plugin to list resource allocations krator - kubernetes operators using state machines hahaha - an operator that cleans up sidecars after Jobs kubectl-watch - a kubectl plugin to provide a pretty delta change view of being watched kubernetes resources","title":"Open Source"},{"location":"adopters/#companies","text":"AWS Buoyant Deis Labs Stackable Datadog logdna Bitnami Materialize Qualified TrueLayer ViacomCBS nais If you're using kube-rs in production and are not on this list, please submit a pull request !","title":"Companies"},{"location":"adopters/#reverse-dependencies","text":"Open source users of kube are additionally viewable through reverse dependency listings on both github and crates.io (for published resources). These will contain a more comprehensive/up-to-date list of adopters, with the caveat that some of these can be more experimental.","title":"Reverse Dependencies"},{"location":"architecture/","text":"Architecture # This document describes the high-level architecture of kube-rs. This is intended for contributors or people interested in architecture. Overview # The kube-rs repository contains 5 main crates, examples and tests. The main crate that users generally import is kube , and it's a straight facade crate that re-exports from the four other crates: kube_core -> re-exported as core kube_client -> re-exported as api + client + config + discovery kube_derive -> re-exported as CustomResource kube_runtime -> re-exported as runtime In terms of dependencies between these 4: kube_core is used by kube_runtime , kube_derive and kube_client kube_client is used by kube_runtime kube_runtime is the highest level abstraction The extra indirection crate kube is there to avoid cyclic dependencies between the client and the runtime (if the client re-exported the runtime then the two crates would be cyclically dependent). NB : We refer to these crates by their crates.io name using underscores for separators, but the folders have dashes as separators. When working on features/issues with kube-rs you will generally work inside one of these crates at a time, so we will focus on these in isolation, but talk about possible overlaps at the end. Kubernetes Ecosystem Considerations # The Rust ecosystem does not exist in a vaccum as we take heavy inspirations from the popular Go ecosystem. In particular: core module contains invariants from apimachinery that is preseved across individual apis client::Client is a re-envisioning of a generic client-go runtime::Controller abstraction follows conventions in controller-runtime derive::CustomResource derive macro for CRDs is loosely inspired by kubebuilder's annotations We do occasionally diverge on matters where following the go side is worse for the rust language, but when it comes to choosing names and finding out where some modules / functionality should reside; a precedent in client-go , apimachinery , controller-runtime and kubebuilder goes a long way. Generated Structs # We do not maintain the kubernetes types generated from the swagger.json or the protos at present moment, and we do not handle client-side validation of fields relating to these types (that's left to the api-server). We generally use k8s-openapi's Rust bindings for Kubernetes' builtin types types, see: github.com:k8s-openapi docs.rs:k8s-openapi We also maintain an experimental set of Protobuf bindings, see k8s-pb . Crate Overviews # kube-core # This crate only contains types relevant to the Kubernetes API , abstractions analogous to what you'll find inside apimachinery , and extra Rust traits that help us with generics further down in kube-client . Starting out with the basic type modules first: metadata : the various metadata types; ObjectMeta , ListMeta , TypeMeta request + response + subresource : a sans-IO style http interface for the API watch : a generic enum and behaviour for the watch api params : generic parameters passed to sans-IO request interface ( ListParams etc, called ListOptions in apimachinery) Then there are traits crd : a versioned CustomResourceExt trait for kube-derive object generic conveniences for iterating over typed lists of objects, and objects following spec/status conventions resource : a Resource trait for kube-client 's Api + a convenience ResourceExt trait for users The most important export here is the Resource trait and its impls. It is a pretty complex trait, with an associated type called DynamicType (that is default empty). Every ObjectMeta -using type that comes from k8s-openapi gets a blanket impl of Resource so we can use them generically (in kube_client::Api ). Finally, there are two modules used by the higher level discovery module (in kube-client ) and they have similar counterparts in apimachinery/restmapper + apimachinery/group_version : discovery : types returned by the discovery api; capabilities, verbs, scopes, key info gvk : partial type information to infer api types The main type here from these two modules is ApiResource because it can also be used to construct a kube_client::Api instance without compile-time type information (both DynamicObject and Object has Resource impls where DynamicType = ApiResource ). kube-client # config # Contains logic for determining the runtime environment (local kubeconfigs or in-cluster ) so that we can construct our Config from either source. Config is the source-agnostic type (with all the information needed by our Client ) Kubeconfig is for loading from ~/.kube/config or from any number of kubeconfig like files set by KUBECONFIG evar. Config::from_cluster_env reads environment variables that are injected when running inside a pod In general this module has similar functionality to the upstream client-go/clientcmd module. client # The Client is one of the most complicated parts of kube-rs , because it has the most generic interface. People can mock the Client , people can replace individual components and force inject headers, people can choose their own tls stack, and - in theory - use whatever http clients they want. Generally, the Client is created from the properties of a Config to create a particular hyper::Client with a pre-configured amount of tower::Layer s (see TryFrom<Config> for Client ), but users can also pass in an arbitrary tower::Service (to fully customise or to mock). The signature restrictions on Client::new is commensurately large. The tls module contains the openssl or rustls interfaces to let users pick their tls stacks. The connectors created in that module is passed to hyper::Client based on feature selection. The Client can be created from a particular type of using the properties in the Config to configure its layers. Some of our layers come straight from tower-http : tower_http::DecompressionLayer to deal with gzip compression tower_http::TraceLayer to propagate http request information onto tracing spans. tower_http::AddAuthorizationLayer to set bearer tokens / basic auth (when needed) but we also have our own layers in the middleware module: BaseUriLayer prefixes Config::base_url to requests AuthLayer configures either AddAuthorizationLayer or AsyncFilterLayer<RefreshableToken> depending on authentication method in the kubeconfig. AsyncFilterLayer<RefreshableToken> is like AddAuthorizationLayer , but with a token that's refreshed when necessary. (The middleware module is kept small to avoid mixing the business logic ( client::auth openid connect oauth provider logic) with the tower layering glue.) The exported layers and tls connectors are mainly exposed through the config_ext module's ConfigExt trait which is only implemented by Config (because the config has all the properties needed for this in general, and it helps minimise our api surface). Finally, the Client manages other key aspects of IO the protocol such as: Client::connect performs an HTTP Upgrade for specialised verbs Client::request handles 90% of all requests Client::request_events handles streaming watch eventss using tokio_utils 's FramedRead codec Client::request_status handles Either<T, Status> responses from kubernetes api # The generic Api type and its methods. Builds on top of the Request / Response interface in kube_core by parametrising over a generic type K that implement Resource (plus whatever else is needed). The Api absorbs a Client on construction and is then configured with its Scope (through its ::namespaced / ::default_namespaced or ::all constructors). For dynamic types ( Object and DynamicObject ) it has slightly more complicated constructors which have the _with suffix. The core_methods and most subresource methods generally follow this recipe: create Request store the kubernetes verb in the [ http::Extensions ] object call the request with the Client and tell it what type(s) to deserialize into Some subresource methods (behind the ws feature) use the remote_command module's AttachedProcess interface expecting a duplex stream to deal with specialised websocket verbs ( exec and attach ) and is calling Client::connect first to get that stream. discovery # Deals with dynamic discovery of what apis are available on the api-server. Normally this can be used to discover custom resources, but also certain standard resources that vary between providers. The Discovery client can be used to do a full recursive sweep of api-groups into all api resources (through filter / exclude -> run ) and then the users can periodically re- run to keep the cache up to date (as kubernetes is being upgraded behind the scenes). The discovery module also contains a way to run smaller queries through the oneshot module; e.g. resolving resource name when having group version kind, resolving every resource within one specific group, or even one group at a pinned version. The equivalent Go logic is found in client-go/discovery kube-derive # The smallest crate. A simple derive proc_macro to generate Kubernetes wrapper structs and trait impls around a data struct. Uses darling to parse #[kube(attrs...)] then uses syn and quote to produce a suitable syntax tree based on the attributes requested. It ultimately contains a lot of ugly json coercing from attributes into serialization code, but this is code that everyone working with custom resources need. It has hooks into schemars when using JsonSchema to ensure the correct type of CRD schema is attached to the right part of the generated custom resource definition. kube-runtime # The highest level crate that deals with the highest level abstractions (such as controllers/watchers/reflectors) and specific Kubernetes apis that need common care (finalisers, waiting for conditions, event publishing). watcher # The watcher module contains state machine wrappers around Api::watch that will watch and auto-recover on allowable failures. The watcher fn is the general purpose one that is similar to informers in Go land, and will watch a collection of objects. The watch_object is a specialised version of this that watches a single object. reflector # The reflector module contains wrappers around watcher that will cache objects in memory. The reflector fn wraps a watcher and a state Store that is updated on every event emitted by the watcher . The reason for the difference between watcher::Event (created by watcher ) and kube::api::WatchEvent (created by Api::watch ) is that watcher will deals with desync errors and do a full relist whose result is then propagated as a single event, ensuring the reflector can do a single, atomic update to its state Store . controller # The controller module contains the Controller type and its associated definitions. The Controller is configured to watch one root object (configured via ::new ), and several owned objects (via ::owns ), and - once ::run - it will hit a users reconcile function for every change to the root object or any of its child objects (and internally it will traverse up the object tree - usually through owner references - to find the affected root object). The user is then meant to provide an idempotent reconcile fn, that does not know what underlying object was changed, to ensure the state configured in its crd, is what can be seen in the world. To manage this, a vector of watchers is converted into a set of streams of the same type by mapping the watchers so they have the same output type. This is why watches and owns differ: owns looks up OwnerReferences , but watches need you to define the relation yourself with a mapper . The mappers we support are trigger_owners , trigger_self , and the custom trigger_with . Once we have combined the stream of streams we essentially have a flattened super stream with events from multiple watchers that will act as our input events. With this, the applier can start running its fairly complex machinery: new input events get sent to the scheduler scheduled events are then passed them through a Runner preventing duplicate parallel requests for the same object when running, we send the affected object to the users reconciler fn and await that future a) on success, prepare the users Action (generally a slow requeue several minutes from now) b) on failure, prepare a Action based on the users error policy (generally a backoff'd requeue with shorter initial delay) Map resulting Action s through an ad-hoc scheduler channel Resulting requeue requests through the channel are picked up at the top of applier and merged with input events in step 1. Ideally, the process runs forever, and it minimises unnecessary reconcile calls (like users changing more than one related object while one reconcile is already happening). See controller internals for some more information on this. finalizer # Contains a helper wrapper finalizer for a reconcile fn used by a Controller when a user is using finalizers to handle garbage collection. This lets the user focus on simply selecting the type of behaviour they would like to exhibit based on whether the object is being deleted or it's just being regularly reconciled (through enum matching on finalizer::Event ). This lets the user elide checking for potential deletion timestamps and manage the state machinery of metadata.finalizers through jsonpatching. wait # Contains helpers for waiting for conditions , or objects to be fully removed (i.e. waiting for finalizers post delete). These build upon watch_object with specific mappers. events # Contains an event Recorder ala client-go/events that controllers can hook into, to publish events related to their reconciliations. Crate Delineation and Overlaps # When working on the the client machinery, it's important to realise that there are effectively 5 layers involved: Sans-IO request builder (in kube_core::Request ) IO (in kube_client::Client ) Typing (in kube_client::Api ) Helpers for using the API correctly (e.g. kube_runtime::watcher ) High-level abstractions for specific tasks (e.g. kube_runtime::controller ) At level 3, we essentially have what the K8s team calls a basic client. As a consequence, new methods/subresources typically cross 2 crate boundaries ( kube_core , kube_client ), and needs to touch 3 main modules. Similarly, there are also the traits and types that define what an api means in kube_core like Resource and ApiResource . If modifying these, then changes to kube-derive are likely necessary, as it needs to directly implement this for users. These types of cross-crate dependencies are why we expose kube as a single versioned facade crate that users can upgrade atomically (without being caught in the middle of a publish cycle). This also gives us better compatibility with dependabot .","title":"Architecture"},{"location":"architecture/#architecture","text":"This document describes the high-level architecture of kube-rs. This is intended for contributors or people interested in architecture.","title":"Architecture"},{"location":"architecture/#overview","text":"The kube-rs repository contains 5 main crates, examples and tests. The main crate that users generally import is kube , and it's a straight facade crate that re-exports from the four other crates: kube_core -> re-exported as core kube_client -> re-exported as api + client + config + discovery kube_derive -> re-exported as CustomResource kube_runtime -> re-exported as runtime In terms of dependencies between these 4: kube_core is used by kube_runtime , kube_derive and kube_client kube_client is used by kube_runtime kube_runtime is the highest level abstraction The extra indirection crate kube is there to avoid cyclic dependencies between the client and the runtime (if the client re-exported the runtime then the two crates would be cyclically dependent). NB : We refer to these crates by their crates.io name using underscores for separators, but the folders have dashes as separators. When working on features/issues with kube-rs you will generally work inside one of these crates at a time, so we will focus on these in isolation, but talk about possible overlaps at the end.","title":"Overview"},{"location":"architecture/#kubernetes-ecosystem-considerations","text":"The Rust ecosystem does not exist in a vaccum as we take heavy inspirations from the popular Go ecosystem. In particular: core module contains invariants from apimachinery that is preseved across individual apis client::Client is a re-envisioning of a generic client-go runtime::Controller abstraction follows conventions in controller-runtime derive::CustomResource derive macro for CRDs is loosely inspired by kubebuilder's annotations We do occasionally diverge on matters where following the go side is worse for the rust language, but when it comes to choosing names and finding out where some modules / functionality should reside; a precedent in client-go , apimachinery , controller-runtime and kubebuilder goes a long way.","title":"Kubernetes Ecosystem Considerations"},{"location":"architecture/#generated-structs","text":"We do not maintain the kubernetes types generated from the swagger.json or the protos at present moment, and we do not handle client-side validation of fields relating to these types (that's left to the api-server). We generally use k8s-openapi's Rust bindings for Kubernetes' builtin types types, see: github.com:k8s-openapi docs.rs:k8s-openapi We also maintain an experimental set of Protobuf bindings, see k8s-pb .","title":"Generated Structs"},{"location":"architecture/#crate-overviews","text":"","title":"Crate Overviews"},{"location":"architecture/#kube-core","text":"This crate only contains types relevant to the Kubernetes API , abstractions analogous to what you'll find inside apimachinery , and extra Rust traits that help us with generics further down in kube-client . Starting out with the basic type modules first: metadata : the various metadata types; ObjectMeta , ListMeta , TypeMeta request + response + subresource : a sans-IO style http interface for the API watch : a generic enum and behaviour for the watch api params : generic parameters passed to sans-IO request interface ( ListParams etc, called ListOptions in apimachinery) Then there are traits crd : a versioned CustomResourceExt trait for kube-derive object generic conveniences for iterating over typed lists of objects, and objects following spec/status conventions resource : a Resource trait for kube-client 's Api + a convenience ResourceExt trait for users The most important export here is the Resource trait and its impls. It is a pretty complex trait, with an associated type called DynamicType (that is default empty). Every ObjectMeta -using type that comes from k8s-openapi gets a blanket impl of Resource so we can use them generically (in kube_client::Api ). Finally, there are two modules used by the higher level discovery module (in kube-client ) and they have similar counterparts in apimachinery/restmapper + apimachinery/group_version : discovery : types returned by the discovery api; capabilities, verbs, scopes, key info gvk : partial type information to infer api types The main type here from these two modules is ApiResource because it can also be used to construct a kube_client::Api instance without compile-time type information (both DynamicObject and Object has Resource impls where DynamicType = ApiResource ).","title":"kube-core"},{"location":"architecture/#kube-client","text":"","title":"kube-client"},{"location":"architecture/#config","text":"Contains logic for determining the runtime environment (local kubeconfigs or in-cluster ) so that we can construct our Config from either source. Config is the source-agnostic type (with all the information needed by our Client ) Kubeconfig is for loading from ~/.kube/config or from any number of kubeconfig like files set by KUBECONFIG evar. Config::from_cluster_env reads environment variables that are injected when running inside a pod In general this module has similar functionality to the upstream client-go/clientcmd module.","title":"config"},{"location":"architecture/#client","text":"The Client is one of the most complicated parts of kube-rs , because it has the most generic interface. People can mock the Client , people can replace individual components and force inject headers, people can choose their own tls stack, and - in theory - use whatever http clients they want. Generally, the Client is created from the properties of a Config to create a particular hyper::Client with a pre-configured amount of tower::Layer s (see TryFrom<Config> for Client ), but users can also pass in an arbitrary tower::Service (to fully customise or to mock). The signature restrictions on Client::new is commensurately large. The tls module contains the openssl or rustls interfaces to let users pick their tls stacks. The connectors created in that module is passed to hyper::Client based on feature selection. The Client can be created from a particular type of using the properties in the Config to configure its layers. Some of our layers come straight from tower-http : tower_http::DecompressionLayer to deal with gzip compression tower_http::TraceLayer to propagate http request information onto tracing spans. tower_http::AddAuthorizationLayer to set bearer tokens / basic auth (when needed) but we also have our own layers in the middleware module: BaseUriLayer prefixes Config::base_url to requests AuthLayer configures either AddAuthorizationLayer or AsyncFilterLayer<RefreshableToken> depending on authentication method in the kubeconfig. AsyncFilterLayer<RefreshableToken> is like AddAuthorizationLayer , but with a token that's refreshed when necessary. (The middleware module is kept small to avoid mixing the business logic ( client::auth openid connect oauth provider logic) with the tower layering glue.) The exported layers and tls connectors are mainly exposed through the config_ext module's ConfigExt trait which is only implemented by Config (because the config has all the properties needed for this in general, and it helps minimise our api surface). Finally, the Client manages other key aspects of IO the protocol such as: Client::connect performs an HTTP Upgrade for specialised verbs Client::request handles 90% of all requests Client::request_events handles streaming watch eventss using tokio_utils 's FramedRead codec Client::request_status handles Either<T, Status> responses from kubernetes","title":"client"},{"location":"architecture/#api","text":"The generic Api type and its methods. Builds on top of the Request / Response interface in kube_core by parametrising over a generic type K that implement Resource (plus whatever else is needed). The Api absorbs a Client on construction and is then configured with its Scope (through its ::namespaced / ::default_namespaced or ::all constructors). For dynamic types ( Object and DynamicObject ) it has slightly more complicated constructors which have the _with suffix. The core_methods and most subresource methods generally follow this recipe: create Request store the kubernetes verb in the [ http::Extensions ] object call the request with the Client and tell it what type(s) to deserialize into Some subresource methods (behind the ws feature) use the remote_command module's AttachedProcess interface expecting a duplex stream to deal with specialised websocket verbs ( exec and attach ) and is calling Client::connect first to get that stream.","title":"api"},{"location":"architecture/#discovery","text":"Deals with dynamic discovery of what apis are available on the api-server. Normally this can be used to discover custom resources, but also certain standard resources that vary between providers. The Discovery client can be used to do a full recursive sweep of api-groups into all api resources (through filter / exclude -> run ) and then the users can periodically re- run to keep the cache up to date (as kubernetes is being upgraded behind the scenes). The discovery module also contains a way to run smaller queries through the oneshot module; e.g. resolving resource name when having group version kind, resolving every resource within one specific group, or even one group at a pinned version. The equivalent Go logic is found in client-go/discovery","title":"discovery"},{"location":"architecture/#kube-derive","text":"The smallest crate. A simple derive proc_macro to generate Kubernetes wrapper structs and trait impls around a data struct. Uses darling to parse #[kube(attrs...)] then uses syn and quote to produce a suitable syntax tree based on the attributes requested. It ultimately contains a lot of ugly json coercing from attributes into serialization code, but this is code that everyone working with custom resources need. It has hooks into schemars when using JsonSchema to ensure the correct type of CRD schema is attached to the right part of the generated custom resource definition.","title":"kube-derive"},{"location":"architecture/#kube-runtime","text":"The highest level crate that deals with the highest level abstractions (such as controllers/watchers/reflectors) and specific Kubernetes apis that need common care (finalisers, waiting for conditions, event publishing).","title":"kube-runtime"},{"location":"architecture/#watcher","text":"The watcher module contains state machine wrappers around Api::watch that will watch and auto-recover on allowable failures. The watcher fn is the general purpose one that is similar to informers in Go land, and will watch a collection of objects. The watch_object is a specialised version of this that watches a single object.","title":"watcher"},{"location":"architecture/#reflector","text":"The reflector module contains wrappers around watcher that will cache objects in memory. The reflector fn wraps a watcher and a state Store that is updated on every event emitted by the watcher . The reason for the difference between watcher::Event (created by watcher ) and kube::api::WatchEvent (created by Api::watch ) is that watcher will deals with desync errors and do a full relist whose result is then propagated as a single event, ensuring the reflector can do a single, atomic update to its state Store .","title":"reflector"},{"location":"architecture/#controller","text":"The controller module contains the Controller type and its associated definitions. The Controller is configured to watch one root object (configured via ::new ), and several owned objects (via ::owns ), and - once ::run - it will hit a users reconcile function for every change to the root object or any of its child objects (and internally it will traverse up the object tree - usually through owner references - to find the affected root object). The user is then meant to provide an idempotent reconcile fn, that does not know what underlying object was changed, to ensure the state configured in its crd, is what can be seen in the world. To manage this, a vector of watchers is converted into a set of streams of the same type by mapping the watchers so they have the same output type. This is why watches and owns differ: owns looks up OwnerReferences , but watches need you to define the relation yourself with a mapper . The mappers we support are trigger_owners , trigger_self , and the custom trigger_with . Once we have combined the stream of streams we essentially have a flattened super stream with events from multiple watchers that will act as our input events. With this, the applier can start running its fairly complex machinery: new input events get sent to the scheduler scheduled events are then passed them through a Runner preventing duplicate parallel requests for the same object when running, we send the affected object to the users reconciler fn and await that future a) on success, prepare the users Action (generally a slow requeue several minutes from now) b) on failure, prepare a Action based on the users error policy (generally a backoff'd requeue with shorter initial delay) Map resulting Action s through an ad-hoc scheduler channel Resulting requeue requests through the channel are picked up at the top of applier and merged with input events in step 1. Ideally, the process runs forever, and it minimises unnecessary reconcile calls (like users changing more than one related object while one reconcile is already happening). See controller internals for some more information on this.","title":"controller"},{"location":"architecture/#finalizer","text":"Contains a helper wrapper finalizer for a reconcile fn used by a Controller when a user is using finalizers to handle garbage collection. This lets the user focus on simply selecting the type of behaviour they would like to exhibit based on whether the object is being deleted or it's just being regularly reconciled (through enum matching on finalizer::Event ). This lets the user elide checking for potential deletion timestamps and manage the state machinery of metadata.finalizers through jsonpatching.","title":"finalizer"},{"location":"architecture/#wait","text":"Contains helpers for waiting for conditions , or objects to be fully removed (i.e. waiting for finalizers post delete). These build upon watch_object with specific mappers.","title":"wait"},{"location":"architecture/#events","text":"Contains an event Recorder ala client-go/events that controllers can hook into, to publish events related to their reconciliations.","title":"events"},{"location":"architecture/#crate-delineation-and-overlaps","text":"When working on the the client machinery, it's important to realise that there are effectively 5 layers involved: Sans-IO request builder (in kube_core::Request ) IO (in kube_client::Client ) Typing (in kube_client::Api ) Helpers for using the API correctly (e.g. kube_runtime::watcher ) High-level abstractions for specific tasks (e.g. kube_runtime::controller ) At level 3, we essentially have what the K8s team calls a basic client. As a consequence, new methods/subresources typically cross 2 crate boundaries ( kube_core , kube_client ), and needs to touch 3 main modules. Similarly, there are also the traits and types that define what an api means in kube_core like Resource and ApiResource . If modifying these, then changes to kube-derive are likely necessary, as it needs to directly implement this for users. These types of cross-crate dependencies are why we expose kube as a single versioned facade crate that users can upgrade atomically (without being caught in the middle of a publish cycle). This also gives us better compatibility with dependabot .","title":"Crate Delineation and Overlaps"},{"location":"changelog/","text":"Changelog # Unreleased # see https://github.com/kube-rs/kube/compare/0.76.0...main 0.76.0 / 2022-10-28 # Highlights # #[derive(CustomResource)] now supports schemas with untagged enums # Expanding on our existing support for storing Rust's struct enums in CRDs, Kube will now try to convert #[serde(untagged)] enums as well. Note that if the same field is present in multiple untagged variants then they must all have the same shape. Removed deprecated try_flatten_* functions # These have been deprecated since 0.72, and are replaced by the equivalent WatchStreamExt methods. What's Changed # Added # Adds example to Controller::watches by @Dav1dde in https://github.com/kube-rs/kube/pull/1026 Discovery: Add ApiGroup::resources_by_stability by @imuxin in https://github.com/kube-rs/kube/pull/1022 Add support for untagged enums in CRDs by @sbernauer in https://github.com/kube-rs/kube/pull/1028 Derive PartialEq for DynamicObject by @pbzweihander in https://github.com/kube-rs/kube/pull/1048 Removed # Runtime: Remove deprecated util try_flatten_ helpers by @clux in https://github.com/kube-rs/kube/pull/1019 Remove native-tls feature by @kazk in https://github.com/kube-rs/kube/pull/1044 Fixed # add fieldManager querystring to all operations by @goenning in https://github.com/kube-rs/kube/pull/1031 Add verify_tls1x_signature for NoCertVerification by @rvql in https://github.com/kube-rs/kube/pull/1034 Fix compatibility with schemars' preserve_order feature by @teozkr in https://github.com/kube-rs/kube/pull/1050 Hoist enum values from subschemas by @teozkr in https://github.com/kube-rs/kube/pull/1051 0.75.0 / 2022-09-21 # Highlights # Upgrade k8s-openapi to 0.16 for Kubernetes 1.25 # The update to k8s-openapi@0.16.0 makes this the first release with tentative Kubernetes 1.25 support. While the new structs and apis now exist, we recommend holding off on using 1.25 until a deserialization bug in the apiserver is resolved upstream. See #997 / #1008 for details. To upgrade, ensure you bump both kube and k8s-openapi : cargo upgrade kube k8s-openapi New/Old Config::incluster default to connect in cluster # Our previous default of connecting to the Kubernetes apiserver via kubernetes.default.svc has been reverted back to use the old environment variables after Kubernetes updated their position that the environment variables are not legacy. This does unfortunately regress on rustls support, so for those users we have included a Config::incluster_dns to work around the old rustls issue while it is open. Controller error_policy extension # The error_policy fn now has access to the object that failed the reconciliation to ease metric creation / failure attribution. The following change is needed on the user side: -fn error_policy(error: &Error, ctx: Arc<Data>) -> Action { +fn error_policy(_obj: Arc<YourObject>, error: &Error, ctx: Arc<Data>) -> Action { Polish / Subresources / Conversion # There are also a slew of ergonomics improvements, closing of gaps in subresources , adding initial support for ConversionReview , making Api::namespaced impossible to use for non-namepaced resources (a common pitfall), as well as many great fixes to the edge cases in portforwarding and finalizers . Many of these changes came from first time contributors. A huge thank you to everyone involved. What's Changed # Added # Make Config::auth_info public by @danrspencer in https://github.com/kube-rs/kube/pull/959 Make raw Client::send method public by @tiagolobocastro in https://github.com/kube-rs/kube/pull/972 Make types on AdmissionRequest and AdmissionResponse public by @clux in https://github.com/kube-rs/kube/pull/977 Add #[serde(default)] to metadata field of DynamicObject by @pbzweihander in https://github.com/kube-rs/kube/pull/987 Add create_subresource method to Api and create_token_request method to Api<ServiceAccount> by @pbzweihander in https://github.com/kube-rs/kube/pull/989 Controller: impl Eq and PartialEq for Action by @Sherlock-Holo in https://github.com/kube-rs/kube/pull/993 Add support for CRD ConversionReview types by @MikailBag in https://github.com/kube-rs/kube/pull/999 Changed # Constrain Resource trait and Api::namespaced by Scope by @clux in https://github.com/kube-rs/kube/pull/956 Add connect/read/write timeouts to Config by @goenning in https://github.com/kube-rs/kube/pull/971 Controller: Include the object being reconciled in the error_policy by @felipesere in https://github.com/kube-rs/kube/pull/995 Config : New incluster and incluster_dns constructors by @olix0r in https://github.com/kube-rs/kube/pull/1001 Upgrade k8s-openapi to 0.16 by @clux in https://github.com/kube-rs/kube/pull/1008 Fixed # Remove tracing::instrument from apply_debug_overrides by @kazk in https://github.com/kube-rs/kube/pull/958 fix duplicate finalizers race condition by @alex-hunt-materialize in https://github.com/kube-rs/kube/pull/965 fix: portforward connection cleanup by @tiagolobocastro in https://github.com/kube-rs/kube/pull/973 0.74.0 / 2022-07-09 # Highlights # Polish, bug fixes, guidelines, ci improvements, and new contributors # This release features smaller improvements/additions/cleanups/fixes, many of which are from new first-time contributors! Thank you everyone! The listed deadlock fix was backported to 0.73.1. We have also been trying to clarify and prove a lot more of our external-facing guarantees, and as a result: We have codified our Kubernetes versioning policy The Rust version policy has extended its support range Our CI has been extended ResourceExt::name deprecation # A consequence of all the policy writing and the improved clarity we have decided to deprecate the common ResourceExt::name helper. This method could panic and it is unexpected for the users and bad for our consistency. To get the old functionality, you can replace any .name() call on a Kubernetes resources with .name_unchecked() ; but as the name implies, it can panic (in a local setting, or during admission). We recommend you replace it with the new ResourceExt::name_any for a general identifier: -pod.name() +pod.name_any() What's Changed # Added # Add support for passing the fieldValidation query parameter on patch by @phroggyy in https://github.com/kube-rs/kube/pull/929 Add conditions::is_job_completed by @clux in https://github.com/kube-rs/kube/pull/935 Changed # Deprecate ResourceExt::name in favour of safe name_* alternatives by @clux in https://github.com/kube-rs/kube/pull/945 Removed # Remove #[kube(apiextensions)] flag from kube-derive by @clux in https://github.com/kube-rs/kube/pull/920 Fixed # Document every public derived fn from kube-derive by @clux in https://github.com/kube-rs/kube/pull/919 fix applier hangs which can happen with many watched objects by @moustafab in https://github.com/kube-rs/kube/pull/925 Applier: Improve reconciler reschedule context to avoid deadlocking on full channel by @teozkr in https://github.com/kube-rs/kube/pull/932 Fix deserialization issue in AdmissionResponse by @clux in https://github.com/kube-rs/kube/pull/939 Admission controller example fixes by @Alibirb in https://github.com/kube-rs/kube/pull/950 0.73.1 / 2022-06-03 # Highlights # This patch release fixes a bug causing applier and Controller to deadlock when too many Kubernetes object change events were ingested at once. All users of applier and Controller are encouraged to upgrade as quickly as possible. Older versions are also affected, this bug is believed to have existed since the original release of kube_runtime . What's Changed # Fixed # [0.73 backport] fix applier hangs which can happen with many watched objects (#925) by @moustafab (backported by @teozkr) in https://github.com/kube-rs/kube/pull/927 Full Changelog : https://github.com/kube-rs/kube/compare/0.73.0...0.73.1 0.73.0 / 2022-05-23 # Highlights # New k8s-openapi version and MSRV # Support added for Kubernetes v1_24 support via the new k8s-openapi version . Please also run cargo upgrade --workspace k8s-openapi when upgrading kube . This also bumps our MSRV to 1.60.0 . Reconciler change # A small ergonomic change in the reconcile signature has removed the need for the Context object. This has been replaced by an Arc . The following change is needed in your controller: -async fn reconcile(doc: Arc<MyObject>, context: Context<Data>) -> Result<Action, Error> +async fn reconcile(doc: Arc<MyObject>, context: Arc<Data>) -> Result<Action, Error> This will simplify the usage of the context argument. You should no longer need to pass .get_ref() on its every use. See the controller-rs upgrade change for details . What's Changed # Added # Add Discovery::groups_alphabetical following kubectl sort order by @clux in https://github.com/kube-rs/kube/pull/887 Changed # Replace runtime::controller::Context with Arc by @teozkr in https://github.com/kube-rs/kube/pull/910 runtime: Return the object from await_condition by @olix0r in https://github.com/kube-rs/kube/pull/877 Bump k8s-openapi to 0.15 for kubernetes v1_24 and bump MSRV to 1.60 by @clux in https://github.com/kube-rs/kube/pull/916 0.72.0 / 2022-05-13 # Highlights # Ergonomics improvements # A new runtime::WatchSteamExt ( #899 + #906 ) allows for simpler setups for streams from watcher or reflector . - let stream = utils::try_flatten_applied(StreamBackoff::new(watcher(api, lp), b)); + let stream = watcher(api, lp).backoff(b).applied_objects(); The util::try_flatten_* helpers have been marked as deprecated since they are not used by the stream impls. A new reflector:store() fn allows simpler reflector setups #907 : - let store = reflector::store::Writer::<Node>::default(); - let reader = store.as_reader(); + let (reader, writer) = reflector::store(); Additional conveniences getters/settes to ResourceExt for manged_fields and creation_timestamp #888 + #898 , plus a GroupVersion::with_kind path to a GVK, and a TryFrom<TypeMeta> for GroupVersionKind in #896 . CRD Version Selection # Managing multiple version in CustomResourceDefinitions can be pretty complicated, but we now have helpers and docs on how to tackle it. A new function kube::core::crd::merge_crds have been added (in #889 ) to help push crd schemas generated by kube-derived crds with different #[kube(version)] properties. See the kube-derive#version documentation for details. A new example showcases how one can manage two or more versions of a crd and what the expected truncation outcomes are when moving between versions. Examples # Examples now have moved to tracing for its logging, respects RUST_LOG , and namespace selection via the kubeconfig context. There is also a larger kubectl example showcasing kubectl apply -f yaml as well as kubectl {edit,delete,get,watch} via #885 + #897 . What's Changed # Added # Allow merging multi-version CRDs into a single schema by @clux in https://github.com/kube-rs/kube/pull/889 Add GroupVersion::with_kind and TypeMeta -> GroupVersionKind converters by @clux in https://github.com/kube-rs/kube/pull/896 Add managed_fields accessors to ResourceExt by @clux in https://github.com/kube-rs/kube/pull/898 Add ResourceExt::creation_timestamp by @clux in https://github.com/kube-rs/kube/pull/888 Support lowercase http_proxy & https_proxy evars by @DevineLiu in https://github.com/kube-rs/kube/pull/892 Add a WatchStreamExt trait for stream chaining by @clux in https://github.com/kube-rs/kube/pull/899 Add Event::modify + reflector::store helpers by @clux in https://github.com/kube-rs/kube/pull/907 Changed # Switch to kubernetes cluster dns for incluster url everywhere by @clux in https://github.com/kube-rs/kube/pull/876 Update tower-http requirement from 0.2.0 to 0.3.2 by @dependabot in https://github.com/kube-rs/kube/pull/893 Removed # Remove deprecated legacy crd v1beta1 by @clux in https://github.com/kube-rs/kube/pull/890 0.71.0 / 2022-04-12 # Highlights # Several quality of life changes and improvement this release for port-forwarding, a new ClientBuilder , better handling of kube-derive edge-cases. We highlight some changes here that you should be especially aware of. events::Recorder publishing to kube-system for cluster scoped resources # Publishing events via Recorder for cluster scoped resources (supported since 0.70.0 ) now publish to kube-system rather than default , as all but the newest clusters struggle with publishing events in the default namespace. Default TLS stack set to OpenSSL # The previous native-tls default was there because we used to depend on reqwest , but because we depended on openssl anyway the feature does not make much sense. Changing to openssl-tls also improves the situation on macOS where the Security Framework struggles with PKCS#12 certs from OpenSSL v3. The native-tls feature will still be available in this release in case of issues, but the plan is to decommission it shortly. Of course, we all ideally want to move to rustls, but we are still blocked by #153 . What's Changed # Added # Add ClientBuilder that lets users add custom middleware without full stack replacement by @teozkr in https://github.com/kube-rs/kube/pull/855 Support top-level enums in CRDs by @sbernauer in https://github.com/kube-rs/kube/pull/856 Changed # portforward: Improve API and support background task cancelation by @olix0r in https://github.com/kube-rs/kube/pull/854 Make remote commands cancellable and remove panics by @kazk in https://github.com/kube-rs/kube/pull/861 Change the default TLS to OpenSSL by @kazk in https://github.com/kube-rs/kube/pull/863 change event recorder cluster namespace to kube-system by @clux in https://github.com/kube-rs/kube/pull/871 Fixed # Fix schemas containing both properties and additionalProperties by @jcaesar in https://github.com/kube-rs/kube/pull/845 Make dependency pins between sibling crates stricter by @clux in https://github.com/kube-rs/kube/pull/864 Fix in-cluster kube_host_port generation for IPv6 by @somnusfish in https://github.com/kube-rs/kube/pull/875 0.70.0 / 2022-03-20 # Highlights # Support for EC keys with rustls # This was one of the big blockers for using rustls against clusters like k3d or k3s While not sufficient to fix using those clusters out of the box, it is now possible to use them with a workarodund More ergonomic reconciler # The signature and end the Ok action in reconcile fns has been simplified slightly, and requires the following user updates: -async fn reconcile(obj: Arc<MyObject>, ctx: Context<Data>) -> Result<ReconcilerAction, Error> { - ... - Ok(ReconcilerAction { - requeue_after: Some(Duration::from_secs(300)), - }) +async fn reconcile(obj: Arc<MyObject>, ctx: Context<Data>) -> Result<Action, Error> { + ... + Ok(Action::requeue(Duration::from_secs(300))) The Action import lives in the same place as the old ReconcilerAction . What's Changed # Added # Add support for EC private keys by @farcaller in https://github.com/kube-rs/kube/pull/804 Add helper for creating a controller owner_ref on Resource by @clux in https://github.com/kube-rs/kube/pull/850 Changed # Remove scheduler::Error by @teozkr in https://github.com/kube-rs/kube/pull/827 Bump parking_lot to 0.12, but allow dep duplicates by @clux in https://github.com/kube-rs/kube/pull/836 Update tokio-tungstenite requirement from 0.16.1 to 0.17.1 by @dependabot in https://github.com/kube-rs/kube/pull/841 Let OccupiedEntry::commit take PostParams by @teozkr in https://github.com/kube-rs/kube/pull/842 Change ReconcileAction to Action and add associated ctors by @clux in https://github.com/kube-rs/kube/pull/851 Fixed # Token reloading with RwLock by @kazk in https://github.com/kube-rs/kube/pull/835 Fix event publishing for cluster scoped crds by @zhrebicek in https://github.com/kube-rs/kube/pull/847 Fix invalid CRD when Enum variants have descriptions by @sbernauer in https://github.com/kube-rs/kube/pull/852 0.69.1 / 2022-02-16 # Highlights # This is an emergency patch release fixing a bug in 0.69.0 where a kube::Client would deadlock after running inside a cluster for about a minute (#829). All users of 0.69.0 are encouraged to upgrade immediately. 0.68.x and below are not affected. What's Changed # Fixed # [0.69.x] Fix deadlock in token reloading by @clux (backported by @teozkr) in https://github.com/kube-rs/kube/pull/831 0.69.0 / 2022-02-14 # Highlights # Ergonomic Additions to Api # Two new methods have been added to the client Api this release to reduce the amount of boiler-plate needed for common patterns. Api::entry via 811 - to aid idempotent crud operation flows (following the style of Map::Entry ) Api::get_opt via 809 - to aid dealing with the NotFound type error via a returned Option In-cluster Token reloading # Following a requirement for Kubernetes clients against versions >= 1.22.0 , our bundled AuthLayer will reload tokens every minute when deployed in-cluster. What's Changed # Added # Add conversion for ObjectRef<K> to ObjectReference by @teozkr in https://github.com/kube-rs/kube/pull/815 Add Api::get_opt for better existence handling by @teozkr in https://github.com/kube-rs/kube/pull/809 Entry API by @teozkr in https://github.com/kube-rs/kube/pull/811 Changed # Reload token file at least once a minute by @kazk in https://github.com/kube-rs/kube/pull/768 Prefer kubeconfig over in-cluster config by @teozkr in https://github.com/kube-rs/kube/pull/823 Fixed # Disable CSR utilities on K8s <1.19 by @teozkr in https://github.com/kube-rs/kube/pull/817 0.68.0 / 2022-02-01 # Interface Changes # To reduce the amount of allocation done inside the runtime by reflectors and controllers, the following change via #786 is needed on the signature of your reconcile functions: -async fn reconcile(myobj: MyK, ctx: Context<Data>) -> Result<ReconcilerAction> +async fn reconcile(myobj: Arc<MyK>, ctx: Context<Data>) -> Result<ReconcilerAction> This also affects the finalizer helper . Port-forwarding # As one of the last steps toward gold level client requirements , port-forwarding landed in #446 . There are 3 new examples ( port_forward*.rs ) that showcases how to use this websocket based functionality. What's Changed # Added # Add a VS Code devcontainer configuration by @olix0r in https://github.com/kube-rs/kube/pull/788 Add support for user impersonation by @teozkr in https://github.com/kube-rs/kube/pull/797 Add port forward by @kazk in https://github.com/kube-rs/kube/pull/446 Changed # runtime: Store resources in an Arc by @olix0r in https://github.com/kube-rs/kube/pull/786 Propagate Arc through the finalizer reconciler helper by @teozkr in https://github.com/kube-rs/kube/pull/792 Disable unused default features of chrono crate by @dreamer in https://github.com/kube-rs/kube/pull/801 Fixed # Use absolute path to Result in derives by @teozkr in https://github.com/kube-rs/kube/pull/795 core: add missing reason to Display on Error::Validation in Request by @clux in https://github.com/kube-rs/kube/pull/798 0.67.0 / 2022-01-25 # Changed # runtime: Replace DashMap with a locked AHashMap by @olix0r in https://github.com/kube-rs/kube/pull/785 update k8s-openapi for kubernetes 1.23 support by @clux in https://github.com/kube-rs/kube/pull/789 0.66.0 / 2022-01-15 # Tons of ergonomics improvements, and 3 new contributors. Highlighted first is the 3 most discussed changes: Support for auto-generating schemas for enums in kube-derive # It is now possible to embed complex enums inside structs that use #[derive(CustomResource)] . This has been a highly requested feature since the inception of auto-generated schemas. It does not work for all cases , and has certain ergonomics caveats , but represents a huge step forwards. Note that if you depend on kube-derive directly rather than via kube then you must now add the schema feature to kube-core New StreamBackoff mechanism in kube-runtime # To avoid spamming the apiserver when on certain watch errors cases, it's now possible to stream wrap the watcher to set backoffs. The new default_backoff follows existing client-go conventions of being kind to the apiserver. Initially, this is default-enabled in Controller watches (configurable via Controller::trigger_backoff ) and avoids spam errors when crds are not installed. New version priority parser in kube-core # To aid users picking the most appropriate version of a kind from api discovery or through a CRD , two new sort orders have been exposed on the new kube_core::Version Version::priority implementing kubernetes version priority Version::generation implementing a more traditional; generational sort (highest version) Changes # Merged PRs from github release . Added # Add DeleteParams constructors for easily setting PropagationPolicy by @kate-goldenring in https://github.com/kube-rs/kube/pull/757 Add Serialize to ObjecList and add field-selector and jsonpath example by @ChinYing-Li in https://github.com/kube-rs/kube/pull/760 Implement cordon/uncordon for Node by @ChinYing-Li in https://github.com/kube-rs/kube/pull/762 Export Version priority parser with Ord impls in kube_core by @clux in https://github.com/kube-rs/kube/pull/764 Add Api fns for arbitrary subresources and approval subresource for CertificateSigningRequest by @ChinYing-Li in https://github.com/kube-rs/kube/pull/773 Changed # Add backoff handling for watcher and Controller by @clux in https://github.com/kube-rs/kube/pull/703 Remove crate private identity_pem field from Config by @kazk in https://github.com/kube-rs/kube/pull/771 Use SecretString in AuthInfo to avoid credential leaking by @ChinYing-Li in https://github.com/kube-rs/kube/pull/766 0.65.0 / 2021-12-10 # BREAKING: Removed kube::Error::OpenSslError - #716 BREAKING: Removed kube::Error::SslError - #704 and #716 BREAKING: Added kube::Error::NativeTls(kube::client::NativeTlsError) for errors from Native TLS - #716 BREAKING: Added kube::Error::RustlsTls(kube::client::RustlsTlsError) for errors from Rustls TLS - #704 Modified Kubeconfig parsing - allow empty kubeconfigs as per kubectl - #721 Added Kubeconfig::from_yaml - #718 via #719 Updated rustls to 0.20.1 - #704 BREAKING: Added ObjectRef to the object that failed to be reconciled to kube::runtime::controller::Error::ReconcileFailed - #733 BREAKING: Removed api_version and kind fields from kind structs generated by kube::CustomResource - #739 Updated tokio-tungstenite to 0.16 - #750 Updated tower-http to 0.2.0 - #748 BREAKING: kube-client : replace RefreshTokenLayer with AsyncFilterLayer in AuthLayer - #752 0.64.0 / 2021-11-16 # BREAKING: Replaced feature kube-derive/schema with attribute #[kube(schema)] - #690 If you currently disable default kube-derive default features to avoid automatic schema generation, add #[kube(schema = \"disabled\")] to your spec struct instead BREAKING: Moved CustomResource derive crate overrides into subattribute #[kube(crates(...))] - #690 Replace #[kube(kube_core = .., k8s_openapi = .., schema = .., serde = .., serde_json = ..)] with #[kube(crates(kube_core = .., k8s_openapi = .., schema = .., serde = .., serde_json = ..))] Added openssl-tls feature to use openssl for TLS on all platforms. Note that, even though native-tls uses a platform specific TLS, kube requires openssl on all platforms because native-tls only allows PKCS12 input to load certificates and private key at the moment, and creating PKCS12 requires openssl . - #700 BREAKING: Changed to fail loading configurations with PEM-encoded certificates containing invalid sections instead of ignoring them. Updated pem to 1.0.1. - #702 oauth : Updated tame-oauth to 0.6.0 which supports the same default credentials flow as the Go oauth2 for Google OAuth. In addition to reading the service account information from JSON file specified with GOOGLE_APPLICATION_CREDENTIALS environment variable, Application Default Credentials from gcloud , and obtaining OAuth tokens from local metadata server when running inside GCP are now supported. - #701 Refining Errors # We started working on improving error ergonomics. See the tracking issue #688 for more details. The following is the summary of changes to kube::Error included in this release: Added Error::Auth(kube::client::AuthError) (errors related to client auth, some of them were previously in Error::Kubeconfig ) Added Error::BuildRequest(kube::core::request::Error) (errors building request from kube::core ) Added Error::InferConfig(kube::config::InferConfigError) (for Client::try_default ) Added Error::OpensslTls(kube::client::OpensslTlsError) (new openssl-tls feature) - #700 Added Error::UpgradeConnection(kube::client::UpgradeConnectinError) ( ws feature, errors from upgrading a connection) Removed Error::Connection (was unused) Removed Error::RequestBuild (was unused) Removed Error::RequestSend (was unused) Removed Error::RequestParse (was unused) Removed Error::InvalidUri (replaced by variants of errors in kube::config errors) Removed Error::RequestValidation (replaced by a variant of Error::BuildRequest ) Removed Error::Kubeconfig (replaced by Error::InferConfig , and Error::Auth ) Removed Error::ProtocolSwitch ( ws only, replaced by Error::UpgradeConnection ) Removed Error::MissingUpgradeWebSocketHeader ( ws only, replaced by Error::UpgradeConnection ) Removed Error::MissingConnectionUpgradeHeader ( ws only, replaced by Error::UpgradeConnection ) Removed Error::SecWebSocketAcceptKeyMismatch ( ws only, replaced by Error::UpgradeConnection ) Removed Error::SecWebSocketProtocolMismatch ( ws only, replaced by Error::UpgradeConnection ) Removed impl From<T> for Error Expand for more details The following breaking changes were made as a part of an effort to refine errors (the list is large, but most of them are lower level, and shouldn't require much change in most cases): * Removed `impl From for kube::Error` - [#686](https://github.com/kube-rs/kube/issues/686) * Removed unused error variants in `kube::Error`: `Connection`, `RequestBuild`, `RequestSend`, `RequestParse` - [#689](https://github.com/kube-rs/kube/issues/689) * Removed unused error variant `kube::error::ConfigError::LoadConfigFile` - [#689](https://github.com/kube-rs/kube/issues/689) * Changed `kube::Error::RequestValidation(String)` to `kube::Error::BuildRequest(kube::core::request::Error)`. Includes possible errors from building an HTTP request, and contains some errors from `kube::core` that was previously grouped under `kube::Error::SerdeError` and `kube::Error::HttpError`. `kube::core::request::Error` is described below. - [#686](https://github.com/kube-rs/kube/issues/686) * Removed `kube::core::Error` and `kube::core::Result`. `kube::core::Error` was replaced by more specific errors. - [#686](https://github.com/kube-rs/kube/issues/686) - Replaced `kube::core::Error::InvalidGroupVersion` with `kube::core::gvk::ParseGroupVersionError` - Changed the error returned from `kube::core::admission::AdmissionRequest::with_patch` to `kube::core::admission::SerializePatchError` (was `kube::core::Error::SerdeError`) - Changed the error associated with `TryInto >` to `kube::core::admission::ConvertAdmissionReviewError` (was `kube::core::Error::RequestValidation`) - Changed the error returned from methods of `kube::core::Request` to `kube::core::request::Error` (was `kube::core::Error`). `kube::core::request::Error` represents possible errors when building an HTTP request. The removed `kube::core::Error` had `RequestValidation(String)`, `SerdeError(serde_json::Error)`, and `HttpError(http::Error)` variants. They are now `Validation(String)`, `SerializeBody(serde_json::Error)`, and `BuildRequest(http::Error)` respectively in `kube::core::request::Error`. * Changed variants of error enums in `kube::runtime` to tuples. Replaced `snafu` with `thiserror`. - [#686](https://github.com/kube-rs/kube/issues/686) * Removed `kube::error::ConfigError` and `kube::Error::Kubeconfig(ConfigError)` - [#696](https://github.com/kube-rs/kube/issues/696) - Error variants related to client auth were moved to a new error `kube::client::AuthError` as described below - Remaining variants were split into `kube::config::{InferConfigError, InClusterError, KubeconfigError}` as described below * Added `kube::client::AuthError` by extracting error variants related to client auth from `kube::ConfigError` and adding more variants to preserve context - [#696](https://github.com/kube-rs/kube/issues/696) * Moved `kube::error::OAuthError` to `kube::client::OAuthError` - [#696](https://github.com/kube-rs/kube/issues/696) * Changed all errors in `kube::client::auth` to `kube::client::AuthError` - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::Error::Auth(kube::client::AuthError)` - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::config::InferConfigError` which is an error from `Config::infer()` and `kube::Error::InferConfig(kube::config::InferConfigError)` - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::config::InClusterError` for errors related to loading in-cluster configuration by splitting `kube::ConfigError` and adding more variants to preserve context. - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::config::KubeconfigError` for errors related to loading kubeconfig by splitting `kube::ConfigError` and adding more variants to preserve context. - [#696](https://github.com/kube-rs/kube/issues/696) * Changed methods of `kube::Config` to return these erorrs instead of `kube::Error` - [#696](https://github.com/kube-rs/kube/issues/696) * Removed `kube::Error::InvalidUri` which was replaced by error variants preserving context, such as `KubeconfigError::ParseProxyUrl` - [#696](https://github.com/kube-rs/kube/issues/696) * Moved all errors from upgrading to a WebSocket connection into `kube::Error::UpgradeConnection(kube::client::UpgradeConnectionError)` - [#696](https://github.com/kube-rs/kube/issues/696) 0.63.2 / 2021-10-28 # kube::runtime::events : fix build and hide module on kubernetes < 1.19 (events/v1 missing there) - #685 0.63.1 / 2021-10-26 # kube::runtime::wait::Condition added boolean combinators ( not / and / or ) - #678 kube : fix docs.rs build - #681 via #682 0.63.0 / 2021-10-26 # rust edition bumped to 2021 - #664 , #666 , #667 kube::CustomResource derive can now take arbitrary #[kube(k8s_openapi)] style-paths for k8s_openapi , schemars , serde , and serde_json - #675 kube : fix native-tls included when only rustls-tls feature is selected - #673 via #674 0.62.0 / 2021-10-22 # kube now re-exports kube-runtime under runtime feature - #651 via #652 no need to keep both kube and kube_runtime in Cargo.toml anymore fixes issues with dependabot / lock-step upgrading change kube_runtime::X import paths to kube::runtime::X when moving to the feature kube::runtime added events module with an event Recorder - #249 via #653 + #662 + #663 kube::runtime::wait::conditions added is_crd_established helper - #659 kube::CustomResource derive can now take an arbitrary #[kube(kube_core)] path for kube::core - #658 kube::core consistently re-exported across crates docs: major overhaul + architecture.md - #416 via #652 0.61.0 / 2021-10-09 # kube-core : BREAKING: extend CustomResourceExt trait with ::shortnames method (impl in kube-derive ) - #641 kube-runtime : add wait module to await_condition , and added watch_object to watcher - #632 via #633 kube : add Restart marker trait to allow Api::restart on core workloads - #630 via #635 bump dependencies: tokio-tungstenite , k8s-openapi , schemars , tokio in particular - #643 + #645 0.60.0 / 2021-09-02 # kube : support k8s-openapi with v1_22 features - #621 via #622 kube : BREAKING : support for CustomResourceDefinition at v1beta1 now requires an opt-in deprecated-crd-v1beta1 feature - #622 kube-core : add content-type header to requests with body - #626 via #627 0.59.0 / 2021-08-09 # BREAKING : bumped k8s-openapi to 0.13.0 - #581 via #616 kube connects to kubernetes via cluster dns when using rustls - #587 via #597 client now works with rustls feature in-cluster - #153 via #597 kube nicer serialization of Kubeconfig - #613 kube-core added serde traits for ApiResource - #590 kube-core added CrdExtensions::crd_name method (implemented by kube-derive ) - #583 kube-core added the HasSpec and HasStatus traits - #605 kube-derive added support to automatically implement the HasSpec and HasStatus traits - #605 kube-runtime fix tracing span hierarchy from applier - #600 0.58.1 / 2021-07-06 # kube-runtime : fix non-unix builds - #582 0.58.0 / 2021-07-05 # kube : BREAKING : subresource marker traits renamed conjugation: Log , Execute , Attach , Evict (previously Logging , Executable , Attachable , Evictable ) - #536 via #560 kube-derive added #[kube(category)] attr to set CRD categories - #559 kube-runtime added finalizer helper #291 via #475 kube-runtime added tracing for why reconciliations happened #457 via #571 kube-runtime added Controller::reconcile_all_on to allow scheduling all objects for reconciliation #551 via #555 kube-runtime added Controller::graceful_shutdown_on for shutting down the Controller while waiting for running reconciliations to finish - #552 via #573 BREAKING: controller::applier now starts a graceful shutdown when the queue terminates BREAKING: scheduler now shuts down immediately when requests terminates, rather than waiting for the pending reconciliations to drain kube-runtime added tracking for reconciliation reason Added: Controller::owns_with and Controller::watches_with to pass a dyntype argument for dynamic Api s - #575 BREAKING: Controller::owns signature changed to not allow DynamicType s BREAKING: controller::trigger_* now returns a ReconcileRequest rather than ObjectRef . The ObjectRef can be accessed via the obj_ref field Known Issues # Api::replace can fail to unset list values with k8s-openapi 0.12 #581 0.57.0 / 2021-06-16 # kube : custom clients now respect default namespaces - fixes #534 via #544 BREAKING: custom clients via Client::new must pass config.default_namespace as 2 nd arg kube : Added CustomResourceExt trait for kube-derive - #497 via #545 BREAKING: kube-derive users must import kube::CustomResourceExt (or kube::core::crd::v1beta1::CustomResourceExt if using legacy #[kube(apiextensions = \"v1beta1\")] ) to use generated methods Foo::crd or Foo::api_resource BREAKING: k8s_openapi bumped to 0.12.0 - #531 Generated structs simplified + Resource trait expanded Adds support for kubernetes v1_21 Contains bugfix for kubernetes#102159 kube resource plurals is no longer inferred from k8s-openapi structs - #284 via #556 BREAKING: kube::Resource trait now requires a plural implementation Known Issues # Api::replace can fail to unset list values with k8s-openapi 0.12 #581 0.56.0 / 2021-06-05 # kube : added Api::default_namespaced - #209 via #534 kube : added config feature - #533 via #535 kube : BREAKING: moved client::discovery module to kube::discovery and rewritten module #538 discovery : added oneshot helpers for quick selection of recommended resources / kinds #538 discovery : moved ApiResource and ApiCapabilities (result of discovery) to kube_core::discovery BREAKING: removed internal ApiResource::from_apiresource kube::Client is now configurable with layers using tower-http #539 via #540 three new examples added: custom_client , custom_client_tls and custom_client_trace Big feature streamlining, big service and layer restructuring, dependency restructurings Changes can hit advanced users, but unlikely to hit base use cases with Api and Client . In depth changes broken down below: TLS Enhancements # Add kube::client::ConfigExt extending Config for custom Client . This includes methods to configure TLS connection when building a custom client #539 native-tls : Config::native_tls_https_connector and Config::native_tls_connector rustls-tls : Config::rustls_https_connector and Config::rustls_client_config Remove the requirement of having native-tls or rustls-tls enabled when client is enabled. Allow one, both or none. When both, the default Service will use native-tls because of #153 . rustls can be still used with a custom client. Users will have an option to configure TLS at runtime. When none, HTTP connector is used. Remove TLS features from kube-runtime BREAKING : Features must be removed if specified Remove client feature from native-tls and rust-tls features config + native-tls / rustls-tls can be used independently, e.g., to create a simple HTTP client BREAKING : client feature must be added if default-features = false Layers # ConfigExt::base_uri_layer ( BaseUriLayer ) to set cluster URL (#539) ConfigExt::auth_layer that returns optional layer to manage Authorization header (#539) gzip : Replaced custom decompression module with DecompressionLayer from tower-http (#539) Replaced custom LogRequest with TraceLayer from tower-http (#539) Request body is no longer shown Basic and Bearer authentication using AddAuthorizationLayer (borrowing from https://github.com/tower-rs/tower-http/pull/95 until released) BREAKING : Remove headers from Config . Injecting arbitrary headers is now done with a layer on a custom client. Dependency Changes # Remove static_assertions since it's no longer used Replace tokio_rustls with rustls and webpki since we're not using tokio_rustls directly Replace uses of rustls::internal::pemfile with rustls-pemfile Remove url and always use http::Uri BREAKING : Config::cluster_url is now http::Uri BREAKING : Error::InternalUrlError(url::ParseError) and Error::MalformedUrl(url::ParseError) replaced by Error::InvalidUri(http::uri::InvalidUri) 0.55.0 / 2021-05-21 # kube : client feature added (default-enabled) - #528 kube : PatchParams force now only works with Patch::Apply #528 kube : api discovery module now uses a new ApiResource struct #495 + #482 kube : api BREAKING: DynamicObject + Object now takes an ApiResource rather than a GroupVersionKind kube : api BREAKING: discovery module's Group renamed to ApiGroup kube : client BREAKING: kube::client::Status moved to kube::core::Status (accidental, re-adding in 0.56) kube-core crate factored out of kube to reduce dependencies - #516 via #517 + #519 + #522 + #528 + #530 kube : kube::Service removed to allow kube::Client to take an abritrary Service<http::Request<hyper::Body>> - #532 0.54.0 / 2021-05-19 # yanked 30 minutes after release due to #525 changes lifted to 0.55.0 0.53.0 / 2021-05-15 # kube : admission controller module added under feature - #477 via #484 + fixes in #488 #498 #499 + #507 + #509 kube : config parsing of pem blobs now resilient against missing newlines - #504 via #505 kube : discovery module added to simplify dynamic api usage - #491 kube : api BREAKING: DynamicObject::namespace renamed to ::within - #502 kube : api BREAKING: added ResourceExt trait moving the getters from Resource trait - #486 kube : api added a generic interface for subresources via Request - #487 kube : api fix bug in PatchParams::dry_run not being serialized correctly - #511 0.53.0 Migration Guide # The most likely issue you'll run into is from kube when using Resource trait which has been split: +use kube::api::ResouceExt; - let name = Resource::name(&foo); - let ns = Resource::namespace(&foo).expect(\"foo is namespaced\"); + let name = ResourceExt::name(&foo); + let ns = ResourceExt::namespace(&foo).expect(\"foo is namespaced\"); 0.52.0 / 2021-03-31 # kube-derive : allow overriding #[kube(plural)] and #[kube(singular)] - #458 via #463 kube : added tracing instrumentation for io operations in kube::Api - #455 kube : DeleteParams 's Preconditions is now public - #459 via #460 kube : remove dependency on duplicate derive_accept_key for ws - #452 kube : Properly verify websocket keys in ws handshake - #447 kube : BREAKING: removed optional, and deprecated runtime module - #454 kube : BREAKING: ListParams bookmarks default enabled - #226 via #445 renames member ::allow_bookmarks to ::bookmarks ::default() sets bookmark to true to avoid bad bad defaults #219 method ::allow_bookmarks() replaced by ::disable_bookmarks() kube : DynamicObject and GroupVersionKind introduced for full dynamic object support kube-runtime : watchers/reflectors/controllers can be used with dynamic objects from api discovery kube : Pluralisation now only happens for k8s_openapi objects by default #481 inflector dependency removed #471 added internal pluralisation helper for k8s_openapi objects kube : BREAKING: Restructuring of low level Resource request builder #474 Resource renamed to Request and requires only a path_url to construct kube : BREAKING: Mostly internal Meta trait revamped to support dynamic types Meta renamed to kube::Resource to mimic k8s_openapi::Resource #478 The trait now takes an optional associated type for runtime type info: DynamicType #385 Api::all_with + Api::namespaced_with added for querying with dynamic families see dynamic_watcher + dynamic_api for example usage kube-runtime : BREAKING: lower level interface changes as a result of kube::api::Meta trait: THESE SHOULD NOT AFFECT YOU UNLESS YOU ARE IMPLEMENTING / CUSTOMISING LOW LEVEL TYPES DIRECTLY ObjectRef now generic over kube::Resource rather than RuntimeResource reflector::{Writer, Store} takes a kube::Resource rather than a k8s_openapi::Resource kube-derive : BREAKING: Generated type no longer generates k8s-openapi traits This allows correct pluralisation via #[kube(plural = \"mycustomplurals\")] #467 via #481 0.52.0 Migration Guide # While we had a few breaking changes. Most are to low level internal interfaces and should not change much, but some changes you might need to make: kube # if using the old, low-level kube::api::Resource , please consider the easier kube::Api , or look at tests in request.rs or typed.rs if you need the low level interface search replace kube::api::Meta with kube::Resource if used - trait was renamed if implementing the trait, add type DynamicType = (); to the impl remove calls to ListParams::allow_bookmarks (allow default) handle WatchEvent::Bookmark or set ListParams::disable_bookmarks() look at examples if replacing the long deprecated legacy runtime kube-derive # The following constants from k8s_openapi::Resource no longer exist. Please use kube::Resource and: - replace Foo::KIND with Foo::kind(&()) - replace Foo::GROUP with Foo::group(&()) - replace Foo::VERSION with Foo::version(&()) - replace Foo::API_VERSION with Foo::api_version(&()) 0.51.0 / 2021-02-28 # kube Config now allows arbirary extension objects - #425 kube Config now allows multiple yaml documents per kubeconfig - #440 via #441 kube-derive now more robust and is using darling - #435 docs improvements to patch + runtime 0.50.1 / 2021-02-17 # bug: fix oidc auth provider - #424 via #419 0.50.0 / 2021-02-10 # feat: added support for stacked kubeconfigs - #132 via #411 refactor: authentication logic moved out of kube::config and into into kube::service - #409 BREAKING: Config::get_auth_header removed refactor: remove hyper dependency from kube::api - #410 refactor: kube::Service simpler auth and gzip handling - #405 + #408 0.49.0 / 2021-02-08 # dependency on reqwest + removed in favour of hyper + tower #394 refactor: kube::Client now uses kube::Service (a tower::Service<http::Request<hyper::Body>> ) instead of reqwest::Client to handle all requests refactor: kube::Client now uses a tokio_util::codec for internal buffering refactor: async-tungstenite ws feature dependency replaced with tokio-tungstenite . WebSocketStream is now created from a connection upgraded with hyper refactor: oauth2 module for GCP OAuth replaced with optional tame-oauth dependency BREAKING: GCP OAuth is now opt-in ( oauth feature). Note that GCP provider with command based token source is supported by default. BREAKING: Gzip decompression is now opt-in ( gzip feature) because Kubernetes does not have compression enabled by default yet and this feature requires extra dependencies. #399 BREAKING: Client::new now takes a Service instead of Config #400 . Allows custom service for features not supported out of the box and testing. To create a Client from Config , use Client::try_from instead. BREAKING: Removed Config::proxy . Proxy is no longer supported out of the box, but it should be possible by using a custom Service. fix: Refreshable token from auth provider not refreshing fix: Panic when loading config with non-GCP provider #238 feat: subresource support added for Evictable types (marked for Pod ) - #393 kube : subresource marker traits renamed to Loggable , Executable , Attachable (previously LoggingObject , ExecutingObject , AttachableObject ) - #395 examples showcasing kubectl cp like behaviour #381 via #392 0.48.0 / 2021-01-23 # bump k8s-openapi to 0.11.0 - #388 breaking: kube : no longer necessary to serialize patches yourself - #386 PatchParams removes PatchStrategy Api::patch* methods now take an enum Patch type optional jsonpatch feature added for Patch::Json 0.47.0 / 2021-01-06 # chore: upgrade tokio to 1.0 - #363 BREAKING: This requires the whole application to upgrade to tokio 1.0 and reqwest to 0.11.0 docs: fix broken documentation in kube 0.46.0 #367 bug: kube : removed panics from ws features, fix rustls support + improve docs #369 via #370 + #373 bug: AttachParams now fixes owned method chaining (slightly breaks from 0.46 if using &mut ref before) - #364 feat: AttachParams::interactive_tty convenience method added - #364 bug: fix Runner (and thus Controller and applier ) not waking correctly when starting new tasks - #375 0.46.1 / 2021-01-06 # maintenance release for 0.46 (last supported tokio 0.2 release) from tokio02 branch bug backport: fix Runner (and thus Controller and applier ) not waking correctly when starting new tasks - #375 0.46.0 / 2021-01-02 # feat: kube now has optional websocket support with async_tungstenite under ws and ws-*-tls features #360 feat: AttachableObject marker trait added and implemented for k8s_openapi::api::core::v1::Pod #360 feat: AttachParams added for Api::exec and Api::attach for AttachableObject s #360 examples: pod_shell , pod_attach , pod_exec demonstrating the new features #360 0.45.0 / 2020-12-26 # feat: kube-derive now has a default enabled schema feature allows opting out of schemars dependency for handwriting crds - #355 breaking: kube-derive attr struct_name renamed to struct - #359 docs: improvements on kube , kube-runtime , kube-derive 0.44.0 / 2020-12-23 # feat: kube-derive now generates openapi v3 schemas and is thus usable with v1 CustomResourceDefinition - #129 and #264 via #348 BREAKING: kube-derive types now require JsonSchema derived via schemars libray (not breaking if going to 0.45.0) feat: kube_runtime::controller : now reconciles objects in parallel - #346 BREAKING: kube_runtime::controller::applier now requires that the reconciler 's Future is Unpin , Box::pin it or submit it to a runtime if this is not acceptable BREAKING: kube_runtime::controller::Controller now requires that the reconciler 's Future is Send + 'static , use the low-level applier interface instead if this is not acceptable bug: kube-runtime : removed accidentally included k8s-openapi default features (you have to opt in to them yourself) feat: kube : TypeMeta now derives additionally Debug, Eq, PartialEq, Hash bump: k8s-openapi to 0.10.0 - #330 bump: serde_yaml - #349 bump: dirs to dirs-next - #340 0.43.0 / 2020-10-08 # bug: kube-derive attr #[kube(shortname)] now working correctly bug: kube-derive now working with badly cased existing types - #313 missing: kube now correctly exports config::NamedAuthInfo - #323 feat: kube : expose Config::get_auth_header for istio use cases - #322 feat: kube : local config now tackles gcloud auth exec params - #328 and #84 kube-derive now actually requires GVK (in particular #[kube(kind = \"Foo\")] which we sometimes inferred earlier, despite documenting the contrary) 0.42.0 / 2020-09-10 # bug: kube-derive 's Default derive now sets typemeta correctly - #315 feat: ListParams now supports continue_token and limit - #320 0.41.0 / 2020-09-10 # yanked release. failed publish. 0.40.0 / 2020-08-17 # DynamicResource::from_api_resource added to allow apiserver returned resources - #305 via #301 Client::list_api_groups added Client::list_ap_group_resources added Client::list_core_api_versions added Client::list_core_api_resources added kube::DynamicResource exposed at top level Bug: PatchParams::default_apply() now requires a manager and renamed to PatchParams::apply(manager: &str) for #300 Bug: DeleteParams no longer missing for Api::delete_collection - #53 Removed paramter ListParams::include_uninitialized deprecated since 1.14 Added optional PostParams::field_manager was missing for Api::create case 0.39.0 / 2020-08-05 # Bug: ObjectRef tweak in kube-runtime to allow controllers triggering across cluster and namespace scopes - #293 via #294 Feature: kube now has a derive feature which will re-export kube::CustomResource from kube-derive::CustomResource . Examples: revamp examples for kube-runtime - #201 0.38.0 / 2020-07-23 # Marked kube::runtime module as deprecated - #281 Config::timeout can now be overridden to None (with caveats) #280 Bug: reflector stores could have multiple copies inside datastore - #286 dashmap backend Store driver downgraded - #286 Store::iter temporarily removed Bug: Specialize WatchEvent::Bookmark so they can be deserialized - #285 Docs: Tons of docs for kube-runtime 0.37.0 / 2020-07-20 # Bump k8s-openapi to 0.9.0 All runtime components now require Sync objects reflector/watcher/Controller streams can be shared in threaded environments 0.36.0 / 2020-07-19 # https://gitlab.com/teozkr/kube-rt/ merged in for a new kube-runtime crate #258 Controller<K> added ( #148 via #258 ) Reflector api redesigned ( #102 via #258 ) Migration release for Informer -> watcher + Reflector -> reflector kube::api::CustomResource removed in favour of kube::api::Resource::dynamic CrBuilder removed in favour of DynamicResource (with new error handling) support level bumped to beta 0.35.1 / 2020-06-18 # Fix in-cluster Client when using having multiple certs in the chain - #251 0.35.0 / 2020-06-15 # Config::proxy support added - #246 PartialEq can be derived with kube-derive - #242 Windows builds no longer clashes with runtime - #240 Rancher hosts (with path specifiers) now works - #244 0.34.0 / 2020-05-08 # Bump k8s-openapi to 0.8.0 Config::from_cluster_env <- renamed from Config::new_from_cluster_env Config::from_kubeconfig <- renamed from Config::new_from_kubeconfig Config::from_custom_kubeconfig added - #236 Majorly overhauled error handlind in config module - #237 0.33.0 / 2020-04-27 # documentation fixes for Api::patch Config: add automatic token refresh - #72 / #224 / #234 0.32.1 / 2020-04-15 # add missing tokio signal feature as a dependency upgrade all dependencies, including minor bumps to rustls and base64 0.32.0 / 2020-04-10 # Major config + client module refactor Config is the new Configuration struct Client is now just a configured reqwest::Client plus a reqwest::Url implement From<Config> for reqwest::ClientBuilder implement TryFrom<Config> for Client Client::try_default or Client::new now recommended constructors People parsing ~/.kube/config must use the KubeConfig struct instead Reflector<K> now only takes an Api<K> to construct (.params method) Informer<K> now only takes an Api<K> to construct (.params method) Informer::init_from -> Informer::set_version Reflector now self-polls #151 + handles signals #152 Reflector::poll made private in favour of Reflector::run Api::watch no longer filters out error events ( next -> try_next ) Api::watch returns Result<WatchEvent> rather than WatchEvent WatchEvent::Bookmark added to enum ListParams::allow_bookmarks added PatchParams::default_apply ctor added PatchParams builder mutators: ::force and ::dry_run added 0.31.0 / 2020-03-27 # Expose config::Configuration at root level Add Configuration::infer as a recommended constructor Rename client::APIClient to client::Client Expose client::Client at root level Client now implements From<Configuration> Added comprehensive documentation on Api Rename config::KubeConfigLoader -> config::ConfigLoader removed futures-timer dependency for tokio (feature=timer) 0.30.0 / 2020-03-17 # Fix #[kube(printcolumn)] when #[kube(apiextensions = \"v1beta1\")] Fix #[kube(status)] causing serializes of empty optional statuses 0.29.0 / 2020-03-12 # Api::log -> Api::logs (now matches Resource::logs ) Object<FooSpec, FooStatus> back for ad-hoc ser/de kube-derive now derives Debug (requires Debug on spec struct) kube-derive now allows multiple derives per file Api::create now takes data K rather than bytes Api::replace now takes data K rather than bytes (note that Resource::create and Resource::replace still takes bytes) 0.28.1 / 2020-03-07 # #[derive(CustomResource)] now implements ::new on the generated Kind derived Kind now properly contains TypeMeta - #170 0.28.0 / 2020-03-05 # RawApi removed -> Resource added Resource implements k8s_openapi::Resource ALL OBJECTS REMOVED -> Depening on light version of k8s-openapi now NB: should generally just mean a few import changes (+casings / unwraps) openapi feature removed (light dependency mandatory now) LIBRARY WORKS WITH ALL k8s_openapi KUBERNETES OBJECTS KubeObject trait removed in favour of Meta trait Object<FooSpec, FooStatus> removed -> types implementing k8s_openapi::Resource required instead kube-derive crate added to derive this trait + other kubebuilder like codegen 0.27.0 / 2020-02-26 # Reflector + Informer moved from kube::api to kube::runtime Informer now resets the version to 0 rather than dropping events - #134 Removed Informer::init , since it is now a no-op when building the Informer Downgrade spurious log message when using service account auth 0.26.0 / 2020-02-25 # Fix a large percentage of EOFs from watches #146 => default timeout down to 290s from 300s => Reflector now re-lists a lot less #146 Fix decoder panic with async-compression (probably) #144 Informer::poll can now be used with TryStream Exposed Config::read and Config::read_from - #124 Fix typo on Api::StatefulSet Fix typo on Api::Endpoints Add Api::v1CustomResourceDefinition when on k8s >= 1.17 Renamed Void to NotUsed 0.25.0 / 2020-02-09 # initial rustls support #114 (some local kube config issues know #120 ) crate does better version checking against openapi features - #106 initial log_stream support - #109 0.24.0 / 2020-01-26 # Add support for ServiceAccount, Role, ClusterRole, RoleBinding, Endpoint - #113 + #111 Upgrade k8s-openapi to 0.7 => breaking changes: https://github.com/Arnavion/k8s-openapi/blob/master/CHANGELOG.md#v070-2020-01-23 0.23.0 / 2019-12-31 # Bump tokio and reqwest to 0.2 and 0.10 Fix bug in log fetcher - #107 Temporarily allow invalid certs when testing on macosx - #105 0.22.2 / 2019-12-04 # Allow sharing Reflectors between threads - #97 Fix Reflector pararall lock issue ( poll no longer blocks state ) 0.22.1 / 2019-11-30 # Improve Reflector reset algorithm (clear history less) 0.22.0 / 2019-11-29 # Default watch timeouts changed to 300s everywhere This increases efficiency of Informers and Reflectors by keeping the connection open longer. However, if your Reflector relies on frequent polling you can set timeout or hide the poll() in a different context so it doesn't block your main work Internal RwLock changed to a futures::Mutex for soundness / proper non-blocking - #94 blocking Reflector::read() renamed to async Reflector::state() Expose metadata.creation_timestamp and .deletion_timestamp (behind openapi flag) - #93 0.21.0 / 2019-11-29 # All watch calls returns a stream of WatchEvent - #92 Informer::poll now returns a stream - #92 0.20.1 / 2019-11-21 # ObjectList now implements Iterator - #91 openapi feature no longer accidentally hardcoded to v1.15 feature - #90 0.19.0 / 2019-11-15 # kube::Error is now a proper error enum and not a Fail impl (thiserror) soft-tokio dependency removed for futures-timer gzip re-introduced 0.18.1 / 2019-11-11 # Fix unpinned gzip dependency breakage - #87 0.18.0 / 2019-11-07 # api converted to use async/await with 1.39.0 (primitively) hyper upgraded to 0.10-alpha synchronous sleep replaced with tokio timer Log trait removed in favour of internal marker trait 0.17.0 / 2019-10-22 # Add support for oidc providerss with auth-provider w/o access-token - #70 Bump most dependencies to more recent versions Expose custom client creation Added support for v1beta1Ingress Expose incluster_config::load_default_ns - #74 0.16.1 / 2019-08-09 # Add missing uid field on ObjectMeta::ownerReferences 0.16.0 / 2019-08-09 # Add Reflector::get and Reflector::get_within as cheaper getters Add support for OpenShift kube configs with multiple CAs - via #64 Add missing ObjectMeta::ownerReferences Reduced memory consumption during compile with k8s-openapi@0.5.1 - #62 0.15.1 / 2019-08-18 # Fix compile issue on 1.37.0 with Utc serialization Fix Void not having Serialize derive 0.15.0 / 2019-08-11 # Added support for v1Job resources - via #58 Added support for v1Namespace , v1DaemonSet , v1ReplicaSet , v1PersistentVolumeClaim , v1PersistentVolume , v1ResourceQuota , v1HorizontalPodAutoscaler - via #59 Added support for v1beta1CronJob , v1ReplicationController , v1VolumeAttachment , v1NetworkPolicy - via #60 k8s-openapi optional dependency bumped to 0.5.0 (for kube 1.14 structs) 0.14.0 / 2019-08-03 # Reflector::read now returns a Vec<K>`` rather than a Vec<(name, K)>`: This fixes an unsoundness bug internally - #56 via @gnieto 0.13.0 / 2019-07-22 # Experimental oauth2 support for some providers - via #44 : a big cherry-pick from various prs upstream originally for GCP EKS works with setup in https://github.com/kube-rs/kube/pull/20#issuecomment-511767551 0.12.0 / 2019-07-18 # Added support for Log subresource - via #50 Added support for v1ConfigMap with example - via #49 Demoted some spammy info messages from Reflector 0.11.0 / 2019-07-10 # Added PatchParams with PatchStrategy to allow arbitrary patch types - #24 via @ragne Event renamed to v1Event to match non-slowflake type names v1Service support added Added v1Secret snowflake type and a secret_reflector example 0.10.0 / 2019-06-03 # Api<P, U> is now Api<K> for some KubeObject K: Big change to allow snowflake objects ( #35 ) - but also slightly nicer You want aliases type Pod = Object<PodSpec, PodStatus> This gives you the required KubeObject trait impl for free Added Event native type to prove snowflakes can be handled - #35 ApiStatus renamed to Status to match kube api conventions #36 Rename Metadata to ObjectMeta #36 Added ListMeta for ObjectList and Status #36 Added TypeMeta object which is flattened onto Object , so: o.types.kind rather than o.kind o.types.version rather than o.version 0.9.0 / 2019-06-02 # Status subresource api commands added to Api : patch_status get_status replace_status ^ See crd_openapi or crd_api examples Scale subresource commands added to Api : patch_scale get_scale replace_scale ^ See crd_openapi example 0.8.0 / 2019-05-31 # Typed Api variant called OpenApi introduced (see crd_openapi example) Revert client.request return type change (back to response only from pre-0.7.0 #28 ) delete now returns `Either , ApiStatus> - for bug #32 delete_collection now returns `Either >, ApiStatus> - for bug #32 Informer::new renamed to Informer::raw Reflector::new renamed to Reflector::raw Reflector::new + Informer::new added for \"openapi\" compile time feature (does not require specifying the generic types) 0.7.0 / 2019-05-27 # Expose list/watch parameters #11 Many API struct renames: ResourceMap -> Cache Resource -> Object ResourceList -> ObjectList ApiResource -> Api ResourceType has been removed in favour of Api::v1Pod() say Object::status now wrapped in an Option (not present everywhere) ObjectList exposed Major API overhaul to support generic operations on Object Api can be used to perform generic actions on resources: create get delete watch list patch replace get_scale (when scale subresource exists) patch_scale (ditto) replace_scale (ditto) get_status (when status subresource exists) patch_status (ditto) replace_status (ditto) crd_api example added to track the action api Bunch of generic parameter structs exposed for common operations: ListParams exposed DeleteParams exposed PostParams exposed Errors from Api exposed in kube::Error : Error::api_error -> Option<ApiError> exposed Various other error types also in there (but awkward setup atm) client.request now returns a tuple (T, StatusCode) (before only T ) 0.6.0 / 2019-05-12 # Expose getter Informer::version Exose ctor Informer::from_version Expose more attributes in Metadata Informer::reset convenience method added Informer::poll no longer returns events straight an Informer now caches WatchEvent elements into an internal queue Informer::pop pops a single element from its internal queue Reflector::refresh renamed to Reflector::reset (matches Informer ) Void type added so we can use Reflector<ActualSpec, Void> removes need for Spec/Status structs: ReflectorSpec , ReflectorStatus removed InformerSpec , InformerStatus removed ResourceSpecMap , ResourceStatusMap removed WatchEvents removed WatchEvent exposed, and now wraps `Resource `` 0.5.0 / 2019-05-09 # added Informer struct dedicated to handling events Reflectors no longer cache events - see #6 0.4.0 / 2019-05-09 # ResourceMap now contains the full Resource struct rather than a tuple as the value. => value.metadata is available in the cache. Reflectors now also cache events to allow apps to handle them 0.3.0 / 2019-05-09 # Named trait removed (inferring from metadata.name now) Reflectors now take two type parameters (unless you use ReflectorSpec or ReflectorStatus ) - see examples for usage Native kube types supported via ApiResource Some native kube resources have easy converters to ApiResource","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#unreleased","text":"see https://github.com/kube-rs/kube/compare/0.76.0...main","title":"Unreleased"},{"location":"changelog/#0760--2022-10-28","text":"","title":"0.76.0 / 2022-10-28"},{"location":"changelog/#highlights","text":"","title":"Highlights"},{"location":"changelog/#derivecustomresource-now-supports-schemas-with-untagged-enums","text":"Expanding on our existing support for storing Rust's struct enums in CRDs, Kube will now try to convert #[serde(untagged)] enums as well. Note that if the same field is present in multiple untagged variants then they must all have the same shape.","title":"#[derive(CustomResource)] now supports schemas with untagged enums"},{"location":"changelog/#removed-deprecated-try_flatten_-functions","text":"These have been deprecated since 0.72, and are replaced by the equivalent WatchStreamExt methods.","title":"Removed deprecated try_flatten_* functions"},{"location":"changelog/#whats-changed","text":"","title":"What's Changed"},{"location":"changelog/#added","text":"Adds example to Controller::watches by @Dav1dde in https://github.com/kube-rs/kube/pull/1026 Discovery: Add ApiGroup::resources_by_stability by @imuxin in https://github.com/kube-rs/kube/pull/1022 Add support for untagged enums in CRDs by @sbernauer in https://github.com/kube-rs/kube/pull/1028 Derive PartialEq for DynamicObject by @pbzweihander in https://github.com/kube-rs/kube/pull/1048","title":"Added"},{"location":"changelog/#removed","text":"Runtime: Remove deprecated util try_flatten_ helpers by @clux in https://github.com/kube-rs/kube/pull/1019 Remove native-tls feature by @kazk in https://github.com/kube-rs/kube/pull/1044","title":"Removed"},{"location":"changelog/#fixed","text":"add fieldManager querystring to all operations by @goenning in https://github.com/kube-rs/kube/pull/1031 Add verify_tls1x_signature for NoCertVerification by @rvql in https://github.com/kube-rs/kube/pull/1034 Fix compatibility with schemars' preserve_order feature by @teozkr in https://github.com/kube-rs/kube/pull/1050 Hoist enum values from subschemas by @teozkr in https://github.com/kube-rs/kube/pull/1051","title":"Fixed"},{"location":"changelog/#0750--2022-09-21","text":"","title":"0.75.0 / 2022-09-21"},{"location":"changelog/#highlights_1","text":"","title":"Highlights"},{"location":"changelog/#upgrade-k8s-openapi-to-016-for-kubernetes-125","text":"The update to k8s-openapi@0.16.0 makes this the first release with tentative Kubernetes 1.25 support. While the new structs and apis now exist, we recommend holding off on using 1.25 until a deserialization bug in the apiserver is resolved upstream. See #997 / #1008 for details. To upgrade, ensure you bump both kube and k8s-openapi : cargo upgrade kube k8s-openapi","title":"Upgrade k8s-openapi to 0.16 for Kubernetes 1.25"},{"location":"changelog/#newold-configincluster-default-to-connect-in-cluster","text":"Our previous default of connecting to the Kubernetes apiserver via kubernetes.default.svc has been reverted back to use the old environment variables after Kubernetes updated their position that the environment variables are not legacy. This does unfortunately regress on rustls support, so for those users we have included a Config::incluster_dns to work around the old rustls issue while it is open.","title":"New/Old Config::incluster default to connect in cluster"},{"location":"changelog/#controller-error_policy-extension","text":"The error_policy fn now has access to the object that failed the reconciliation to ease metric creation / failure attribution. The following change is needed on the user side: -fn error_policy(error: &Error, ctx: Arc<Data>) -> Action { +fn error_policy(_obj: Arc<YourObject>, error: &Error, ctx: Arc<Data>) -> Action {","title":"Controller error_policy extension"},{"location":"changelog/#polish--subresources--conversion","text":"There are also a slew of ergonomics improvements, closing of gaps in subresources , adding initial support for ConversionReview , making Api::namespaced impossible to use for non-namepaced resources (a common pitfall), as well as many great fixes to the edge cases in portforwarding and finalizers . Many of these changes came from first time contributors. A huge thank you to everyone involved.","title":"Polish / Subresources / Conversion"},{"location":"changelog/#whats-changed_1","text":"","title":"What's Changed"},{"location":"changelog/#added_1","text":"Make Config::auth_info public by @danrspencer in https://github.com/kube-rs/kube/pull/959 Make raw Client::send method public by @tiagolobocastro in https://github.com/kube-rs/kube/pull/972 Make types on AdmissionRequest and AdmissionResponse public by @clux in https://github.com/kube-rs/kube/pull/977 Add #[serde(default)] to metadata field of DynamicObject by @pbzweihander in https://github.com/kube-rs/kube/pull/987 Add create_subresource method to Api and create_token_request method to Api<ServiceAccount> by @pbzweihander in https://github.com/kube-rs/kube/pull/989 Controller: impl Eq and PartialEq for Action by @Sherlock-Holo in https://github.com/kube-rs/kube/pull/993 Add support for CRD ConversionReview types by @MikailBag in https://github.com/kube-rs/kube/pull/999","title":"Added"},{"location":"changelog/#changed","text":"Constrain Resource trait and Api::namespaced by Scope by @clux in https://github.com/kube-rs/kube/pull/956 Add connect/read/write timeouts to Config by @goenning in https://github.com/kube-rs/kube/pull/971 Controller: Include the object being reconciled in the error_policy by @felipesere in https://github.com/kube-rs/kube/pull/995 Config : New incluster and incluster_dns constructors by @olix0r in https://github.com/kube-rs/kube/pull/1001 Upgrade k8s-openapi to 0.16 by @clux in https://github.com/kube-rs/kube/pull/1008","title":"Changed"},{"location":"changelog/#fixed_1","text":"Remove tracing::instrument from apply_debug_overrides by @kazk in https://github.com/kube-rs/kube/pull/958 fix duplicate finalizers race condition by @alex-hunt-materialize in https://github.com/kube-rs/kube/pull/965 fix: portforward connection cleanup by @tiagolobocastro in https://github.com/kube-rs/kube/pull/973","title":"Fixed"},{"location":"changelog/#0740--2022-07-09","text":"","title":"0.74.0 / 2022-07-09"},{"location":"changelog/#highlights_2","text":"","title":"Highlights"},{"location":"changelog/#polish-bug-fixes-guidelines-ci-improvements-and-new-contributors","text":"This release features smaller improvements/additions/cleanups/fixes, many of which are from new first-time contributors! Thank you everyone! The listed deadlock fix was backported to 0.73.1. We have also been trying to clarify and prove a lot more of our external-facing guarantees, and as a result: We have codified our Kubernetes versioning policy The Rust version policy has extended its support range Our CI has been extended","title":"Polish, bug fixes, guidelines, ci improvements, and new contributors"},{"location":"changelog/#resourceextname-deprecation","text":"A consequence of all the policy writing and the improved clarity we have decided to deprecate the common ResourceExt::name helper. This method could panic and it is unexpected for the users and bad for our consistency. To get the old functionality, you can replace any .name() call on a Kubernetes resources with .name_unchecked() ; but as the name implies, it can panic (in a local setting, or during admission). We recommend you replace it with the new ResourceExt::name_any for a general identifier: -pod.name() +pod.name_any()","title":"ResourceExt::name deprecation"},{"location":"changelog/#whats-changed_2","text":"","title":"What's Changed"},{"location":"changelog/#added_2","text":"Add support for passing the fieldValidation query parameter on patch by @phroggyy in https://github.com/kube-rs/kube/pull/929 Add conditions::is_job_completed by @clux in https://github.com/kube-rs/kube/pull/935","title":"Added"},{"location":"changelog/#changed_1","text":"Deprecate ResourceExt::name in favour of safe name_* alternatives by @clux in https://github.com/kube-rs/kube/pull/945","title":"Changed"},{"location":"changelog/#removed_1","text":"Remove #[kube(apiextensions)] flag from kube-derive by @clux in https://github.com/kube-rs/kube/pull/920","title":"Removed"},{"location":"changelog/#fixed_2","text":"Document every public derived fn from kube-derive by @clux in https://github.com/kube-rs/kube/pull/919 fix applier hangs which can happen with many watched objects by @moustafab in https://github.com/kube-rs/kube/pull/925 Applier: Improve reconciler reschedule context to avoid deadlocking on full channel by @teozkr in https://github.com/kube-rs/kube/pull/932 Fix deserialization issue in AdmissionResponse by @clux in https://github.com/kube-rs/kube/pull/939 Admission controller example fixes by @Alibirb in https://github.com/kube-rs/kube/pull/950","title":"Fixed"},{"location":"changelog/#0731--2022-06-03","text":"","title":"0.73.1 / 2022-06-03"},{"location":"changelog/#highlights_3","text":"This patch release fixes a bug causing applier and Controller to deadlock when too many Kubernetes object change events were ingested at once. All users of applier and Controller are encouraged to upgrade as quickly as possible. Older versions are also affected, this bug is believed to have existed since the original release of kube_runtime .","title":"Highlights"},{"location":"changelog/#whats-changed_3","text":"","title":"What's Changed"},{"location":"changelog/#fixed_3","text":"[0.73 backport] fix applier hangs which can happen with many watched objects (#925) by @moustafab (backported by @teozkr) in https://github.com/kube-rs/kube/pull/927 Full Changelog : https://github.com/kube-rs/kube/compare/0.73.0...0.73.1","title":"Fixed"},{"location":"changelog/#0730--2022-05-23","text":"","title":"0.73.0 / 2022-05-23"},{"location":"changelog/#highlights_4","text":"","title":"Highlights"},{"location":"changelog/#new-k8s-openapi-version-and-msrv","text":"Support added for Kubernetes v1_24 support via the new k8s-openapi version . Please also run cargo upgrade --workspace k8s-openapi when upgrading kube . This also bumps our MSRV to 1.60.0 .","title":"New k8s-openapi version and MSRV"},{"location":"changelog/#reconciler-change","text":"A small ergonomic change in the reconcile signature has removed the need for the Context object. This has been replaced by an Arc . The following change is needed in your controller: -async fn reconcile(doc: Arc<MyObject>, context: Context<Data>) -> Result<Action, Error> +async fn reconcile(doc: Arc<MyObject>, context: Arc<Data>) -> Result<Action, Error> This will simplify the usage of the context argument. You should no longer need to pass .get_ref() on its every use. See the controller-rs upgrade change for details .","title":"Reconciler change"},{"location":"changelog/#whats-changed_4","text":"","title":"What's Changed"},{"location":"changelog/#added_3","text":"Add Discovery::groups_alphabetical following kubectl sort order by @clux in https://github.com/kube-rs/kube/pull/887","title":"Added"},{"location":"changelog/#changed_2","text":"Replace runtime::controller::Context with Arc by @teozkr in https://github.com/kube-rs/kube/pull/910 runtime: Return the object from await_condition by @olix0r in https://github.com/kube-rs/kube/pull/877 Bump k8s-openapi to 0.15 for kubernetes v1_24 and bump MSRV to 1.60 by @clux in https://github.com/kube-rs/kube/pull/916","title":"Changed"},{"location":"changelog/#0720--2022-05-13","text":"","title":"0.72.0 / 2022-05-13"},{"location":"changelog/#highlights_5","text":"","title":"Highlights"},{"location":"changelog/#ergonomics-improvements","text":"A new runtime::WatchSteamExt ( #899 + #906 ) allows for simpler setups for streams from watcher or reflector . - let stream = utils::try_flatten_applied(StreamBackoff::new(watcher(api, lp), b)); + let stream = watcher(api, lp).backoff(b).applied_objects(); The util::try_flatten_* helpers have been marked as deprecated since they are not used by the stream impls. A new reflector:store() fn allows simpler reflector setups #907 : - let store = reflector::store::Writer::<Node>::default(); - let reader = store.as_reader(); + let (reader, writer) = reflector::store(); Additional conveniences getters/settes to ResourceExt for manged_fields and creation_timestamp #888 + #898 , plus a GroupVersion::with_kind path to a GVK, and a TryFrom<TypeMeta> for GroupVersionKind in #896 .","title":"Ergonomics improvements"},{"location":"changelog/#crd-version-selection","text":"Managing multiple version in CustomResourceDefinitions can be pretty complicated, but we now have helpers and docs on how to tackle it. A new function kube::core::crd::merge_crds have been added (in #889 ) to help push crd schemas generated by kube-derived crds with different #[kube(version)] properties. See the kube-derive#version documentation for details. A new example showcases how one can manage two or more versions of a crd and what the expected truncation outcomes are when moving between versions.","title":"CRD Version Selection"},{"location":"changelog/#examples","text":"Examples now have moved to tracing for its logging, respects RUST_LOG , and namespace selection via the kubeconfig context. There is also a larger kubectl example showcasing kubectl apply -f yaml as well as kubectl {edit,delete,get,watch} via #885 + #897 .","title":"Examples"},{"location":"changelog/#whats-changed_5","text":"","title":"What's Changed"},{"location":"changelog/#added_4","text":"Allow merging multi-version CRDs into a single schema by @clux in https://github.com/kube-rs/kube/pull/889 Add GroupVersion::with_kind and TypeMeta -> GroupVersionKind converters by @clux in https://github.com/kube-rs/kube/pull/896 Add managed_fields accessors to ResourceExt by @clux in https://github.com/kube-rs/kube/pull/898 Add ResourceExt::creation_timestamp by @clux in https://github.com/kube-rs/kube/pull/888 Support lowercase http_proxy & https_proxy evars by @DevineLiu in https://github.com/kube-rs/kube/pull/892 Add a WatchStreamExt trait for stream chaining by @clux in https://github.com/kube-rs/kube/pull/899 Add Event::modify + reflector::store helpers by @clux in https://github.com/kube-rs/kube/pull/907","title":"Added"},{"location":"changelog/#changed_3","text":"Switch to kubernetes cluster dns for incluster url everywhere by @clux in https://github.com/kube-rs/kube/pull/876 Update tower-http requirement from 0.2.0 to 0.3.2 by @dependabot in https://github.com/kube-rs/kube/pull/893","title":"Changed"},{"location":"changelog/#removed_2","text":"Remove deprecated legacy crd v1beta1 by @clux in https://github.com/kube-rs/kube/pull/890","title":"Removed"},{"location":"changelog/#0710--2022-04-12","text":"","title":"0.71.0 / 2022-04-12"},{"location":"changelog/#highlights_6","text":"Several quality of life changes and improvement this release for port-forwarding, a new ClientBuilder , better handling of kube-derive edge-cases. We highlight some changes here that you should be especially aware of.","title":"Highlights"},{"location":"changelog/#eventsrecorder-publishing-to-kube-system-for-cluster-scoped-resources","text":"Publishing events via Recorder for cluster scoped resources (supported since 0.70.0 ) now publish to kube-system rather than default , as all but the newest clusters struggle with publishing events in the default namespace.","title":"events::Recorder publishing to kube-system for cluster scoped resources"},{"location":"changelog/#default-tls-stack-set-to-openssl","text":"The previous native-tls default was there because we used to depend on reqwest , but because we depended on openssl anyway the feature does not make much sense. Changing to openssl-tls also improves the situation on macOS where the Security Framework struggles with PKCS#12 certs from OpenSSL v3. The native-tls feature will still be available in this release in case of issues, but the plan is to decommission it shortly. Of course, we all ideally want to move to rustls, but we are still blocked by #153 .","title":"Default TLS stack set to OpenSSL"},{"location":"changelog/#whats-changed_6","text":"","title":"What's Changed"},{"location":"changelog/#added_5","text":"Add ClientBuilder that lets users add custom middleware without full stack replacement by @teozkr in https://github.com/kube-rs/kube/pull/855 Support top-level enums in CRDs by @sbernauer in https://github.com/kube-rs/kube/pull/856","title":"Added"},{"location":"changelog/#changed_4","text":"portforward: Improve API and support background task cancelation by @olix0r in https://github.com/kube-rs/kube/pull/854 Make remote commands cancellable and remove panics by @kazk in https://github.com/kube-rs/kube/pull/861 Change the default TLS to OpenSSL by @kazk in https://github.com/kube-rs/kube/pull/863 change event recorder cluster namespace to kube-system by @clux in https://github.com/kube-rs/kube/pull/871","title":"Changed"},{"location":"changelog/#fixed_4","text":"Fix schemas containing both properties and additionalProperties by @jcaesar in https://github.com/kube-rs/kube/pull/845 Make dependency pins between sibling crates stricter by @clux in https://github.com/kube-rs/kube/pull/864 Fix in-cluster kube_host_port generation for IPv6 by @somnusfish in https://github.com/kube-rs/kube/pull/875","title":"Fixed"},{"location":"changelog/#0700--2022-03-20","text":"","title":"0.70.0 / 2022-03-20"},{"location":"changelog/#highlights_7","text":"","title":"Highlights"},{"location":"changelog/#support-for-ec-keys-with-rustls","text":"This was one of the big blockers for using rustls against clusters like k3d or k3s While not sufficient to fix using those clusters out of the box, it is now possible to use them with a workarodund","title":"Support for EC keys with rustls"},{"location":"changelog/#more-ergonomic-reconciler","text":"The signature and end the Ok action in reconcile fns has been simplified slightly, and requires the following user updates: -async fn reconcile(obj: Arc<MyObject>, ctx: Context<Data>) -> Result<ReconcilerAction, Error> { - ... - Ok(ReconcilerAction { - requeue_after: Some(Duration::from_secs(300)), - }) +async fn reconcile(obj: Arc<MyObject>, ctx: Context<Data>) -> Result<Action, Error> { + ... + Ok(Action::requeue(Duration::from_secs(300))) The Action import lives in the same place as the old ReconcilerAction .","title":"More ergonomic reconciler"},{"location":"changelog/#whats-changed_7","text":"","title":"What's Changed"},{"location":"changelog/#added_6","text":"Add support for EC private keys by @farcaller in https://github.com/kube-rs/kube/pull/804 Add helper for creating a controller owner_ref on Resource by @clux in https://github.com/kube-rs/kube/pull/850","title":"Added"},{"location":"changelog/#changed_5","text":"Remove scheduler::Error by @teozkr in https://github.com/kube-rs/kube/pull/827 Bump parking_lot to 0.12, but allow dep duplicates by @clux in https://github.com/kube-rs/kube/pull/836 Update tokio-tungstenite requirement from 0.16.1 to 0.17.1 by @dependabot in https://github.com/kube-rs/kube/pull/841 Let OccupiedEntry::commit take PostParams by @teozkr in https://github.com/kube-rs/kube/pull/842 Change ReconcileAction to Action and add associated ctors by @clux in https://github.com/kube-rs/kube/pull/851","title":"Changed"},{"location":"changelog/#fixed_5","text":"Token reloading with RwLock by @kazk in https://github.com/kube-rs/kube/pull/835 Fix event publishing for cluster scoped crds by @zhrebicek in https://github.com/kube-rs/kube/pull/847 Fix invalid CRD when Enum variants have descriptions by @sbernauer in https://github.com/kube-rs/kube/pull/852","title":"Fixed"},{"location":"changelog/#0691--2022-02-16","text":"","title":"0.69.1 / 2022-02-16"},{"location":"changelog/#highlights_8","text":"This is an emergency patch release fixing a bug in 0.69.0 where a kube::Client would deadlock after running inside a cluster for about a minute (#829). All users of 0.69.0 are encouraged to upgrade immediately. 0.68.x and below are not affected.","title":"Highlights"},{"location":"changelog/#whats-changed_8","text":"","title":"What's Changed"},{"location":"changelog/#fixed_6","text":"[0.69.x] Fix deadlock in token reloading by @clux (backported by @teozkr) in https://github.com/kube-rs/kube/pull/831","title":"Fixed"},{"location":"changelog/#0690--2022-02-14","text":"","title":"0.69.0 / 2022-02-14"},{"location":"changelog/#highlights_9","text":"","title":"Highlights"},{"location":"changelog/#ergonomic-additions-to-api","text":"Two new methods have been added to the client Api this release to reduce the amount of boiler-plate needed for common patterns. Api::entry via 811 - to aid idempotent crud operation flows (following the style of Map::Entry ) Api::get_opt via 809 - to aid dealing with the NotFound type error via a returned Option","title":"Ergonomic Additions to Api"},{"location":"changelog/#in-cluster-token-reloading","text":"Following a requirement for Kubernetes clients against versions >= 1.22.0 , our bundled AuthLayer will reload tokens every minute when deployed in-cluster.","title":"In-cluster Token reloading"},{"location":"changelog/#whats-changed_9","text":"","title":"What's Changed"},{"location":"changelog/#added_7","text":"Add conversion for ObjectRef<K> to ObjectReference by @teozkr in https://github.com/kube-rs/kube/pull/815 Add Api::get_opt for better existence handling by @teozkr in https://github.com/kube-rs/kube/pull/809 Entry API by @teozkr in https://github.com/kube-rs/kube/pull/811","title":"Added"},{"location":"changelog/#changed_6","text":"Reload token file at least once a minute by @kazk in https://github.com/kube-rs/kube/pull/768 Prefer kubeconfig over in-cluster config by @teozkr in https://github.com/kube-rs/kube/pull/823","title":"Changed"},{"location":"changelog/#fixed_7","text":"Disable CSR utilities on K8s <1.19 by @teozkr in https://github.com/kube-rs/kube/pull/817","title":"Fixed"},{"location":"changelog/#0680--2022-02-01","text":"","title":"0.68.0 / 2022-02-01"},{"location":"changelog/#interface-changes","text":"To reduce the amount of allocation done inside the runtime by reflectors and controllers, the following change via #786 is needed on the signature of your reconcile functions: -async fn reconcile(myobj: MyK, ctx: Context<Data>) -> Result<ReconcilerAction> +async fn reconcile(myobj: Arc<MyK>, ctx: Context<Data>) -> Result<ReconcilerAction> This also affects the finalizer helper .","title":"Interface Changes"},{"location":"changelog/#port-forwarding","text":"As one of the last steps toward gold level client requirements , port-forwarding landed in #446 . There are 3 new examples ( port_forward*.rs ) that showcases how to use this websocket based functionality.","title":"Port-forwarding"},{"location":"changelog/#whats-changed_10","text":"","title":"What's Changed"},{"location":"changelog/#added_8","text":"Add a VS Code devcontainer configuration by @olix0r in https://github.com/kube-rs/kube/pull/788 Add support for user impersonation by @teozkr in https://github.com/kube-rs/kube/pull/797 Add port forward by @kazk in https://github.com/kube-rs/kube/pull/446","title":"Added"},{"location":"changelog/#changed_7","text":"runtime: Store resources in an Arc by @olix0r in https://github.com/kube-rs/kube/pull/786 Propagate Arc through the finalizer reconciler helper by @teozkr in https://github.com/kube-rs/kube/pull/792 Disable unused default features of chrono crate by @dreamer in https://github.com/kube-rs/kube/pull/801","title":"Changed"},{"location":"changelog/#fixed_8","text":"Use absolute path to Result in derives by @teozkr in https://github.com/kube-rs/kube/pull/795 core: add missing reason to Display on Error::Validation in Request by @clux in https://github.com/kube-rs/kube/pull/798","title":"Fixed"},{"location":"changelog/#0670--2022-01-25","text":"","title":"0.67.0 / 2022-01-25"},{"location":"changelog/#changed_8","text":"runtime: Replace DashMap with a locked AHashMap by @olix0r in https://github.com/kube-rs/kube/pull/785 update k8s-openapi for kubernetes 1.23 support by @clux in https://github.com/kube-rs/kube/pull/789","title":"Changed"},{"location":"changelog/#0660--2022-01-15","text":"Tons of ergonomics improvements, and 3 new contributors. Highlighted first is the 3 most discussed changes:","title":"0.66.0 / 2022-01-15"},{"location":"changelog/#support-for-auto-generating-schemas-for-enums-in-kube-derive","text":"It is now possible to embed complex enums inside structs that use #[derive(CustomResource)] . This has been a highly requested feature since the inception of auto-generated schemas. It does not work for all cases , and has certain ergonomics caveats , but represents a huge step forwards. Note that if you depend on kube-derive directly rather than via kube then you must now add the schema feature to kube-core","title":"Support for auto-generating schemas for enums in kube-derive"},{"location":"changelog/#new-streambackoff-mechanism-in-kube-runtime","text":"To avoid spamming the apiserver when on certain watch errors cases, it's now possible to stream wrap the watcher to set backoffs. The new default_backoff follows existing client-go conventions of being kind to the apiserver. Initially, this is default-enabled in Controller watches (configurable via Controller::trigger_backoff ) and avoids spam errors when crds are not installed.","title":"New StreamBackoff mechanism in kube-runtime"},{"location":"changelog/#new-version-priority-parser-in-kube-core","text":"To aid users picking the most appropriate version of a kind from api discovery or through a CRD , two new sort orders have been exposed on the new kube_core::Version Version::priority implementing kubernetes version priority Version::generation implementing a more traditional; generational sort (highest version)","title":"New version priority parser in kube-core"},{"location":"changelog/#changes","text":"Merged PRs from github release .","title":"Changes"},{"location":"changelog/#added_9","text":"Add DeleteParams constructors for easily setting PropagationPolicy by @kate-goldenring in https://github.com/kube-rs/kube/pull/757 Add Serialize to ObjecList and add field-selector and jsonpath example by @ChinYing-Li in https://github.com/kube-rs/kube/pull/760 Implement cordon/uncordon for Node by @ChinYing-Li in https://github.com/kube-rs/kube/pull/762 Export Version priority parser with Ord impls in kube_core by @clux in https://github.com/kube-rs/kube/pull/764 Add Api fns for arbitrary subresources and approval subresource for CertificateSigningRequest by @ChinYing-Li in https://github.com/kube-rs/kube/pull/773","title":"Added"},{"location":"changelog/#changed_9","text":"Add backoff handling for watcher and Controller by @clux in https://github.com/kube-rs/kube/pull/703 Remove crate private identity_pem field from Config by @kazk in https://github.com/kube-rs/kube/pull/771 Use SecretString in AuthInfo to avoid credential leaking by @ChinYing-Li in https://github.com/kube-rs/kube/pull/766","title":"Changed"},{"location":"changelog/#0650--2021-12-10","text":"BREAKING: Removed kube::Error::OpenSslError - #716 BREAKING: Removed kube::Error::SslError - #704 and #716 BREAKING: Added kube::Error::NativeTls(kube::client::NativeTlsError) for errors from Native TLS - #716 BREAKING: Added kube::Error::RustlsTls(kube::client::RustlsTlsError) for errors from Rustls TLS - #704 Modified Kubeconfig parsing - allow empty kubeconfigs as per kubectl - #721 Added Kubeconfig::from_yaml - #718 via #719 Updated rustls to 0.20.1 - #704 BREAKING: Added ObjectRef to the object that failed to be reconciled to kube::runtime::controller::Error::ReconcileFailed - #733 BREAKING: Removed api_version and kind fields from kind structs generated by kube::CustomResource - #739 Updated tokio-tungstenite to 0.16 - #750 Updated tower-http to 0.2.0 - #748 BREAKING: kube-client : replace RefreshTokenLayer with AsyncFilterLayer in AuthLayer - #752","title":"0.65.0 / 2021-12-10"},{"location":"changelog/#0640--2021-11-16","text":"BREAKING: Replaced feature kube-derive/schema with attribute #[kube(schema)] - #690 If you currently disable default kube-derive default features to avoid automatic schema generation, add #[kube(schema = \"disabled\")] to your spec struct instead BREAKING: Moved CustomResource derive crate overrides into subattribute #[kube(crates(...))] - #690 Replace #[kube(kube_core = .., k8s_openapi = .., schema = .., serde = .., serde_json = ..)] with #[kube(crates(kube_core = .., k8s_openapi = .., schema = .., serde = .., serde_json = ..))] Added openssl-tls feature to use openssl for TLS on all platforms. Note that, even though native-tls uses a platform specific TLS, kube requires openssl on all platforms because native-tls only allows PKCS12 input to load certificates and private key at the moment, and creating PKCS12 requires openssl . - #700 BREAKING: Changed to fail loading configurations with PEM-encoded certificates containing invalid sections instead of ignoring them. Updated pem to 1.0.1. - #702 oauth : Updated tame-oauth to 0.6.0 which supports the same default credentials flow as the Go oauth2 for Google OAuth. In addition to reading the service account information from JSON file specified with GOOGLE_APPLICATION_CREDENTIALS environment variable, Application Default Credentials from gcloud , and obtaining OAuth tokens from local metadata server when running inside GCP are now supported. - #701","title":"0.64.0 / 2021-11-16"},{"location":"changelog/#refining-errors","text":"We started working on improving error ergonomics. See the tracking issue #688 for more details. The following is the summary of changes to kube::Error included in this release: Added Error::Auth(kube::client::AuthError) (errors related to client auth, some of them were previously in Error::Kubeconfig ) Added Error::BuildRequest(kube::core::request::Error) (errors building request from kube::core ) Added Error::InferConfig(kube::config::InferConfigError) (for Client::try_default ) Added Error::OpensslTls(kube::client::OpensslTlsError) (new openssl-tls feature) - #700 Added Error::UpgradeConnection(kube::client::UpgradeConnectinError) ( ws feature, errors from upgrading a connection) Removed Error::Connection (was unused) Removed Error::RequestBuild (was unused) Removed Error::RequestSend (was unused) Removed Error::RequestParse (was unused) Removed Error::InvalidUri (replaced by variants of errors in kube::config errors) Removed Error::RequestValidation (replaced by a variant of Error::BuildRequest ) Removed Error::Kubeconfig (replaced by Error::InferConfig , and Error::Auth ) Removed Error::ProtocolSwitch ( ws only, replaced by Error::UpgradeConnection ) Removed Error::MissingUpgradeWebSocketHeader ( ws only, replaced by Error::UpgradeConnection ) Removed Error::MissingConnectionUpgradeHeader ( ws only, replaced by Error::UpgradeConnection ) Removed Error::SecWebSocketAcceptKeyMismatch ( ws only, replaced by Error::UpgradeConnection ) Removed Error::SecWebSocketProtocolMismatch ( ws only, replaced by Error::UpgradeConnection ) Removed impl From<T> for Error Expand for more details The following breaking changes were made as a part of an effort to refine errors (the list is large, but most of them are lower level, and shouldn't require much change in most cases): * Removed `impl From for kube::Error` - [#686](https://github.com/kube-rs/kube/issues/686) * Removed unused error variants in `kube::Error`: `Connection`, `RequestBuild`, `RequestSend`, `RequestParse` - [#689](https://github.com/kube-rs/kube/issues/689) * Removed unused error variant `kube::error::ConfigError::LoadConfigFile` - [#689](https://github.com/kube-rs/kube/issues/689) * Changed `kube::Error::RequestValidation(String)` to `kube::Error::BuildRequest(kube::core::request::Error)`. Includes possible errors from building an HTTP request, and contains some errors from `kube::core` that was previously grouped under `kube::Error::SerdeError` and `kube::Error::HttpError`. `kube::core::request::Error` is described below. - [#686](https://github.com/kube-rs/kube/issues/686) * Removed `kube::core::Error` and `kube::core::Result`. `kube::core::Error` was replaced by more specific errors. - [#686](https://github.com/kube-rs/kube/issues/686) - Replaced `kube::core::Error::InvalidGroupVersion` with `kube::core::gvk::ParseGroupVersionError` - Changed the error returned from `kube::core::admission::AdmissionRequest::with_patch` to `kube::core::admission::SerializePatchError` (was `kube::core::Error::SerdeError`) - Changed the error associated with `TryInto >` to `kube::core::admission::ConvertAdmissionReviewError` (was `kube::core::Error::RequestValidation`) - Changed the error returned from methods of `kube::core::Request` to `kube::core::request::Error` (was `kube::core::Error`). `kube::core::request::Error` represents possible errors when building an HTTP request. The removed `kube::core::Error` had `RequestValidation(String)`, `SerdeError(serde_json::Error)`, and `HttpError(http::Error)` variants. They are now `Validation(String)`, `SerializeBody(serde_json::Error)`, and `BuildRequest(http::Error)` respectively in `kube::core::request::Error`. * Changed variants of error enums in `kube::runtime` to tuples. Replaced `snafu` with `thiserror`. - [#686](https://github.com/kube-rs/kube/issues/686) * Removed `kube::error::ConfigError` and `kube::Error::Kubeconfig(ConfigError)` - [#696](https://github.com/kube-rs/kube/issues/696) - Error variants related to client auth were moved to a new error `kube::client::AuthError` as described below - Remaining variants were split into `kube::config::{InferConfigError, InClusterError, KubeconfigError}` as described below * Added `kube::client::AuthError` by extracting error variants related to client auth from `kube::ConfigError` and adding more variants to preserve context - [#696](https://github.com/kube-rs/kube/issues/696) * Moved `kube::error::OAuthError` to `kube::client::OAuthError` - [#696](https://github.com/kube-rs/kube/issues/696) * Changed all errors in `kube::client::auth` to `kube::client::AuthError` - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::Error::Auth(kube::client::AuthError)` - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::config::InferConfigError` which is an error from `Config::infer()` and `kube::Error::InferConfig(kube::config::InferConfigError)` - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::config::InClusterError` for errors related to loading in-cluster configuration by splitting `kube::ConfigError` and adding more variants to preserve context. - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::config::KubeconfigError` for errors related to loading kubeconfig by splitting `kube::ConfigError` and adding more variants to preserve context. - [#696](https://github.com/kube-rs/kube/issues/696) * Changed methods of `kube::Config` to return these erorrs instead of `kube::Error` - [#696](https://github.com/kube-rs/kube/issues/696) * Removed `kube::Error::InvalidUri` which was replaced by error variants preserving context, such as `KubeconfigError::ParseProxyUrl` - [#696](https://github.com/kube-rs/kube/issues/696) * Moved all errors from upgrading to a WebSocket connection into `kube::Error::UpgradeConnection(kube::client::UpgradeConnectionError)` - [#696](https://github.com/kube-rs/kube/issues/696)","title":"Refining Errors"},{"location":"changelog/#0632--2021-10-28","text":"kube::runtime::events : fix build and hide module on kubernetes < 1.19 (events/v1 missing there) - #685","title":"0.63.2 / 2021-10-28"},{"location":"changelog/#0631--2021-10-26","text":"kube::runtime::wait::Condition added boolean combinators ( not / and / or ) - #678 kube : fix docs.rs build - #681 via #682","title":"0.63.1 / 2021-10-26"},{"location":"changelog/#0630--2021-10-26","text":"rust edition bumped to 2021 - #664 , #666 , #667 kube::CustomResource derive can now take arbitrary #[kube(k8s_openapi)] style-paths for k8s_openapi , schemars , serde , and serde_json - #675 kube : fix native-tls included when only rustls-tls feature is selected - #673 via #674","title":"0.63.0 / 2021-10-26"},{"location":"changelog/#0620--2021-10-22","text":"kube now re-exports kube-runtime under runtime feature - #651 via #652 no need to keep both kube and kube_runtime in Cargo.toml anymore fixes issues with dependabot / lock-step upgrading change kube_runtime::X import paths to kube::runtime::X when moving to the feature kube::runtime added events module with an event Recorder - #249 via #653 + #662 + #663 kube::runtime::wait::conditions added is_crd_established helper - #659 kube::CustomResource derive can now take an arbitrary #[kube(kube_core)] path for kube::core - #658 kube::core consistently re-exported across crates docs: major overhaul + architecture.md - #416 via #652","title":"0.62.0 / 2021-10-22"},{"location":"changelog/#0610--2021-10-09","text":"kube-core : BREAKING: extend CustomResourceExt trait with ::shortnames method (impl in kube-derive ) - #641 kube-runtime : add wait module to await_condition , and added watch_object to watcher - #632 via #633 kube : add Restart marker trait to allow Api::restart on core workloads - #630 via #635 bump dependencies: tokio-tungstenite , k8s-openapi , schemars , tokio in particular - #643 + #645","title":"0.61.0 / 2021-10-09"},{"location":"changelog/#0600--2021-09-02","text":"kube : support k8s-openapi with v1_22 features - #621 via #622 kube : BREAKING : support for CustomResourceDefinition at v1beta1 now requires an opt-in deprecated-crd-v1beta1 feature - #622 kube-core : add content-type header to requests with body - #626 via #627","title":"0.60.0 / 2021-09-02"},{"location":"changelog/#0590--2021-08-09","text":"BREAKING : bumped k8s-openapi to 0.13.0 - #581 via #616 kube connects to kubernetes via cluster dns when using rustls - #587 via #597 client now works with rustls feature in-cluster - #153 via #597 kube nicer serialization of Kubeconfig - #613 kube-core added serde traits for ApiResource - #590 kube-core added CrdExtensions::crd_name method (implemented by kube-derive ) - #583 kube-core added the HasSpec and HasStatus traits - #605 kube-derive added support to automatically implement the HasSpec and HasStatus traits - #605 kube-runtime fix tracing span hierarchy from applier - #600","title":"0.59.0 / 2021-08-09"},{"location":"changelog/#0581--2021-07-06","text":"kube-runtime : fix non-unix builds - #582","title":"0.58.1 / 2021-07-06"},{"location":"changelog/#0580--2021-07-05","text":"kube : BREAKING : subresource marker traits renamed conjugation: Log , Execute , Attach , Evict (previously Logging , Executable , Attachable , Evictable ) - #536 via #560 kube-derive added #[kube(category)] attr to set CRD categories - #559 kube-runtime added finalizer helper #291 via #475 kube-runtime added tracing for why reconciliations happened #457 via #571 kube-runtime added Controller::reconcile_all_on to allow scheduling all objects for reconciliation #551 via #555 kube-runtime added Controller::graceful_shutdown_on for shutting down the Controller while waiting for running reconciliations to finish - #552 via #573 BREAKING: controller::applier now starts a graceful shutdown when the queue terminates BREAKING: scheduler now shuts down immediately when requests terminates, rather than waiting for the pending reconciliations to drain kube-runtime added tracking for reconciliation reason Added: Controller::owns_with and Controller::watches_with to pass a dyntype argument for dynamic Api s - #575 BREAKING: Controller::owns signature changed to not allow DynamicType s BREAKING: controller::trigger_* now returns a ReconcileRequest rather than ObjectRef . The ObjectRef can be accessed via the obj_ref field","title":"0.58.0 / 2021-07-05"},{"location":"changelog/#known-issues","text":"Api::replace can fail to unset list values with k8s-openapi 0.12 #581","title":"Known Issues"},{"location":"changelog/#0570--2021-06-16","text":"kube : custom clients now respect default namespaces - fixes #534 via #544 BREAKING: custom clients via Client::new must pass config.default_namespace as 2 nd arg kube : Added CustomResourceExt trait for kube-derive - #497 via #545 BREAKING: kube-derive users must import kube::CustomResourceExt (or kube::core::crd::v1beta1::CustomResourceExt if using legacy #[kube(apiextensions = \"v1beta1\")] ) to use generated methods Foo::crd or Foo::api_resource BREAKING: k8s_openapi bumped to 0.12.0 - #531 Generated structs simplified + Resource trait expanded Adds support for kubernetes v1_21 Contains bugfix for kubernetes#102159 kube resource plurals is no longer inferred from k8s-openapi structs - #284 via #556 BREAKING: kube::Resource trait now requires a plural implementation","title":"0.57.0 / 2021-06-16"},{"location":"changelog/#known-issues_1","text":"Api::replace can fail to unset list values with k8s-openapi 0.12 #581","title":"Known Issues"},{"location":"changelog/#0560--2021-06-05","text":"kube : added Api::default_namespaced - #209 via #534 kube : added config feature - #533 via #535 kube : BREAKING: moved client::discovery module to kube::discovery and rewritten module #538 discovery : added oneshot helpers for quick selection of recommended resources / kinds #538 discovery : moved ApiResource and ApiCapabilities (result of discovery) to kube_core::discovery BREAKING: removed internal ApiResource::from_apiresource kube::Client is now configurable with layers using tower-http #539 via #540 three new examples added: custom_client , custom_client_tls and custom_client_trace Big feature streamlining, big service and layer restructuring, dependency restructurings Changes can hit advanced users, but unlikely to hit base use cases with Api and Client . In depth changes broken down below:","title":"0.56.0 / 2021-06-05"},{"location":"changelog/#tls-enhancements","text":"Add kube::client::ConfigExt extending Config for custom Client . This includes methods to configure TLS connection when building a custom client #539 native-tls : Config::native_tls_https_connector and Config::native_tls_connector rustls-tls : Config::rustls_https_connector and Config::rustls_client_config Remove the requirement of having native-tls or rustls-tls enabled when client is enabled. Allow one, both or none. When both, the default Service will use native-tls because of #153 . rustls can be still used with a custom client. Users will have an option to configure TLS at runtime. When none, HTTP connector is used. Remove TLS features from kube-runtime BREAKING : Features must be removed if specified Remove client feature from native-tls and rust-tls features config + native-tls / rustls-tls can be used independently, e.g., to create a simple HTTP client BREAKING : client feature must be added if default-features = false","title":"TLS Enhancements"},{"location":"changelog/#layers","text":"ConfigExt::base_uri_layer ( BaseUriLayer ) to set cluster URL (#539) ConfigExt::auth_layer that returns optional layer to manage Authorization header (#539) gzip : Replaced custom decompression module with DecompressionLayer from tower-http (#539) Replaced custom LogRequest with TraceLayer from tower-http (#539) Request body is no longer shown Basic and Bearer authentication using AddAuthorizationLayer (borrowing from https://github.com/tower-rs/tower-http/pull/95 until released) BREAKING : Remove headers from Config . Injecting arbitrary headers is now done with a layer on a custom client.","title":"Layers"},{"location":"changelog/#dependency-changes","text":"Remove static_assertions since it's no longer used Replace tokio_rustls with rustls and webpki since we're not using tokio_rustls directly Replace uses of rustls::internal::pemfile with rustls-pemfile Remove url and always use http::Uri BREAKING : Config::cluster_url is now http::Uri BREAKING : Error::InternalUrlError(url::ParseError) and Error::MalformedUrl(url::ParseError) replaced by Error::InvalidUri(http::uri::InvalidUri)","title":"Dependency Changes"},{"location":"changelog/#0550--2021-05-21","text":"kube : client feature added (default-enabled) - #528 kube : PatchParams force now only works with Patch::Apply #528 kube : api discovery module now uses a new ApiResource struct #495 + #482 kube : api BREAKING: DynamicObject + Object now takes an ApiResource rather than a GroupVersionKind kube : api BREAKING: discovery module's Group renamed to ApiGroup kube : client BREAKING: kube::client::Status moved to kube::core::Status (accidental, re-adding in 0.56) kube-core crate factored out of kube to reduce dependencies - #516 via #517 + #519 + #522 + #528 + #530 kube : kube::Service removed to allow kube::Client to take an abritrary Service<http::Request<hyper::Body>> - #532","title":"0.55.0 / 2021-05-21"},{"location":"changelog/#0540--2021-05-19","text":"yanked 30 minutes after release due to #525 changes lifted to 0.55.0","title":"0.54.0 / 2021-05-19"},{"location":"changelog/#0530--2021-05-15","text":"kube : admission controller module added under feature - #477 via #484 + fixes in #488 #498 #499 + #507 + #509 kube : config parsing of pem blobs now resilient against missing newlines - #504 via #505 kube : discovery module added to simplify dynamic api usage - #491 kube : api BREAKING: DynamicObject::namespace renamed to ::within - #502 kube : api BREAKING: added ResourceExt trait moving the getters from Resource trait - #486 kube : api added a generic interface for subresources via Request - #487 kube : api fix bug in PatchParams::dry_run not being serialized correctly - #511","title":"0.53.0 / 2021-05-15"},{"location":"changelog/#0530-migration-guide","text":"The most likely issue you'll run into is from kube when using Resource trait which has been split: +use kube::api::ResouceExt; - let name = Resource::name(&foo); - let ns = Resource::namespace(&foo).expect(\"foo is namespaced\"); + let name = ResourceExt::name(&foo); + let ns = ResourceExt::namespace(&foo).expect(\"foo is namespaced\");","title":"0.53.0 Migration Guide"},{"location":"changelog/#0520--2021-03-31","text":"kube-derive : allow overriding #[kube(plural)] and #[kube(singular)] - #458 via #463 kube : added tracing instrumentation for io operations in kube::Api - #455 kube : DeleteParams 's Preconditions is now public - #459 via #460 kube : remove dependency on duplicate derive_accept_key for ws - #452 kube : Properly verify websocket keys in ws handshake - #447 kube : BREAKING: removed optional, and deprecated runtime module - #454 kube : BREAKING: ListParams bookmarks default enabled - #226 via #445 renames member ::allow_bookmarks to ::bookmarks ::default() sets bookmark to true to avoid bad bad defaults #219 method ::allow_bookmarks() replaced by ::disable_bookmarks() kube : DynamicObject and GroupVersionKind introduced for full dynamic object support kube-runtime : watchers/reflectors/controllers can be used with dynamic objects from api discovery kube : Pluralisation now only happens for k8s_openapi objects by default #481 inflector dependency removed #471 added internal pluralisation helper for k8s_openapi objects kube : BREAKING: Restructuring of low level Resource request builder #474 Resource renamed to Request and requires only a path_url to construct kube : BREAKING: Mostly internal Meta trait revamped to support dynamic types Meta renamed to kube::Resource to mimic k8s_openapi::Resource #478 The trait now takes an optional associated type for runtime type info: DynamicType #385 Api::all_with + Api::namespaced_with added for querying with dynamic families see dynamic_watcher + dynamic_api for example usage kube-runtime : BREAKING: lower level interface changes as a result of kube::api::Meta trait: THESE SHOULD NOT AFFECT YOU UNLESS YOU ARE IMPLEMENTING / CUSTOMISING LOW LEVEL TYPES DIRECTLY ObjectRef now generic over kube::Resource rather than RuntimeResource reflector::{Writer, Store} takes a kube::Resource rather than a k8s_openapi::Resource kube-derive : BREAKING: Generated type no longer generates k8s-openapi traits This allows correct pluralisation via #[kube(plural = \"mycustomplurals\")] #467 via #481","title":"0.52.0 / 2021-03-31"},{"location":"changelog/#0520-migration-guide","text":"While we had a few breaking changes. Most are to low level internal interfaces and should not change much, but some changes you might need to make:","title":"0.52.0 Migration Guide"},{"location":"changelog/#kube","text":"if using the old, low-level kube::api::Resource , please consider the easier kube::Api , or look at tests in request.rs or typed.rs if you need the low level interface search replace kube::api::Meta with kube::Resource if used - trait was renamed if implementing the trait, add type DynamicType = (); to the impl remove calls to ListParams::allow_bookmarks (allow default) handle WatchEvent::Bookmark or set ListParams::disable_bookmarks() look at examples if replacing the long deprecated legacy runtime","title":"kube"},{"location":"changelog/#kube-derive","text":"The following constants from k8s_openapi::Resource no longer exist. Please use kube::Resource and: - replace Foo::KIND with Foo::kind(&()) - replace Foo::GROUP with Foo::group(&()) - replace Foo::VERSION with Foo::version(&()) - replace Foo::API_VERSION with Foo::api_version(&())","title":"kube-derive"},{"location":"changelog/#0510--2021-02-28","text":"kube Config now allows arbirary extension objects - #425 kube Config now allows multiple yaml documents per kubeconfig - #440 via #441 kube-derive now more robust and is using darling - #435 docs improvements to patch + runtime","title":"0.51.0 / 2021-02-28"},{"location":"changelog/#0501--2021-02-17","text":"bug: fix oidc auth provider - #424 via #419","title":"0.50.1 / 2021-02-17"},{"location":"changelog/#0500--2021-02-10","text":"feat: added support for stacked kubeconfigs - #132 via #411 refactor: authentication logic moved out of kube::config and into into kube::service - #409 BREAKING: Config::get_auth_header removed refactor: remove hyper dependency from kube::api - #410 refactor: kube::Service simpler auth and gzip handling - #405 + #408","title":"0.50.0 / 2021-02-10"},{"location":"changelog/#0490--2021-02-08","text":"dependency on reqwest + removed in favour of hyper + tower #394 refactor: kube::Client now uses kube::Service (a tower::Service<http::Request<hyper::Body>> ) instead of reqwest::Client to handle all requests refactor: kube::Client now uses a tokio_util::codec for internal buffering refactor: async-tungstenite ws feature dependency replaced with tokio-tungstenite . WebSocketStream is now created from a connection upgraded with hyper refactor: oauth2 module for GCP OAuth replaced with optional tame-oauth dependency BREAKING: GCP OAuth is now opt-in ( oauth feature). Note that GCP provider with command based token source is supported by default. BREAKING: Gzip decompression is now opt-in ( gzip feature) because Kubernetes does not have compression enabled by default yet and this feature requires extra dependencies. #399 BREAKING: Client::new now takes a Service instead of Config #400 . Allows custom service for features not supported out of the box and testing. To create a Client from Config , use Client::try_from instead. BREAKING: Removed Config::proxy . Proxy is no longer supported out of the box, but it should be possible by using a custom Service. fix: Refreshable token from auth provider not refreshing fix: Panic when loading config with non-GCP provider #238 feat: subresource support added for Evictable types (marked for Pod ) - #393 kube : subresource marker traits renamed to Loggable , Executable , Attachable (previously LoggingObject , ExecutingObject , AttachableObject ) - #395 examples showcasing kubectl cp like behaviour #381 via #392","title":"0.49.0 / 2021-02-08"},{"location":"changelog/#0480--2021-01-23","text":"bump k8s-openapi to 0.11.0 - #388 breaking: kube : no longer necessary to serialize patches yourself - #386 PatchParams removes PatchStrategy Api::patch* methods now take an enum Patch type optional jsonpatch feature added for Patch::Json","title":"0.48.0 / 2021-01-23"},{"location":"changelog/#0470--2021-01-06","text":"chore: upgrade tokio to 1.0 - #363 BREAKING: This requires the whole application to upgrade to tokio 1.0 and reqwest to 0.11.0 docs: fix broken documentation in kube 0.46.0 #367 bug: kube : removed panics from ws features, fix rustls support + improve docs #369 via #370 + #373 bug: AttachParams now fixes owned method chaining (slightly breaks from 0.46 if using &mut ref before) - #364 feat: AttachParams::interactive_tty convenience method added - #364 bug: fix Runner (and thus Controller and applier ) not waking correctly when starting new tasks - #375","title":"0.47.0 / 2021-01-06"},{"location":"changelog/#0461--2021-01-06","text":"maintenance release for 0.46 (last supported tokio 0.2 release) from tokio02 branch bug backport: fix Runner (and thus Controller and applier ) not waking correctly when starting new tasks - #375","title":"0.46.1 / 2021-01-06"},{"location":"changelog/#0460--2021-01-02","text":"feat: kube now has optional websocket support with async_tungstenite under ws and ws-*-tls features #360 feat: AttachableObject marker trait added and implemented for k8s_openapi::api::core::v1::Pod #360 feat: AttachParams added for Api::exec and Api::attach for AttachableObject s #360 examples: pod_shell , pod_attach , pod_exec demonstrating the new features #360","title":"0.46.0 / 2021-01-02"},{"location":"changelog/#0450--2020-12-26","text":"feat: kube-derive now has a default enabled schema feature allows opting out of schemars dependency for handwriting crds - #355 breaking: kube-derive attr struct_name renamed to struct - #359 docs: improvements on kube , kube-runtime , kube-derive","title":"0.45.0 / 2020-12-26"},{"location":"changelog/#0440--2020-12-23","text":"feat: kube-derive now generates openapi v3 schemas and is thus usable with v1 CustomResourceDefinition - #129 and #264 via #348 BREAKING: kube-derive types now require JsonSchema derived via schemars libray (not breaking if going to 0.45.0) feat: kube_runtime::controller : now reconciles objects in parallel - #346 BREAKING: kube_runtime::controller::applier now requires that the reconciler 's Future is Unpin , Box::pin it or submit it to a runtime if this is not acceptable BREAKING: kube_runtime::controller::Controller now requires that the reconciler 's Future is Send + 'static , use the low-level applier interface instead if this is not acceptable bug: kube-runtime : removed accidentally included k8s-openapi default features (you have to opt in to them yourself) feat: kube : TypeMeta now derives additionally Debug, Eq, PartialEq, Hash bump: k8s-openapi to 0.10.0 - #330 bump: serde_yaml - #349 bump: dirs to dirs-next - #340","title":"0.44.0 / 2020-12-23"},{"location":"changelog/#0430--2020-10-08","text":"bug: kube-derive attr #[kube(shortname)] now working correctly bug: kube-derive now working with badly cased existing types - #313 missing: kube now correctly exports config::NamedAuthInfo - #323 feat: kube : expose Config::get_auth_header for istio use cases - #322 feat: kube : local config now tackles gcloud auth exec params - #328 and #84 kube-derive now actually requires GVK (in particular #[kube(kind = \"Foo\")] which we sometimes inferred earlier, despite documenting the contrary)","title":"0.43.0 / 2020-10-08"},{"location":"changelog/#0420--2020-09-10","text":"bug: kube-derive 's Default derive now sets typemeta correctly - #315 feat: ListParams now supports continue_token and limit - #320","title":"0.42.0 / 2020-09-10"},{"location":"changelog/#0410--2020-09-10","text":"yanked release. failed publish.","title":"0.41.0 / 2020-09-10"},{"location":"changelog/#0400--2020-08-17","text":"DynamicResource::from_api_resource added to allow apiserver returned resources - #305 via #301 Client::list_api_groups added Client::list_ap_group_resources added Client::list_core_api_versions added Client::list_core_api_resources added kube::DynamicResource exposed at top level Bug: PatchParams::default_apply() now requires a manager and renamed to PatchParams::apply(manager: &str) for #300 Bug: DeleteParams no longer missing for Api::delete_collection - #53 Removed paramter ListParams::include_uninitialized deprecated since 1.14 Added optional PostParams::field_manager was missing for Api::create case","title":"0.40.0 / 2020-08-17"},{"location":"changelog/#0390--2020-08-05","text":"Bug: ObjectRef tweak in kube-runtime to allow controllers triggering across cluster and namespace scopes - #293 via #294 Feature: kube now has a derive feature which will re-export kube::CustomResource from kube-derive::CustomResource . Examples: revamp examples for kube-runtime - #201","title":"0.39.0 / 2020-08-05"},{"location":"changelog/#0380--2020-07-23","text":"Marked kube::runtime module as deprecated - #281 Config::timeout can now be overridden to None (with caveats) #280 Bug: reflector stores could have multiple copies inside datastore - #286 dashmap backend Store driver downgraded - #286 Store::iter temporarily removed Bug: Specialize WatchEvent::Bookmark so they can be deserialized - #285 Docs: Tons of docs for kube-runtime","title":"0.38.0 / 2020-07-23"},{"location":"changelog/#0370--2020-07-20","text":"Bump k8s-openapi to 0.9.0 All runtime components now require Sync objects reflector/watcher/Controller streams can be shared in threaded environments","title":"0.37.0 / 2020-07-20"},{"location":"changelog/#0360--2020-07-19","text":"https://gitlab.com/teozkr/kube-rt/ merged in for a new kube-runtime crate #258 Controller<K> added ( #148 via #258 ) Reflector api redesigned ( #102 via #258 ) Migration release for Informer -> watcher + Reflector -> reflector kube::api::CustomResource removed in favour of kube::api::Resource::dynamic CrBuilder removed in favour of DynamicResource (with new error handling) support level bumped to beta","title":"0.36.0 / 2020-07-19"},{"location":"changelog/#0351--2020-06-18","text":"Fix in-cluster Client when using having multiple certs in the chain - #251","title":"0.35.1 / 2020-06-18"},{"location":"changelog/#0350--2020-06-15","text":"Config::proxy support added - #246 PartialEq can be derived with kube-derive - #242 Windows builds no longer clashes with runtime - #240 Rancher hosts (with path specifiers) now works - #244","title":"0.35.0 / 2020-06-15"},{"location":"changelog/#0340--2020-05-08","text":"Bump k8s-openapi to 0.8.0 Config::from_cluster_env <- renamed from Config::new_from_cluster_env Config::from_kubeconfig <- renamed from Config::new_from_kubeconfig Config::from_custom_kubeconfig added - #236 Majorly overhauled error handlind in config module - #237","title":"0.34.0 / 2020-05-08"},{"location":"changelog/#0330--2020-04-27","text":"documentation fixes for Api::patch Config: add automatic token refresh - #72 / #224 / #234","title":"0.33.0 / 2020-04-27"},{"location":"changelog/#0321--2020-04-15","text":"add missing tokio signal feature as a dependency upgrade all dependencies, including minor bumps to rustls and base64","title":"0.32.1 / 2020-04-15"},{"location":"changelog/#0320--2020-04-10","text":"Major config + client module refactor Config is the new Configuration struct Client is now just a configured reqwest::Client plus a reqwest::Url implement From<Config> for reqwest::ClientBuilder implement TryFrom<Config> for Client Client::try_default or Client::new now recommended constructors People parsing ~/.kube/config must use the KubeConfig struct instead Reflector<K> now only takes an Api<K> to construct (.params method) Informer<K> now only takes an Api<K> to construct (.params method) Informer::init_from -> Informer::set_version Reflector now self-polls #151 + handles signals #152 Reflector::poll made private in favour of Reflector::run Api::watch no longer filters out error events ( next -> try_next ) Api::watch returns Result<WatchEvent> rather than WatchEvent WatchEvent::Bookmark added to enum ListParams::allow_bookmarks added PatchParams::default_apply ctor added PatchParams builder mutators: ::force and ::dry_run added","title":"0.32.0 / 2020-04-10"},{"location":"changelog/#0310--2020-03-27","text":"Expose config::Configuration at root level Add Configuration::infer as a recommended constructor Rename client::APIClient to client::Client Expose client::Client at root level Client now implements From<Configuration> Added comprehensive documentation on Api Rename config::KubeConfigLoader -> config::ConfigLoader removed futures-timer dependency for tokio (feature=timer)","title":"0.31.0 / 2020-03-27"},{"location":"changelog/#0300--2020-03-17","text":"Fix #[kube(printcolumn)] when #[kube(apiextensions = \"v1beta1\")] Fix #[kube(status)] causing serializes of empty optional statuses","title":"0.30.0 / 2020-03-17"},{"location":"changelog/#0290--2020-03-12","text":"Api::log -> Api::logs (now matches Resource::logs ) Object<FooSpec, FooStatus> back for ad-hoc ser/de kube-derive now derives Debug (requires Debug on spec struct) kube-derive now allows multiple derives per file Api::create now takes data K rather than bytes Api::replace now takes data K rather than bytes (note that Resource::create and Resource::replace still takes bytes)","title":"0.29.0 / 2020-03-12"},{"location":"changelog/#0281--2020-03-07","text":"#[derive(CustomResource)] now implements ::new on the generated Kind derived Kind now properly contains TypeMeta - #170","title":"0.28.1 / 2020-03-07"},{"location":"changelog/#0280--2020-03-05","text":"RawApi removed -> Resource added Resource implements k8s_openapi::Resource ALL OBJECTS REMOVED -> Depening on light version of k8s-openapi now NB: should generally just mean a few import changes (+casings / unwraps) openapi feature removed (light dependency mandatory now) LIBRARY WORKS WITH ALL k8s_openapi KUBERNETES OBJECTS KubeObject trait removed in favour of Meta trait Object<FooSpec, FooStatus> removed -> types implementing k8s_openapi::Resource required instead kube-derive crate added to derive this trait + other kubebuilder like codegen","title":"0.28.0 / 2020-03-05"},{"location":"changelog/#0270--2020-02-26","text":"Reflector + Informer moved from kube::api to kube::runtime Informer now resets the version to 0 rather than dropping events - #134 Removed Informer::init , since it is now a no-op when building the Informer Downgrade spurious log message when using service account auth","title":"0.27.0 / 2020-02-26"},{"location":"changelog/#0260--2020-02-25","text":"Fix a large percentage of EOFs from watches #146 => default timeout down to 290s from 300s => Reflector now re-lists a lot less #146 Fix decoder panic with async-compression (probably) #144 Informer::poll can now be used with TryStream Exposed Config::read and Config::read_from - #124 Fix typo on Api::StatefulSet Fix typo on Api::Endpoints Add Api::v1CustomResourceDefinition when on k8s >= 1.17 Renamed Void to NotUsed","title":"0.26.0 / 2020-02-25"},{"location":"changelog/#0250--2020-02-09","text":"initial rustls support #114 (some local kube config issues know #120 ) crate does better version checking against openapi features - #106 initial log_stream support - #109","title":"0.25.0 / 2020-02-09"},{"location":"changelog/#0240--2020-01-26","text":"Add support for ServiceAccount, Role, ClusterRole, RoleBinding, Endpoint - #113 + #111 Upgrade k8s-openapi to 0.7 => breaking changes: https://github.com/Arnavion/k8s-openapi/blob/master/CHANGELOG.md#v070-2020-01-23","title":"0.24.0 / 2020-01-26"},{"location":"changelog/#0230--2019-12-31","text":"Bump tokio and reqwest to 0.2 and 0.10 Fix bug in log fetcher - #107 Temporarily allow invalid certs when testing on macosx - #105","title":"0.23.0 / 2019-12-31"},{"location":"changelog/#0222--2019-12-04","text":"Allow sharing Reflectors between threads - #97 Fix Reflector pararall lock issue ( poll no longer blocks state )","title":"0.22.2 / 2019-12-04"},{"location":"changelog/#0221--2019-11-30","text":"Improve Reflector reset algorithm (clear history less)","title":"0.22.1 / 2019-11-30"},{"location":"changelog/#0220--2019-11-29","text":"Default watch timeouts changed to 300s everywhere This increases efficiency of Informers and Reflectors by keeping the connection open longer. However, if your Reflector relies on frequent polling you can set timeout or hide the poll() in a different context so it doesn't block your main work Internal RwLock changed to a futures::Mutex for soundness / proper non-blocking - #94 blocking Reflector::read() renamed to async Reflector::state() Expose metadata.creation_timestamp and .deletion_timestamp (behind openapi flag) - #93","title":"0.22.0 / 2019-11-29"},{"location":"changelog/#0210--2019-11-29","text":"All watch calls returns a stream of WatchEvent - #92 Informer::poll now returns a stream - #92","title":"0.21.0 / 2019-11-29"},{"location":"changelog/#0201--2019-11-21","text":"ObjectList now implements Iterator - #91 openapi feature no longer accidentally hardcoded to v1.15 feature - #90","title":"0.20.1 / 2019-11-21"},{"location":"changelog/#0190--2019-11-15","text":"kube::Error is now a proper error enum and not a Fail impl (thiserror) soft-tokio dependency removed for futures-timer gzip re-introduced","title":"0.19.0 / 2019-11-15"},{"location":"changelog/#0181--2019-11-11","text":"Fix unpinned gzip dependency breakage - #87","title":"0.18.1 / 2019-11-11"},{"location":"changelog/#0180--2019-11-07","text":"api converted to use async/await with 1.39.0 (primitively) hyper upgraded to 0.10-alpha synchronous sleep replaced with tokio timer Log trait removed in favour of internal marker trait","title":"0.18.0 / 2019-11-07"},{"location":"changelog/#0170--2019-10-22","text":"Add support for oidc providerss with auth-provider w/o access-token - #70 Bump most dependencies to more recent versions Expose custom client creation Added support for v1beta1Ingress Expose incluster_config::load_default_ns - #74","title":"0.17.0 / 2019-10-22"},{"location":"changelog/#0161--2019-08-09","text":"Add missing uid field on ObjectMeta::ownerReferences","title":"0.16.1 / 2019-08-09"},{"location":"changelog/#0160--2019-08-09","text":"Add Reflector::get and Reflector::get_within as cheaper getters Add support for OpenShift kube configs with multiple CAs - via #64 Add missing ObjectMeta::ownerReferences Reduced memory consumption during compile with k8s-openapi@0.5.1 - #62","title":"0.16.0 / 2019-08-09"},{"location":"changelog/#0151--2019-08-18","text":"Fix compile issue on 1.37.0 with Utc serialization Fix Void not having Serialize derive","title":"0.15.1 / 2019-08-18"},{"location":"changelog/#0150--2019-08-11","text":"Added support for v1Job resources - via #58 Added support for v1Namespace , v1DaemonSet , v1ReplicaSet , v1PersistentVolumeClaim , v1PersistentVolume , v1ResourceQuota , v1HorizontalPodAutoscaler - via #59 Added support for v1beta1CronJob , v1ReplicationController , v1VolumeAttachment , v1NetworkPolicy - via #60 k8s-openapi optional dependency bumped to 0.5.0 (for kube 1.14 structs)","title":"0.15.0 / 2019-08-11"},{"location":"changelog/#0140--2019-08-03","text":"Reflector::read now returns a Vec<K>`` rather than a Vec<(name, K)>`: This fixes an unsoundness bug internally - #56 via @gnieto","title":"0.14.0 / 2019-08-03"},{"location":"changelog/#0130--2019-07-22","text":"Experimental oauth2 support for some providers - via #44 : a big cherry-pick from various prs upstream originally for GCP EKS works with setup in https://github.com/kube-rs/kube/pull/20#issuecomment-511767551","title":"0.13.0 / 2019-07-22"},{"location":"changelog/#0120--2019-07-18","text":"Added support for Log subresource - via #50 Added support for v1ConfigMap with example - via #49 Demoted some spammy info messages from Reflector","title":"0.12.0 / 2019-07-18"},{"location":"changelog/#0110--2019-07-10","text":"Added PatchParams with PatchStrategy to allow arbitrary patch types - #24 via @ragne Event renamed to v1Event to match non-slowflake type names v1Service support added Added v1Secret snowflake type and a secret_reflector example","title":"0.11.0 / 2019-07-10"},{"location":"changelog/#0100--2019-06-03","text":"Api<P, U> is now Api<K> for some KubeObject K: Big change to allow snowflake objects ( #35 ) - but also slightly nicer You want aliases type Pod = Object<PodSpec, PodStatus> This gives you the required KubeObject trait impl for free Added Event native type to prove snowflakes can be handled - #35 ApiStatus renamed to Status to match kube api conventions #36 Rename Metadata to ObjectMeta #36 Added ListMeta for ObjectList and Status #36 Added TypeMeta object which is flattened onto Object , so: o.types.kind rather than o.kind o.types.version rather than o.version","title":"0.10.0 / 2019-06-03"},{"location":"changelog/#090--2019-06-02","text":"Status subresource api commands added to Api : patch_status get_status replace_status ^ See crd_openapi or crd_api examples Scale subresource commands added to Api : patch_scale get_scale replace_scale ^ See crd_openapi example","title":"0.9.0 / 2019-06-02"},{"location":"changelog/#080--2019-05-31","text":"Typed Api variant called OpenApi introduced (see crd_openapi example) Revert client.request return type change (back to response only from pre-0.7.0 #28 ) delete now returns `Either , ApiStatus> - for bug #32 delete_collection now returns `Either >, ApiStatus> - for bug #32 Informer::new renamed to Informer::raw Reflector::new renamed to Reflector::raw Reflector::new + Informer::new added for \"openapi\" compile time feature (does not require specifying the generic types)","title":"0.8.0 / 2019-05-31"},{"location":"changelog/#070--2019-05-27","text":"Expose list/watch parameters #11 Many API struct renames: ResourceMap -> Cache Resource -> Object ResourceList -> ObjectList ApiResource -> Api ResourceType has been removed in favour of Api::v1Pod() say Object::status now wrapped in an Option (not present everywhere) ObjectList exposed Major API overhaul to support generic operations on Object Api can be used to perform generic actions on resources: create get delete watch list patch replace get_scale (when scale subresource exists) patch_scale (ditto) replace_scale (ditto) get_status (when status subresource exists) patch_status (ditto) replace_status (ditto) crd_api example added to track the action api Bunch of generic parameter structs exposed for common operations: ListParams exposed DeleteParams exposed PostParams exposed Errors from Api exposed in kube::Error : Error::api_error -> Option<ApiError> exposed Various other error types also in there (but awkward setup atm) client.request now returns a tuple (T, StatusCode) (before only T )","title":"0.7.0 / 2019-05-27"},{"location":"changelog/#060--2019-05-12","text":"Expose getter Informer::version Exose ctor Informer::from_version Expose more attributes in Metadata Informer::reset convenience method added Informer::poll no longer returns events straight an Informer now caches WatchEvent elements into an internal queue Informer::pop pops a single element from its internal queue Reflector::refresh renamed to Reflector::reset (matches Informer ) Void type added so we can use Reflector<ActualSpec, Void> removes need for Spec/Status structs: ReflectorSpec , ReflectorStatus removed InformerSpec , InformerStatus removed ResourceSpecMap , ResourceStatusMap removed WatchEvents removed WatchEvent exposed, and now wraps `Resource ``","title":"0.6.0 / 2019-05-12"},{"location":"changelog/#050--2019-05-09","text":"added Informer struct dedicated to handling events Reflectors no longer cache events - see #6","title":"0.5.0 / 2019-05-09"},{"location":"changelog/#040--2019-05-09","text":"ResourceMap now contains the full Resource struct rather than a tuple as the value. => value.metadata is available in the cache. Reflectors now also cache events to allow apps to handle them","title":"0.4.0 / 2019-05-09"},{"location":"changelog/#030--2019-05-09","text":"Named trait removed (inferring from metadata.name now) Reflectors now take two type parameters (unless you use ReflectorSpec or ReflectorStatus ) - see examples for usage Native kube types supported via ApiResource Some native kube resources have easy converters to ApiResource","title":"0.3.0 / 2019-05-09"},{"location":"contributing/","text":"Contributing Guide # This document describes the requirements for committing to this repository. Developer Certificate of Origin (DCO) # In order to contribute to this project, you must sign each of your commits to attest that you have the right to contribute that code. This is done with the -s / --signoff flag on git commit . More information about DCO can be found here Pull Request Management # All code that is contributed to kube-rs must go through the Pull Request (PR) process. To contribute a PR, fork this project, create a new branch, make changes on that branch, and then use GitHub to open a pull request with your changes. Every PR must be reviewed by at least one Maintainer of the project. Once a PR has been marked \"Approved\" by a Maintainer (and no other Maintainer has an open \"Rejected\" vote), the PR may be merged. While it is fine for non-maintainers to contribute their own code reviews, those reviews do not satisfy the above requirement. Code of Conduct # This project has adopted the CNCF Code of Conduct . Rust Guidelines # Channel : Code is built and tested using the stable channel of Rust, but documented and formatted with nightly * Formatting : To format the codebase, run just fmt Documentation To check documentation, run just doc Testing : To run tests, run just test and see below. For a list of tooling that we glue together everything see TOOLS.md . Testing # We have 3 classes of tests. Unit tests & Documentation Tests Integration tests (requires Kubernetes) End to End tests (requires Kubernetes) The last two will try to access the Kubernetes cluster that is your current-context ; i.e. via your local KUBECONFIG evar or ~/.kube/config file. The easiest way set up a minimal Kubernetes cluster for these is with k3d ( just k3d ). Unit Tests & Documentation Tests # Most unit/doc tests are run from cargo test --lib --doc --all , but because of feature-sets, and examples, you will need a couple of extra invocations to replicate our CI. For the complete variations, run the just test target in the justfile . All public interfaces must be documented, and most should have minor documentation examples to show usage. Integration Tests # Slower set of tests within the crates marked with an #[ignore] attribute. These WILL try to modify resources in your current cluster Most integration tests are run with cargo test --all --lib -- --ignored , but because of feature-sets, you will need a few invocations of these to replicate our CI. See just test-integration End to End Tests # We have a small set of e2e tests that tests difference between in-cluster and local configuration. These tests are the heaviest tests we have because they require a full docker build , image import (or push/pull flow), yaml construction, and kubectl usage to verify that the outcome was sufficient. To run E2E tests, use (or follow) just e2e as appropriate. Test Guidelines # When to add a test # All public interfaces should have doc tests with examples for docs.rs . When adding new non-trivial pieces of logic that results in a drop in coverage you should add a test. Cross-reference with the coverage build and go to your branch. Coverage can also be run locally with cargo tarpaulin at project root. This will use our tarpaulin.toml config, and will run both unit and integration tests. What type of test # Unit tests MUST NOT try to contact a Kubernetes cluster Doc tests MUST be marked as no_run when they need to contact a Kubernetes cluster Integration tests MUST NOT be used when a unit test is sufficient Integration tests MUST NOT assume existence of non-standard objects in the cluster Integration tests MUST NOT cross-depend on other unit tests completing (and installing what you need) E2E tests MUST NOT be used where an integration test is sufficient In general: use the least powerful method of testing available to you: use unit tests in kube-core use unit tests in kube-client (and in rare cases integration tests) use unit tests in kube-runtime (and occassionally integration tests) use e2e tests when testing differences between in-cluster and local configuration Support # Documentation # The high-level architecture document is written for contributors. Contact # You can ask general questions / share ideas / query the community at the kube-rs discussions forum . You can reach the maintainers of this project at #kube channel on the Tokio discord.","title":"Contributing"},{"location":"contributing/#contributing-guide","text":"This document describes the requirements for committing to this repository.","title":"Contributing Guide"},{"location":"contributing/#developer-certificate-of-origin-dco","text":"In order to contribute to this project, you must sign each of your commits to attest that you have the right to contribute that code. This is done with the -s / --signoff flag on git commit . More information about DCO can be found here","title":"Developer Certificate of Origin (DCO)"},{"location":"contributing/#pull-request-management","text":"All code that is contributed to kube-rs must go through the Pull Request (PR) process. To contribute a PR, fork this project, create a new branch, make changes on that branch, and then use GitHub to open a pull request with your changes. Every PR must be reviewed by at least one Maintainer of the project. Once a PR has been marked \"Approved\" by a Maintainer (and no other Maintainer has an open \"Rejected\" vote), the PR may be merged. While it is fine for non-maintainers to contribute their own code reviews, those reviews do not satisfy the above requirement.","title":"Pull Request Management"},{"location":"contributing/#code-of-conduct","text":"This project has adopted the CNCF Code of Conduct .","title":"Code of Conduct"},{"location":"contributing/#rust-guidelines","text":"Channel : Code is built and tested using the stable channel of Rust, but documented and formatted with nightly * Formatting : To format the codebase, run just fmt Documentation To check documentation, run just doc Testing : To run tests, run just test and see below. For a list of tooling that we glue together everything see TOOLS.md .","title":"Rust Guidelines"},{"location":"contributing/#testing","text":"We have 3 classes of tests. Unit tests & Documentation Tests Integration tests (requires Kubernetes) End to End tests (requires Kubernetes) The last two will try to access the Kubernetes cluster that is your current-context ; i.e. via your local KUBECONFIG evar or ~/.kube/config file. The easiest way set up a minimal Kubernetes cluster for these is with k3d ( just k3d ).","title":"Testing"},{"location":"contributing/#unit-tests--documentation-tests","text":"Most unit/doc tests are run from cargo test --lib --doc --all , but because of feature-sets, and examples, you will need a couple of extra invocations to replicate our CI. For the complete variations, run the just test target in the justfile . All public interfaces must be documented, and most should have minor documentation examples to show usage.","title":"Unit Tests &amp; Documentation Tests"},{"location":"contributing/#integration-tests","text":"Slower set of tests within the crates marked with an #[ignore] attribute. These WILL try to modify resources in your current cluster Most integration tests are run with cargo test --all --lib -- --ignored , but because of feature-sets, you will need a few invocations of these to replicate our CI. See just test-integration","title":"Integration Tests"},{"location":"contributing/#end-to-end-tests","text":"We have a small set of e2e tests that tests difference between in-cluster and local configuration. These tests are the heaviest tests we have because they require a full docker build , image import (or push/pull flow), yaml construction, and kubectl usage to verify that the outcome was sufficient. To run E2E tests, use (or follow) just e2e as appropriate.","title":"End to End Tests"},{"location":"contributing/#test-guidelines","text":"","title":"Test Guidelines"},{"location":"contributing/#when-to-add-a-test","text":"All public interfaces should have doc tests with examples for docs.rs . When adding new non-trivial pieces of logic that results in a drop in coverage you should add a test. Cross-reference with the coverage build and go to your branch. Coverage can also be run locally with cargo tarpaulin at project root. This will use our tarpaulin.toml config, and will run both unit and integration tests.","title":"When to add a test"},{"location":"contributing/#what-type-of-test","text":"Unit tests MUST NOT try to contact a Kubernetes cluster Doc tests MUST be marked as no_run when they need to contact a Kubernetes cluster Integration tests MUST NOT be used when a unit test is sufficient Integration tests MUST NOT assume existence of non-standard objects in the cluster Integration tests MUST NOT cross-depend on other unit tests completing (and installing what you need) E2E tests MUST NOT be used where an integration test is sufficient In general: use the least powerful method of testing available to you: use unit tests in kube-core use unit tests in kube-client (and in rare cases integration tests) use unit tests in kube-runtime (and occassionally integration tests) use e2e tests when testing differences between in-cluster and local configuration","title":"What type of test"},{"location":"contributing/#support","text":"","title":"Support"},{"location":"contributing/#documentation","text":"The high-level architecture document is written for contributors.","title":"Documentation"},{"location":"contributing/#contact","text":"You can ask general questions / share ideas / query the community at the kube-rs discussions forum . You can reach the maintainers of this project at #kube channel on the Tokio discord.","title":"Contact"},{"location":"getting-started/","text":"Getting Started # Installation # Select a version of kube along with the generated k8s-openapi types corresponding for your cluster version: [dependencies] kube = { version = \"0.76.0\" , features = [ \"runtime\" , \"derive\" ] } k8s-openapi = { version = \"0.16.0\" , features = [ \"v1_25\" ] } Features are available . Upgrading # Please check the CHANGELOG when upgrading. All crates herein are versioned and released together to guarantee compatibility before 1.0 . Usage # See the examples directory for how to use any of these crates. kube API Docs Official examples: version-rs : lightweight deployment reflector using axum controller-rs : Controller of a crd inside actix For real world projects see ADOPTERS . Api # The Api is what interacts with kubernetes resources, and is generic over Resource : use k8s_openapi :: api :: core :: v1 :: Pod ; let pods : Api < Pod > = Api :: default_namespaced ( client ); let p = pods . get ( \"blog\" ). await ? ; println! ( \"Got blog pod with containers: {:?}\" , p . spec . unwrap (). containers ); let patch = json ! ({ \"spec\" : { \"activeDeadlineSeconds\" : 5 }}); let pp = PatchParams :: apply ( \"kube\" ); let patched = pods . patch ( \"blog\" , & pp , & Patch :: Apply ( patch )). await ? ; assert_eq! ( patched . spec . active_deadline_seconds , Some ( 5 )); pods . delete ( \"blog\" , & DeleteParams :: default ()). await ? ; See the examples ending in _api examples for more detail. Custom Resource Definitions # Working with custom resources uses automatic code-generation via proc_macros in kube-derive . You need to #[derive(CustomResource)] and some #[kube(attrs..)] on a spec struct: #[derive(CustomResource, Debug, Serialize, Deserialize, Default, Clone, JsonSchema)] #[kube(group = \"kube.rs\" , version = \"v1\" , kind = \"Document\" , namespaced)] pub struct DocumentSpec { title : String , content : String , } Then you can use the generated wrapper struct Document as a kube::Resource : let docs : Api < Document > = Api :: default_namespaced ( client ); let d = Document :: new ( \"guide\" , DocumentSpec :: default ()); println! ( \"doc: {:?}\" , d ); println! ( \"crd: {:?}\" , serde_yaml :: to_string ( & Document :: crd ())); There are a ton of kubebuilder-like instructions that you can annotate with here. See the documentation or the crd_ prefixed examples for more. NB: #[derive(CustomResource)] requires the derive feature enabled on kube . Runtime # The runtime module exports the kube_runtime crate and contains higher level abstractions on top of the Api and Resource types so that you don't have to do all the watch / resourceVersion /storage book-keeping yourself. Watchers # A low level streaming interface (similar to informers) that presents Applied , Deleted or Restarted events. let api = Api :: < Pod > :: default_namespaced ( client ); let stream = watcher ( api , ListParams :: default ()). applied_objects (); This now gives a continual stream of events and you do not need to care about the watch having to restart, or connections dropping. while let Some ( event ) = stream . try_next (). await ? { println! ( \"Applied: {}\" , event . name ()); } NB: the plain items in a watcher stream are different from WatchEvent . If you are following along to \"see what changed\", you should flatten it with one of the utilities from WatchStreamExt , such as applied_objects . Reflectors # A reflector is a watcher with Store on K . It acts on all the Event<K> exposed by watcher to ensure that the state in the Store is as accurate as possible. let nodes : Api < Node > = Api :: all ( client ); let lp = ListParams :: default (). labels ( \"kubernetes.io/arch=amd64\" ); let ( reader , writer ) = reflector :: store (); let rf = reflector ( writer , watcher ( nodes , lp )); At this point you can listen to the reflector as if it was a watcher , but you can also query the reader at any point. Controllers # A Controller is a reflector along with an arbitrary number of watchers that schedule events internally to send events through a reconciler: Controller :: new ( root_kind_api , ListParams :: default ()) . owns ( child_kind_api , ListParams :: default ()) . run ( reconcile , error_policy , context ) . for_each ( | res | async move { match res { Ok ( o ) => info ! ( \"reconciled {:?}\" , o ), Err ( e ) => warn ! ( \"reconcile failed: {}\" , Report :: from ( e )), } }) . await ; Here reconcile and error_policy refer to functions you define. The first will be called when the root or child elements change, and the second when the reconciler returns an Err . Rustls # Kube has basic support ( with caveats ) for rustls as a replacement for the openssl dependency. To use this, turn off default features, and enable rustls-tls : [dependencies] kube = { version = \"0.76.0\" , default-features = false , features = [ \"client\" , \"rustls-tls\" ] } k8s-openapi = { version = \"0.16.0\" , features = [ \"v1_25\" ] } This will pull in rustls and hyper-rustls . musl-libc # Kube will work with distroless , scratch , and alpine (it's also possible to use alpine as a builder with some caveats ). License # Apache 2.0 licensed. See LICENSE for details.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#installation","text":"Select a version of kube along with the generated k8s-openapi types corresponding for your cluster version: [dependencies] kube = { version = \"0.76.0\" , features = [ \"runtime\" , \"derive\" ] } k8s-openapi = { version = \"0.16.0\" , features = [ \"v1_25\" ] } Features are available .","title":"Installation"},{"location":"getting-started/#upgrading","text":"Please check the CHANGELOG when upgrading. All crates herein are versioned and released together to guarantee compatibility before 1.0 .","title":"Upgrading"},{"location":"getting-started/#usage","text":"See the examples directory for how to use any of these crates. kube API Docs Official examples: version-rs : lightweight deployment reflector using axum controller-rs : Controller of a crd inside actix For real world projects see ADOPTERS .","title":"Usage"},{"location":"getting-started/#api","text":"The Api is what interacts with kubernetes resources, and is generic over Resource : use k8s_openapi :: api :: core :: v1 :: Pod ; let pods : Api < Pod > = Api :: default_namespaced ( client ); let p = pods . get ( \"blog\" ). await ? ; println! ( \"Got blog pod with containers: {:?}\" , p . spec . unwrap (). containers ); let patch = json ! ({ \"spec\" : { \"activeDeadlineSeconds\" : 5 }}); let pp = PatchParams :: apply ( \"kube\" ); let patched = pods . patch ( \"blog\" , & pp , & Patch :: Apply ( patch )). await ? ; assert_eq! ( patched . spec . active_deadline_seconds , Some ( 5 )); pods . delete ( \"blog\" , & DeleteParams :: default ()). await ? ; See the examples ending in _api examples for more detail.","title":"Api"},{"location":"getting-started/#custom-resource-definitions","text":"Working with custom resources uses automatic code-generation via proc_macros in kube-derive . You need to #[derive(CustomResource)] and some #[kube(attrs..)] on a spec struct: #[derive(CustomResource, Debug, Serialize, Deserialize, Default, Clone, JsonSchema)] #[kube(group = \"kube.rs\" , version = \"v1\" , kind = \"Document\" , namespaced)] pub struct DocumentSpec { title : String , content : String , } Then you can use the generated wrapper struct Document as a kube::Resource : let docs : Api < Document > = Api :: default_namespaced ( client ); let d = Document :: new ( \"guide\" , DocumentSpec :: default ()); println! ( \"doc: {:?}\" , d ); println! ( \"crd: {:?}\" , serde_yaml :: to_string ( & Document :: crd ())); There are a ton of kubebuilder-like instructions that you can annotate with here. See the documentation or the crd_ prefixed examples for more. NB: #[derive(CustomResource)] requires the derive feature enabled on kube .","title":"Custom Resource Definitions"},{"location":"getting-started/#runtime","text":"The runtime module exports the kube_runtime crate and contains higher level abstractions on top of the Api and Resource types so that you don't have to do all the watch / resourceVersion /storage book-keeping yourself.","title":"Runtime"},{"location":"getting-started/#watchers","text":"A low level streaming interface (similar to informers) that presents Applied , Deleted or Restarted events. let api = Api :: < Pod > :: default_namespaced ( client ); let stream = watcher ( api , ListParams :: default ()). applied_objects (); This now gives a continual stream of events and you do not need to care about the watch having to restart, or connections dropping. while let Some ( event ) = stream . try_next (). await ? { println! ( \"Applied: {}\" , event . name ()); } NB: the plain items in a watcher stream are different from WatchEvent . If you are following along to \"see what changed\", you should flatten it with one of the utilities from WatchStreamExt , such as applied_objects .","title":"Watchers"},{"location":"getting-started/#reflectors","text":"A reflector is a watcher with Store on K . It acts on all the Event<K> exposed by watcher to ensure that the state in the Store is as accurate as possible. let nodes : Api < Node > = Api :: all ( client ); let lp = ListParams :: default (). labels ( \"kubernetes.io/arch=amd64\" ); let ( reader , writer ) = reflector :: store (); let rf = reflector ( writer , watcher ( nodes , lp )); At this point you can listen to the reflector as if it was a watcher , but you can also query the reader at any point.","title":"Reflectors"},{"location":"getting-started/#controllers","text":"A Controller is a reflector along with an arbitrary number of watchers that schedule events internally to send events through a reconciler: Controller :: new ( root_kind_api , ListParams :: default ()) . owns ( child_kind_api , ListParams :: default ()) . run ( reconcile , error_policy , context ) . for_each ( | res | async move { match res { Ok ( o ) => info ! ( \"reconciled {:?}\" , o ), Err ( e ) => warn ! ( \"reconcile failed: {}\" , Report :: from ( e )), } }) . await ; Here reconcile and error_policy refer to functions you define. The first will be called when the root or child elements change, and the second when the reconciler returns an Err .","title":"Controllers"},{"location":"getting-started/#rustls","text":"Kube has basic support ( with caveats ) for rustls as a replacement for the openssl dependency. To use this, turn off default features, and enable rustls-tls : [dependencies] kube = { version = \"0.76.0\" , default-features = false , features = [ \"client\" , \"rustls-tls\" ] } k8s-openapi = { version = \"0.16.0\" , features = [ \"v1_25\" ] } This will pull in rustls and hyper-rustls .","title":"Rustls"},{"location":"getting-started/#musl-libc","text":"Kube will work with distroless , scratch , and alpine (it's also possible to use alpine as a builder with some caveats ).","title":"musl-libc"},{"location":"getting-started/#license","text":"Apache 2.0 licensed. See LICENSE for details.","title":"License"},{"location":"governance/","text":"Governance # This document defines project governance for Kube-rs. Contributors # Kube-rs is for everyone. Anyone can become a Kube-rs contributor simply by contributing to the project, whether through code, documentation, blog posts, community management, or other means. As with all Kube-rs community members, contributors are expected to follow the Kube-rs Code of Conduct . All contributions to Kube-rs code, documentation, or other components in the Kube-rs GitHub org must follow the guidelines in CONTRIBUTING.md . Whether these contributions are merged into the project is the prerogative of the maintainers. Maintainer Expectations # Maintainers have the ability to merge code into the project. Anyone can become a Kube-rs maintainer (see \"Becoming a maintainer\" below.) As such, there are certain expectations for maintainers. Kube-rs maintainers are expected to: Review pull requests, triage issues, and fix bugs in their areas of expertise, ensuring that all changes go through the project's code review and integration processes. Monitor the Kube-rs Discord, and Discussions and help out when possible. Rapidly respond to any time-sensitive security release processes. Participate on discussions on the roadmap. If a maintainer is no longer interested in or cannot perform the duties listed above, they should move themselves to emeritus status. If necessary, this can also occur through the decision-making process outlined below. Maintainer decision-making # Ideally, all project decisions are resolved by maintainer consensus. If this is not possible, maintainers may call a vote. The voting process is a simple majority in which each maintainer receives one vote. Special Tasks # In addition to the outlined abilities and responsibilities outlined above, some maintainers take on additional tasks and responsibilities. Release Tasks # As a maintainer on the release team, you are expected to be cut releases . In particular: Cut releases, and update the CHANGELOG Pre-verify big releases against example repos Publish and update versions in example repos Verify the release Becoming a maintainer # Anyone can become a Kube-rs maintainer. Maintainers should be highly proficient in Rust; have relevant domain expertise; have the time and ability to meet the maintainer expectations above; and demonstrate the ability to work with the existing maintainers and project processes. To become a maintainer, start by expressing interest to existing maintainers. Existing maintainers will then ask you to demonstrate the qualifications above by contributing PRs, doing code reviews, and other such tasks under their guidance. After several months of working together, maintainers will decide whether to grant maintainer status.","title":"Governance"},{"location":"governance/#governance","text":"This document defines project governance for Kube-rs.","title":"Governance"},{"location":"governance/#contributors","text":"Kube-rs is for everyone. Anyone can become a Kube-rs contributor simply by contributing to the project, whether through code, documentation, blog posts, community management, or other means. As with all Kube-rs community members, contributors are expected to follow the Kube-rs Code of Conduct . All contributions to Kube-rs code, documentation, or other components in the Kube-rs GitHub org must follow the guidelines in CONTRIBUTING.md . Whether these contributions are merged into the project is the prerogative of the maintainers.","title":"Contributors"},{"location":"governance/#maintainer-expectations","text":"Maintainers have the ability to merge code into the project. Anyone can become a Kube-rs maintainer (see \"Becoming a maintainer\" below.) As such, there are certain expectations for maintainers. Kube-rs maintainers are expected to: Review pull requests, triage issues, and fix bugs in their areas of expertise, ensuring that all changes go through the project's code review and integration processes. Monitor the Kube-rs Discord, and Discussions and help out when possible. Rapidly respond to any time-sensitive security release processes. Participate on discussions on the roadmap. If a maintainer is no longer interested in or cannot perform the duties listed above, they should move themselves to emeritus status. If necessary, this can also occur through the decision-making process outlined below.","title":"Maintainer Expectations"},{"location":"governance/#maintainer-decision-making","text":"Ideally, all project decisions are resolved by maintainer consensus. If this is not possible, maintainers may call a vote. The voting process is a simple majority in which each maintainer receives one vote.","title":"Maintainer decision-making"},{"location":"governance/#special-tasks","text":"In addition to the outlined abilities and responsibilities outlined above, some maintainers take on additional tasks and responsibilities.","title":"Special Tasks"},{"location":"governance/#release-tasks","text":"As a maintainer on the release team, you are expected to be cut releases . In particular: Cut releases, and update the CHANGELOG Pre-verify big releases against example repos Publish and update versions in example repos Verify the release","title":"Release Tasks"},{"location":"governance/#becoming-a-maintainer","text":"Anyone can become a Kube-rs maintainer. Maintainers should be highly proficient in Rust; have relevant domain expertise; have the time and ability to meet the maintainer expectations above; and demonstrate the ability to work with the existing maintainers and project processes. To become a maintainer, start by expressing interest to existing maintainers. Existing maintainers will then ask you to demonstrate the qualifications above by contributing PRs, doing code reviews, and other such tasks under their guidance. After several months of working together, maintainers will decide whether to grant maintainer status.","title":"Becoming a maintainer"},{"location":"integrations/","text":"Integrations #","title":"Integrations"},{"location":"integrations/#integrations","text":"","title":"Integrations"},{"location":"kubernetes-version/","text":"Compatibility # Our Kubernetes version compatibility is similar to the strategy employed by client-go and can interoperate well under a wide range of target Kubernetes versions defined by a soft minimum ( MK8SV ) and the current latest available Kubernetes feature version. kube version MK8SV Latest Generated Source 0.48.0 1.15 1.20 k8s-openapi@0.11.0 0.57.0 1.16 1.21 k8s-openapi@0.12.0 0.66.0 1.17 1.22 k8s-openapi@0.13.0 0.67.0 1.18 1.23 k8s-openapi@0.14.0 0.73.0 1.19 1.24 k8s-openapi@0.15.0 0.75.0 1.20 1.25 k8s-openapi@0.16.0 The MK8SV is listed in our README as a badge: The minimum indicates the lower bound of our testing range, and the latest is the maximum Kubernetes version selectable as a target version. Minimum Kubernetes Version Policy The Minimum Supported Kubernetes Version ( MK8SV ) is set as 5 releases below the latest Kubernetes version. This policy is intended to match stable channel support within major cloud providers . Compare with: EKS , AKS , GKE , upstream Kubernetes . Picking Versions # Given a kube versions, you must pick a target Kubernetes version from the available ones in the generated source that is used by that kube version. E.g. if using kube@0.73.0 , we see its generated source is k8s-openapi@0.15.0 , which exports the following version features . You can find the latest supported from this feature list and pick this as your target. In this case the latest supported version feature is v1_24 . By default; you SHOULD pick the latest as your target version even when running against older clusters. The exception is if you are programming explicitly against apis that have been removed in newer versions. With k8s-pb , we plan on doing this automatically . See below for details on a skew between your cluster and your target version. Version Skew # How kube version skew interacts with clusters is largely determined by how Kubernetes deprecates api versions upstream . Consider the following outcomes when picking target versions based on your cluster version : if target version == cluster version (cluster in sync with kube), then: kube has api parity with cluster Rust structs are all queryable via kube if target version > cluster version (cluster behind kube), then: kube has more recent api features than the cluster supports recent Rust api structs might not work with the cluster version yet deprecated/alpha apis might have been removed from Rust structs \u26a1 if target version < cluster version (cluster ahead of kube), then: kube has less recent api features than the cluster supports recent Kubernetes resources might not have Rust struct counterparts deprecated/alpha apis might have been removed from the cluster \u26a1 Kubernetes takes a long time to remove deprecated apis (unless they alpha or beta apis), so the acceptable distance from your cluster version actually depends on what apis you target . In particular, when using your own custom or stable official api resources - where exceeding the range will have little impact . If you are targeting deprecated/alpha apis on the other hand, then you should pick a target version in sync with your cluster . Note that alpha apis may vanish or change significantly in a single release, and is not covered by any guarantees. As a result; relying on alpha apis will make the amount of upgrades required to an application more frequent . To alleviate this; consider using api discovery to match on available api versions rather than writing code against each Kubernetes version. Outside The Range # We recommend developers stay within the supported version range for the best experience, but it is technically possible to operate outside the bounds of this range (by picking older features from k8s-openapi , or by running against older clusters). Untested Version Combinations While exceeding the supported version range is likely to work for most api resources: we do not test kube's functionality outside this version range . In minor skews, kube and Kubernetes will share a large functioning API surface, while relying on deprecated apis to fill the gap. However, the further you stray from the range you are increasingly likely to encounter Rust structs that doesn't work against your cluster, or miss support for resources entirely. Special Abstractions # For a small number of api resources, kube provides abstractions that are not managed along with the generated sources. For these cases we currently track the source and remove when Kubernetes removes them. This only affects a small number of special resources such as CustomResourceDefinition , Event , Lease , AdmissionReview . Example # The CustomResourceDefinition resource at v1beta1 was removed in Kubernetes 1.22 : The apiextensions.k8s.io/v1beta1 API version of CustomResourceDefinition is no longer served as of v1.22. Their replacement; in v1 was released in Kubernetes 1.16 . Kube had special support for both versions of CustomResourceDefinition from 0.26.0 up until 0.72.0 when kube supported structs from Kubernetes >= 1.22. This special support took the form of the proc macro CustomResource and associated helpers that allowing pinning the crd version to v1beta1 up until its removal. It is now v1 only.","title":"Kubernetes version"},{"location":"kubernetes-version/#compatibility","text":"Our Kubernetes version compatibility is similar to the strategy employed by client-go and can interoperate well under a wide range of target Kubernetes versions defined by a soft minimum ( MK8SV ) and the current latest available Kubernetes feature version. kube version MK8SV Latest Generated Source 0.48.0 1.15 1.20 k8s-openapi@0.11.0 0.57.0 1.16 1.21 k8s-openapi@0.12.0 0.66.0 1.17 1.22 k8s-openapi@0.13.0 0.67.0 1.18 1.23 k8s-openapi@0.14.0 0.73.0 1.19 1.24 k8s-openapi@0.15.0 0.75.0 1.20 1.25 k8s-openapi@0.16.0 The MK8SV is listed in our README as a badge: The minimum indicates the lower bound of our testing range, and the latest is the maximum Kubernetes version selectable as a target version. Minimum Kubernetes Version Policy The Minimum Supported Kubernetes Version ( MK8SV ) is set as 5 releases below the latest Kubernetes version. This policy is intended to match stable channel support within major cloud providers . Compare with: EKS , AKS , GKE , upstream Kubernetes .","title":"Compatibility"},{"location":"kubernetes-version/#picking-versions","text":"Given a kube versions, you must pick a target Kubernetes version from the available ones in the generated source that is used by that kube version. E.g. if using kube@0.73.0 , we see its generated source is k8s-openapi@0.15.0 , which exports the following version features . You can find the latest supported from this feature list and pick this as your target. In this case the latest supported version feature is v1_24 . By default; you SHOULD pick the latest as your target version even when running against older clusters. The exception is if you are programming explicitly against apis that have been removed in newer versions. With k8s-pb , we plan on doing this automatically . See below for details on a skew between your cluster and your target version.","title":"Picking Versions"},{"location":"kubernetes-version/#version-skew","text":"How kube version skew interacts with clusters is largely determined by how Kubernetes deprecates api versions upstream . Consider the following outcomes when picking target versions based on your cluster version : if target version == cluster version (cluster in sync with kube), then: kube has api parity with cluster Rust structs are all queryable via kube if target version > cluster version (cluster behind kube), then: kube has more recent api features than the cluster supports recent Rust api structs might not work with the cluster version yet deprecated/alpha apis might have been removed from Rust structs \u26a1 if target version < cluster version (cluster ahead of kube), then: kube has less recent api features than the cluster supports recent Kubernetes resources might not have Rust struct counterparts deprecated/alpha apis might have been removed from the cluster \u26a1 Kubernetes takes a long time to remove deprecated apis (unless they alpha or beta apis), so the acceptable distance from your cluster version actually depends on what apis you target . In particular, when using your own custom or stable official api resources - where exceeding the range will have little impact . If you are targeting deprecated/alpha apis on the other hand, then you should pick a target version in sync with your cluster . Note that alpha apis may vanish or change significantly in a single release, and is not covered by any guarantees. As a result; relying on alpha apis will make the amount of upgrades required to an application more frequent . To alleviate this; consider using api discovery to match on available api versions rather than writing code against each Kubernetes version.","title":"Version Skew"},{"location":"kubernetes-version/#outside-the-range","text":"We recommend developers stay within the supported version range for the best experience, but it is technically possible to operate outside the bounds of this range (by picking older features from k8s-openapi , or by running against older clusters). Untested Version Combinations While exceeding the supported version range is likely to work for most api resources: we do not test kube's functionality outside this version range . In minor skews, kube and Kubernetes will share a large functioning API surface, while relying on deprecated apis to fill the gap. However, the further you stray from the range you are increasingly likely to encounter Rust structs that doesn't work against your cluster, or miss support for resources entirely.","title":"Outside The Range"},{"location":"kubernetes-version/#special-abstractions","text":"For a small number of api resources, kube provides abstractions that are not managed along with the generated sources. For these cases we currently track the source and remove when Kubernetes removes them. This only affects a small number of special resources such as CustomResourceDefinition , Event , Lease , AdmissionReview .","title":"Special Abstractions"},{"location":"kubernetes-version/#example","text":"The CustomResourceDefinition resource at v1beta1 was removed in Kubernetes 1.22 : The apiextensions.k8s.io/v1beta1 API version of CustomResourceDefinition is no longer served as of v1.22. Their replacement; in v1 was released in Kubernetes 1.16 . Kube had special support for both versions of CustomResourceDefinition from 0.26.0 up until 0.72.0 when kube supported structs from Kubernetes >= 1.22. This special support took the form of the proc macro CustomResource and associated helpers that allowing pinning the crd version to v1beta1 up until its removal. It is now v1 only.","title":"Example"},{"location":"maintainers/","text":"Maintainers # The Kube-rs maintainers are: Eirik Albrigtsen sszynrae@gmail.com @clux Teo Klestrup R\u00f6ijezon teo@nullable.se @teozkr Kaz Yoshihara kazk.dev@gmail.com @kazk Emeriti # Former maintainers include: Ryan Levick ryan.levick@gmail.com @rylev","title":"Maintainers"},{"location":"maintainers/#maintainers","text":"The Kube-rs maintainers are: Eirik Albrigtsen sszynrae@gmail.com @clux Teo Klestrup R\u00f6ijezon teo@nullable.se @teozkr Kaz Yoshihara kazk.dev@gmail.com @kazk","title":"Maintainers"},{"location":"maintainers/#emeriti","text":"Former maintainers include: Ryan Levick ryan.levick@gmail.com @rylev","title":"Emeriti"},{"location":"quick-tutorial/","text":"Quick Tutorial #","title":"Quick Tutorial"},{"location":"quick-tutorial/#quick-tutorial","text":"","title":"Quick Tutorial"},{"location":"release-process/","text":"Release Process # The release process for all the crates in kube is briefly outlined in release.toml . Versioning # We currently release all crates with the same version. The crates are thus version-locked and new version in certain sub-crates does not necessarily guarantee changes to that crate. Our changelog considers changes to the facade crate kube as the highest importance. The crates are published in reverse order of importance, releasing the final facade crate kube last, so users who depend on this do not notice any version-mismatches during releases. Cadence # We currently have no fixed cadence , but we still try to release roughly once a month , or whenever important PRs are merged (whichever is earliest). For maintainers: Cutting Releases # Cutting releases is a task for the maintenance team ( contributing ) and requires developer tools installed. The process is automated where possible, and the non-writing bits usually only take a few minutes, whereas the management of documentation and release resources require a bit of manual oversight. Preliminary Steps # Close the current ongoing milestone , and ensure the prs merged since the last version are included in the milestone. Ensure the PRs in the milestone all have exactly one changelog-* label to ensure the release notes are generated correctly (we follow Keep a Changelog with the setup as outlined in #754 ). Publishing Crates # Start the process by publishing to crates.io (crate-by-crate) locally with the latest stable rust toolchain installed and active: PUBLISH_GRACE_SLEEP = 20 cargo release minor --execute once this completes, double check that all the crates are correctly published to crates.io: kube-core kube-derive kube-client kube-runtime kube This will enqueue a documentation build in docs.rs to complete. Docs build usually completes in less than 30m , but we have seen it take around half a day when publishing during an anticipated rust release. Generating the release # Once the crates have been published, we can start the process for creating a GitHub Release . If you just published, you will have at least one commit unpushed locally. You can push and tag this in one go using it: ./scripts/release-post.sh This creates a tag, and a draft release using our release workflow . The resulting github release will show up on kube-rs/releases immediately. However, we should not publish this until the enqueued documentation build in docs.rs completes. We use this wait-time to fully prepare the release, and write the manual release header: Editing the draft # At this point we can edit the draft release. Click the edit release pencil icon, and start editing. You will notice auto-generated notes already present in the textarea along with new contributors - please leave these lines intact ! Check if any of the PRs in the release contain any notices or are particularly noteworthy. We strongly advocate for highlighting some or more of the following, as part of the manually written header: big features big fixes contributor recognition interface changes A release is more than just a git tag , it should be something to celebrate , for the maintainers, the contributors, and the community. See the appendix below for ideas. Of course, not every release is going to be noteworthy. For these cases, it's perfectly OK to just hit Publish without much ceremony. Completion Steps # Create a new milestone for current minor version + 1 Press Publish on the release once docs.rs build completes Run ./scripts/release-afterdoc.sh to port the changed release notes into the CHANGELOG.md and push Run ./sync.sh in the website repo to port the new release notes onto the website Appendix # Header Formatting Tips # Some example release notes from recent history has some ideas: 0.68.0 0.66.0 Note that headers should link to PRs/important documents, but it is not necessary to link into the release or the milestone in this document yourself (the afterdoc step automates this). For breaking changes; consider including migration code samples for users if it provides an easier way to understand the changes. Fenced code blocks with diff language are easy to scan: -async fn reconcile(myobj: MyK, ctx: Arc<Data>) -> Result<ReconcilerAction> +async fn reconcile(myobj: Arc<MyK>, ctx: Arc<Data>) -> Result<ReconcilerAction> New features should link to the new additions under docs.rs/kube once the documentation build completes. ...","title":"Release Process"},{"location":"release-process/#release-process","text":"The release process for all the crates in kube is briefly outlined in release.toml .","title":"Release Process"},{"location":"release-process/#versioning","text":"We currently release all crates with the same version. The crates are thus version-locked and new version in certain sub-crates does not necessarily guarantee changes to that crate. Our changelog considers changes to the facade crate kube as the highest importance. The crates are published in reverse order of importance, releasing the final facade crate kube last, so users who depend on this do not notice any version-mismatches during releases.","title":"Versioning"},{"location":"release-process/#cadence","text":"We currently have no fixed cadence , but we still try to release roughly once a month , or whenever important PRs are merged (whichever is earliest).","title":"Cadence"},{"location":"release-process/#for-maintainers-cutting-releases","text":"Cutting releases is a task for the maintenance team ( contributing ) and requires developer tools installed. The process is automated where possible, and the non-writing bits usually only take a few minutes, whereas the management of documentation and release resources require a bit of manual oversight.","title":"For maintainers: Cutting Releases"},{"location":"release-process/#preliminary-steps","text":"Close the current ongoing milestone , and ensure the prs merged since the last version are included in the milestone. Ensure the PRs in the milestone all have exactly one changelog-* label to ensure the release notes are generated correctly (we follow Keep a Changelog with the setup as outlined in #754 ).","title":"Preliminary Steps"},{"location":"release-process/#publishing-crates","text":"Start the process by publishing to crates.io (crate-by-crate) locally with the latest stable rust toolchain installed and active: PUBLISH_GRACE_SLEEP = 20 cargo release minor --execute once this completes, double check that all the crates are correctly published to crates.io: kube-core kube-derive kube-client kube-runtime kube This will enqueue a documentation build in docs.rs to complete. Docs build usually completes in less than 30m , but we have seen it take around half a day when publishing during an anticipated rust release.","title":"Publishing Crates"},{"location":"release-process/#generating-the-release","text":"Once the crates have been published, we can start the process for creating a GitHub Release . If you just published, you will have at least one commit unpushed locally. You can push and tag this in one go using it: ./scripts/release-post.sh This creates a tag, and a draft release using our release workflow . The resulting github release will show up on kube-rs/releases immediately. However, we should not publish this until the enqueued documentation build in docs.rs completes. We use this wait-time to fully prepare the release, and write the manual release header:","title":"Generating the release"},{"location":"release-process/#editing-the-draft","text":"At this point we can edit the draft release. Click the edit release pencil icon, and start editing. You will notice auto-generated notes already present in the textarea along with new contributors - please leave these lines intact ! Check if any of the PRs in the release contain any notices or are particularly noteworthy. We strongly advocate for highlighting some or more of the following, as part of the manually written header: big features big fixes contributor recognition interface changes A release is more than just a git tag , it should be something to celebrate , for the maintainers, the contributors, and the community. See the appendix below for ideas. Of course, not every release is going to be noteworthy. For these cases, it's perfectly OK to just hit Publish without much ceremony.","title":"Editing the draft"},{"location":"release-process/#completion-steps","text":"Create a new milestone for current minor version + 1 Press Publish on the release once docs.rs build completes Run ./scripts/release-afterdoc.sh to port the changed release notes into the CHANGELOG.md and push Run ./sync.sh in the website repo to port the new release notes onto the website","title":"Completion Steps"},{"location":"release-process/#appendix","text":"","title":"Appendix"},{"location":"release-process/#header-formatting-tips","text":"Some example release notes from recent history has some ideas: 0.68.0 0.66.0 Note that headers should link to PRs/important documents, but it is not necessary to link into the release or the milestone in this document yourself (the afterdoc step automates this). For breaking changes; consider including migration code samples for users if it provides an easier way to understand the changes. Fenced code blocks with diff language are easy to scan: -async fn reconcile(myobj: MyK, ctx: Arc<Data>) -> Result<ReconcilerAction> +async fn reconcile(myobj: Arc<MyK>, ctx: Arc<Data>) -> Result<ReconcilerAction> New features should link to the new additions under docs.rs/kube once the documentation build completes. ...","title":"Header Formatting Tips"},{"location":"rust-version/","text":"Rust Version # Builds on CI always run against the most stable rust version, but we also support stable versions down to a specified Minimum Supported Rust Version ( MSRV ). Minimum Supported Rust Version # The MSRV is shown in the main Cargo.toml and as a readme badge : Our MSRV policy is to try to let our MSRV trail 2 stable versions behind the latest stable as a convenience to users downstream. Best Effort Policy Note that this policy, while sometimes more lenient than 2 versions, it is also not guaranteed to hold due to dependencies we have to upgrade. The version shown should be taken as best effort and descriptive , and it is verified as buildable by CI. For maintainers: Bumping the MSRV # Bumping the MSRV is done either as: a response to a PR that brings in a dependency with a higher MSRV an explicit choice to get new rust features Performing the change requires developer tools , and is done by running just bump-msrv 1.60.0 to bump the all the Cargo.toml files: -rust-version = \"1.56.0\" +rust-version = \"1.60.0\" as well as the badge plus devcontainer. If the bump is sufficient, CI will verify that the specified MSRV works with the current dependencies and feature usage before it can be merged. NB : An MSRV change must have the changelog-change label so that it is sufficiently highlighted in our changelog . For contributors: Nightly tooling # Due to some limitations in stable rustdoc and rustfmt , we use the nightly toolchain for auto-formatting and documentation. NB : This is contributor quirk only. All crates will always build with the stable toolchain. CI always runs documentation and formatting builds against the latest nightly , but in general, the relevant features change very infrequently, so it's generally safe to rely on older nightly builds unless CI complains. rustup update nightly","title":"Rust Version"},{"location":"rust-version/#rust-version","text":"Builds on CI always run against the most stable rust version, but we also support stable versions down to a specified Minimum Supported Rust Version ( MSRV ).","title":"Rust Version"},{"location":"rust-version/#minimum-supported-rust-version","text":"The MSRV is shown in the main Cargo.toml and as a readme badge : Our MSRV policy is to try to let our MSRV trail 2 stable versions behind the latest stable as a convenience to users downstream. Best Effort Policy Note that this policy, while sometimes more lenient than 2 versions, it is also not guaranteed to hold due to dependencies we have to upgrade. The version shown should be taken as best effort and descriptive , and it is verified as buildable by CI.","title":"Minimum Supported Rust Version"},{"location":"rust-version/#for-maintainers-bumping-the-msrv","text":"Bumping the MSRV is done either as: a response to a PR that brings in a dependency with a higher MSRV an explicit choice to get new rust features Performing the change requires developer tools , and is done by running just bump-msrv 1.60.0 to bump the all the Cargo.toml files: -rust-version = \"1.56.0\" +rust-version = \"1.60.0\" as well as the badge plus devcontainer. If the bump is sufficient, CI will verify that the specified MSRV works with the current dependencies and feature usage before it can be merged. NB : An MSRV change must have the changelog-change label so that it is sufficiently highlighted in our changelog .","title":"For maintainers: Bumping the MSRV"},{"location":"rust-version/#for-contributors-nightly-tooling","text":"Due to some limitations in stable rustdoc and rustfmt , we use the nightly toolchain for auto-formatting and documentation. NB : This is contributor quirk only. All crates will always build with the stable toolchain. CI always runs documentation and formatting builds against the latest nightly , but in general, the relevant features change very infrequently, so it's generally safe to rely on older nightly builds unless CI complains. rustup update nightly","title":"For contributors: Nightly tooling"},{"location":"security/","text":"Security Policy # Supported Versions # We provide security updates for the two most recent minor versions released on crates.io . For example, if 0.70.1 is the most recent stable version, we will address security updates for 0.69 and later. Once 0.71.1 is released, we will no longer provide updates for 0.69 releases. Reporting a Vulnerability # To report a security problem in Kube-rs, please contact at least two maintainers . These people will help diagnose the severity of the issue and determine how to address the issue. Issues deemed to be non-critical will be filed as GitHub issues. Critical issues will receive immediate attention and be fixed as quickly as possible. Security Advisories # When serious security problems in Kube-rs are discovered and corrected, we issue a security advisory, describing the problem and containing a pointer to the fix. These are announced the RustSec Advisory Database , to our github issues under the label critical , as well as discord and other primary communication channels. Security issues are fixed as soon as possible, and the fixes are propagated to the stable branches as fast as possible. However, when a vulnerability is found during a code audit, or when several other issues are likely to be spotted and fixed in the near future, the security team may delay the release of a Security Advisory, so that one unique, comprehensive Security Advisory covering several vulnerabilities can be issued. Communication with vendors and other distributions shipping the same code may also cause these delays.","title":"Security"},{"location":"security/#security-policy","text":"","title":"Security Policy"},{"location":"security/#supported-versions","text":"We provide security updates for the two most recent minor versions released on crates.io . For example, if 0.70.1 is the most recent stable version, we will address security updates for 0.69 and later. Once 0.71.1 is released, we will no longer provide updates for 0.69 releases.","title":"Supported Versions"},{"location":"security/#reporting-a-vulnerability","text":"To report a security problem in Kube-rs, please contact at least two maintainers . These people will help diagnose the severity of the issue and determine how to address the issue. Issues deemed to be non-critical will be filed as GitHub issues. Critical issues will receive immediate attention and be fixed as quickly as possible.","title":"Reporting a Vulnerability"},{"location":"security/#security-advisories","text":"When serious security problems in Kube-rs are discovered and corrected, we issue a security advisory, describing the problem and containing a pointer to the fix. These are announced the RustSec Advisory Database , to our github issues under the label critical , as well as discord and other primary communication channels. Security issues are fixed as soon as possible, and the fixes are propagated to the stable branches as fast as possible. However, when a vulnerability is found during a code audit, or when several other issues are likely to be spotted and fixed in the near future, the security team may delay the release of a Security Advisory, so that one unique, comprehensive Security Advisory covering several vulnerabilities can be issued. Communication with vendors and other distributions shipping the same code may also cause these delays.","title":"Security Advisories"},{"location":"tools/","text":"Tools # All repositories under kube-rs are buildable using FLOSS tools, and they are listed herein. User Dependencies # Dependencies a user needs to use kube-rs. Rust various crates satisfying our license allowlist Development Dependencies # Dependencies a developer might find helpful to develop on kube-rs. Build Dependencies # CLIs that are used for occasional build or release time manipulation. Maintainers need these. fd jq just sd rg fastmod curl cargo-release GNU tools like grep + head + tail + awk + sed , are also referenced a handful of times, but are generally avoided due to more modern tools above. CI Dependencies # cargo-audit cargo-deny cargo-msrv cargo-tarpaulin Integration Tests # CLIs that are used in integration tests, or referenced as ways recommended to test locally. k3d tilt docker/cli","title":"Tools"},{"location":"tools/#tools","text":"All repositories under kube-rs are buildable using FLOSS tools, and they are listed herein.","title":"Tools"},{"location":"tools/#user-dependencies","text":"Dependencies a user needs to use kube-rs. Rust various crates satisfying our license allowlist","title":"User Dependencies"},{"location":"tools/#development-dependencies","text":"Dependencies a developer might find helpful to develop on kube-rs.","title":"Development Dependencies"},{"location":"tools/#build-dependencies","text":"CLIs that are used for occasional build or release time manipulation. Maintainers need these. fd jq just sd rg fastmod curl cargo-release GNU tools like grep + head + tail + awk + sed , are also referenced a handful of times, but are generally avoided due to more modern tools above.","title":"Build Dependencies"},{"location":"tools/#ci-dependencies","text":"cargo-audit cargo-deny cargo-msrv cargo-tarpaulin","title":"CI Dependencies"},{"location":"tools/#integration-tests","text":"CLIs that are used in integration tests, or referenced as ways recommended to test locally. k3d tilt docker/cli","title":"Integration Tests"},{"location":"website/","text":"Website # This website is hosted on kube.rs via github pages from the kube-rs/website repo, and accepts contributions in the form of pull requests to change the markdown files that generate the website. Structure # The docs folder contains all the resources that's inlined on the webpage and can be edited on this page using any editor. It is recommended having markdown preview, wikilink, and the foam extension, but what is rendered is ultimately just generated from markdown with wikilinks. Synchronization # A subset of markdown documents show up in certain paths of the github contribution process and must remain in these original repos. Synchronized markdown documents will be overwritten if edited herein! Notice the first line of these files contain a line like the following: <!--GENERATED FROM https://github.com/kube-rs/kube-rs/blob/master/CONTRIBUTING.md CHANGES MUST BE MADE THERE --> These files must be edited upstream at the given path, and will be synchronized to this site on the next kube release or sooner.","title":"Website"},{"location":"website/#website","text":"This website is hosted on kube.rs via github pages from the kube-rs/website repo, and accepts contributions in the form of pull requests to change the markdown files that generate the website.","title":"Website"},{"location":"website/#structure","text":"The docs folder contains all the resources that's inlined on the webpage and can be edited on this page using any editor. It is recommended having markdown preview, wikilink, and the foam extension, but what is rendered is ultimately just generated from markdown with wikilinks.","title":"Structure"},{"location":"website/#synchronization","text":"A subset of markdown documents show up in certain paths of the github contribution process and must remain in these original repos. Synchronized markdown documents will be overwritten if edited herein! Notice the first line of these files contain a line like the following: <!--GENERATED FROM https://github.com/kube-rs/kube-rs/blob/master/CONTRIBUTING.md CHANGES MUST BE MADE THERE --> These files must be edited upstream at the given path, and will be synchronized to this site on the next kube release or sooner.","title":"Synchronization"},{"location":"controllers/admission/","text":"Admission WIP # admission into Kubernetes .","title":"Admission WIP"},{"location":"controllers/admission/#admission-wip","text":"admission into Kubernetes .","title":"Admission WIP"},{"location":"controllers/application/","text":"The Application # The application starts the Controller and links it up with the reconciler for your object . Goal # This document shows the basics of creating a simple controller with a Pod as the main object . Requirements # We will assume that you have latest stable rust installed, along with cargo-edit : Project Setup # cargo new --bin ctrl cd ctrl add then install kube , k8s-openapi , thiserror , futures , and tokio using cargo-edit : cargo add kube --features = runtime,client,derive cargo add k8s-openapi --features = v1_23 cargo add thiserror cargo add tokio --features = macros,rt-multi-thread cargo add futures This will populate some [dependencies] in your Cargo.toml file. Main Dependencies # The kube dependency is what we provide. It's used here with its controller runtime feature, its Kubernetes client and the derive macro for custom resources. The k8s-openapi dependency is needed if using core Kubernetes resources. The thiserror dependency is used in this guide as an easy way to do basic error handling, but it is optional. The futures dependency provides helpful abstractions when working with asynchronous rust. The tokio runtime dependency is needed to use async rust features, and is the supported way to use futures created by kube. Alternate async runtimes We depend on tokio for its time , signal and sync features, and while it is in theory possible to swap out a runtime, you would be sacrificing the most actively supported and most advanced runtime available. Avoid going down this alternate path unless you have a good reason. Additional dependencies are useful, but we will go through these later as we add more features. Setting up errors # We will start with the right thing from the start and define a proper Error enum: #[derive(thiserror::Error, Debug)] pub enum Error {} pub type Result < T , E = Error > = std :: result :: Result < T , E > ; Define the object # Import the object that you want to control into your main.rs . For the purposes of this demo we are going to use Pod (hence the explicit k8s-openapi dependency): use k8s_openapi :: api :: core :: v1 :: Pod ; Seting up the controller # This is where we will start defining our main and glue everything together: #[tokio::main] async fn main () -> Result < (), kube :: Error > { let client = Client :: try_default (). await ? ; let pods = Api :: < Pod > :: all ( client ); Controller :: new ( pods . clone (), Default :: default ()) . run ( reconcile , error_policy , Arc :: new (())) . for_each ( | _ | futures :: future :: ready (())) . await ; Ok (()) } This creates a Client , a Pod Api object (for all namespaces), and a Controller for the full list of pods defined by a default ListParams . We are not using relations here, so we merely tell the controller to call reconcile when a pod changes. Creating the reconciler # You need to define at least a basic reconcile fn async fn reconcile ( obj : Arc < Pod > , ctx : Arc < () > ) -> Result < Action > { println! ( \"reconcile request: {}\" , obj . name ()); Ok ( Action :: requeue ( Duration :: from_secs ( 3600 ))) } and a basic error handler (for what to do when reconcile returns an Err ): fn error_policy ( _error : & Error , _ctx : Arc < () > ) -> Action { Action :: requeue ( Duration :: from_secs ( 5 )) } To make this reconciler useful, we can reuse the one created in the reconciler document, on a custom object . Checkpoint # If you copy-pasted everything above, and fixed imports, you should have a src/main.rs in your ctrl directory with this: use std :: { sync :: Arc , time :: Duration }; use futures :: StreamExt ; use k8s_openapi :: api :: core :: v1 :: Pod ; use kube :: { Api , Client , ResourceExt , runtime :: controller :: { Action , Controller } }; #[derive(thiserror::Error, Debug)] pub enum Error {} pub type Result < T , E = Error > = std :: result :: Result < T , E > ; #[tokio::main] async fn main () -> Result < (), kube :: Error > { let client = Client :: try_default (). await ? ; let pods = Api :: < Pod > :: all ( client ); Controller :: new ( pods . clone (), Default :: default ()) . run ( reconcile , error_policy , Arc :: new (())) . for_each ( | _ | futures :: future :: ready (())) . await ; Ok (()) } async fn reconcile ( obj : Arc < Pod > , ctx : Arc < () > ) -> Result < Action > { println! ( \"reconcile request: {}\" , obj . name ()); Ok ( Action :: requeue ( Duration :: from_secs ( 3600 ))) } fn error_policy ( _error : & Error , _ctx : Arc < () > ) -> Action { Action :: requeue ( Duration :: from_secs ( 5 )) } Developing # At this point, you are ready start the app and see if it works. I.e. you need Kubernetes. Prerequisites # If you already have a cluster, skip this part. We will develop locally against a k3d cluster (which requires docker and kubectl ). Install the latest k3d release , then run: k3d cluster create kube --servers 1 --agents 1 --registry-create kube If you can run kubectl get nodes after this, you are good to go. See k3d/quick-start for help. Local Development # In your ctrl directory, you can now cargo run and check that you can successfully connect to your cluster. You should see an output like the following: reconcile request: helm-install-traefik-pxnnd reconcile request: helm-install-traefik-crd-8z56p reconcile request: traefik-97b44b794-wj5ql reconcile request: svclb-traefik-5gmsm reconcile request: coredns-7448499f4d-72rvq reconcile request: metrics-server-86cbb8457f-8fct5 reconcile request: local-path-provisioner-5ff76fc89d-4x86w reconcile request: svclb-traefik-q8zkw I.e. you should get a reconcile request for every pod in your cluster ( kubectl get pods --all ). If you now edit a pod (via kubectl edit pod traefik-xxx and make a change), or create a new pod, you should immediately get a reconcile request. Congratulations . You have just built your first kube controller. \ud83c\udf89 Continuation At this point, you have gotten the 3 main components; an object , a reconciler and an application , but there are many topics we have not touched on. Follow the links to other pages to learn more. Deploying # Containerising # WIP . Showcase both multi-stage rust build and musl builds into distroless. Containerised Development # WIP . Showcase a basic tilt setup with k3d . Continuous Integration # WIP . In separate document showcase a caching CI setup, and best practice builds; clippy/rustfmt/deny/audit. Extras # TODO: link completed WIP documents here. Adding observability # Want to add tracing , metrics or just get better logs than println , see the observability document. Useful Dependencies # The following dependencies are already used transitively within kube that may be of use to you. Use of these will generally not inflate your total build times due to already being present in the tree: tracing futures k8s-openapi serde serde_json serde_yaml tower tower-http hyper These in turn also pull in their own dependencies (and tls features, depending on your tls stack), consult cargo-tree for help minimizing your dependency tree.","title":"The Application"},{"location":"controllers/application/#the-application","text":"The application starts the Controller and links it up with the reconciler for your object .","title":"The Application"},{"location":"controllers/application/#goal","text":"This document shows the basics of creating a simple controller with a Pod as the main object .","title":"Goal"},{"location":"controllers/application/#requirements","text":"We will assume that you have latest stable rust installed, along with cargo-edit :","title":"Requirements"},{"location":"controllers/application/#project-setup","text":"cargo new --bin ctrl cd ctrl add then install kube , k8s-openapi , thiserror , futures , and tokio using cargo-edit : cargo add kube --features = runtime,client,derive cargo add k8s-openapi --features = v1_23 cargo add thiserror cargo add tokio --features = macros,rt-multi-thread cargo add futures This will populate some [dependencies] in your Cargo.toml file.","title":"Project Setup"},{"location":"controllers/application/#main-dependencies","text":"The kube dependency is what we provide. It's used here with its controller runtime feature, its Kubernetes client and the derive macro for custom resources. The k8s-openapi dependency is needed if using core Kubernetes resources. The thiserror dependency is used in this guide as an easy way to do basic error handling, but it is optional. The futures dependency provides helpful abstractions when working with asynchronous rust. The tokio runtime dependency is needed to use async rust features, and is the supported way to use futures created by kube. Alternate async runtimes We depend on tokio for its time , signal and sync features, and while it is in theory possible to swap out a runtime, you would be sacrificing the most actively supported and most advanced runtime available. Avoid going down this alternate path unless you have a good reason. Additional dependencies are useful, but we will go through these later as we add more features.","title":"Main Dependencies"},{"location":"controllers/application/#setting-up-errors","text":"We will start with the right thing from the start and define a proper Error enum: #[derive(thiserror::Error, Debug)] pub enum Error {} pub type Result < T , E = Error > = std :: result :: Result < T , E > ;","title":"Setting up errors"},{"location":"controllers/application/#define-the-object","text":"Import the object that you want to control into your main.rs . For the purposes of this demo we are going to use Pod (hence the explicit k8s-openapi dependency): use k8s_openapi :: api :: core :: v1 :: Pod ;","title":"Define the object"},{"location":"controllers/application/#seting-up-the-controller","text":"This is where we will start defining our main and glue everything together: #[tokio::main] async fn main () -> Result < (), kube :: Error > { let client = Client :: try_default (). await ? ; let pods = Api :: < Pod > :: all ( client ); Controller :: new ( pods . clone (), Default :: default ()) . run ( reconcile , error_policy , Arc :: new (())) . for_each ( | _ | futures :: future :: ready (())) . await ; Ok (()) } This creates a Client , a Pod Api object (for all namespaces), and a Controller for the full list of pods defined by a default ListParams . We are not using relations here, so we merely tell the controller to call reconcile when a pod changes.","title":"Seting up the controller"},{"location":"controllers/application/#creating-the-reconciler","text":"You need to define at least a basic reconcile fn async fn reconcile ( obj : Arc < Pod > , ctx : Arc < () > ) -> Result < Action > { println! ( \"reconcile request: {}\" , obj . name ()); Ok ( Action :: requeue ( Duration :: from_secs ( 3600 ))) } and a basic error handler (for what to do when reconcile returns an Err ): fn error_policy ( _error : & Error , _ctx : Arc < () > ) -> Action { Action :: requeue ( Duration :: from_secs ( 5 )) } To make this reconciler useful, we can reuse the one created in the reconciler document, on a custom object .","title":"Creating the reconciler"},{"location":"controllers/application/#checkpoint","text":"If you copy-pasted everything above, and fixed imports, you should have a src/main.rs in your ctrl directory with this: use std :: { sync :: Arc , time :: Duration }; use futures :: StreamExt ; use k8s_openapi :: api :: core :: v1 :: Pod ; use kube :: { Api , Client , ResourceExt , runtime :: controller :: { Action , Controller } }; #[derive(thiserror::Error, Debug)] pub enum Error {} pub type Result < T , E = Error > = std :: result :: Result < T , E > ; #[tokio::main] async fn main () -> Result < (), kube :: Error > { let client = Client :: try_default (). await ? ; let pods = Api :: < Pod > :: all ( client ); Controller :: new ( pods . clone (), Default :: default ()) . run ( reconcile , error_policy , Arc :: new (())) . for_each ( | _ | futures :: future :: ready (())) . await ; Ok (()) } async fn reconcile ( obj : Arc < Pod > , ctx : Arc < () > ) -> Result < Action > { println! ( \"reconcile request: {}\" , obj . name ()); Ok ( Action :: requeue ( Duration :: from_secs ( 3600 ))) } fn error_policy ( _error : & Error , _ctx : Arc < () > ) -> Action { Action :: requeue ( Duration :: from_secs ( 5 )) }","title":"Checkpoint"},{"location":"controllers/application/#developing","text":"At this point, you are ready start the app and see if it works. I.e. you need Kubernetes.","title":"Developing"},{"location":"controllers/application/#prerequisites","text":"If you already have a cluster, skip this part. We will develop locally against a k3d cluster (which requires docker and kubectl ). Install the latest k3d release , then run: k3d cluster create kube --servers 1 --agents 1 --registry-create kube If you can run kubectl get nodes after this, you are good to go. See k3d/quick-start for help.","title":"Prerequisites"},{"location":"controllers/application/#local-development","text":"In your ctrl directory, you can now cargo run and check that you can successfully connect to your cluster. You should see an output like the following: reconcile request: helm-install-traefik-pxnnd reconcile request: helm-install-traefik-crd-8z56p reconcile request: traefik-97b44b794-wj5ql reconcile request: svclb-traefik-5gmsm reconcile request: coredns-7448499f4d-72rvq reconcile request: metrics-server-86cbb8457f-8fct5 reconcile request: local-path-provisioner-5ff76fc89d-4x86w reconcile request: svclb-traefik-q8zkw I.e. you should get a reconcile request for every pod in your cluster ( kubectl get pods --all ). If you now edit a pod (via kubectl edit pod traefik-xxx and make a change), or create a new pod, you should immediately get a reconcile request. Congratulations . You have just built your first kube controller. \ud83c\udf89 Continuation At this point, you have gotten the 3 main components; an object , a reconciler and an application , but there are many topics we have not touched on. Follow the links to other pages to learn more.","title":"Local Development"},{"location":"controllers/application/#deploying","text":"","title":"Deploying"},{"location":"controllers/application/#containerising","text":"WIP . Showcase both multi-stage rust build and musl builds into distroless.","title":"Containerising"},{"location":"controllers/application/#containerised-development","text":"WIP . Showcase a basic tilt setup with k3d .","title":"Containerised Development"},{"location":"controllers/application/#continuous-integration","text":"WIP . In separate document showcase a caching CI setup, and best practice builds; clippy/rustfmt/deny/audit.","title":"Continuous Integration"},{"location":"controllers/application/#extras","text":"TODO: link completed WIP documents here.","title":"Extras"},{"location":"controllers/application/#adding-observability","text":"Want to add tracing , metrics or just get better logs than println , see the observability document.","title":"Adding observability"},{"location":"controllers/application/#useful-dependencies","text":"The following dependencies are already used transitively within kube that may be of use to you. Use of these will generally not inflate your total build times due to already being present in the tree: tracing futures k8s-openapi serde serde_json serde_yaml tower tower-http hyper These in turn also pull in their own dependencies (and tls features, depending on your tls stack), consult cargo-tree for help minimizing your dependency tree.","title":"Useful Dependencies"},{"location":"controllers/internals/","text":"Internals # This is a brief overview of Controller internals. Suppose you have a Controller for a main object K which owns a child object C . I.e. if child_c is an Api<C> and main_k is an Api<K> , then the following code sets up this basic scenario: Controller :: new ( main_k , ListParams :: default ()) . owns ( child_c , ListParams :: default ()) . run ( reconcile , error_policy , context ) . for_each ( | _ | futures :: future :: ready (())) . await This Controller builder sets up a series of streams and links between them: graph TD K{{Kubernetes}} -->|Event K| W(watchers) K -->|Event C| W W -->|map C -> K| I(queue) W -->|owned K| I RU -->|run| R(reconciler) A(applier) -->|poll| I A -->|schedule| S(scheduler) A -->|poll| S A -->|next| RU(runner) R -->|Update| X{{World}} R -.->|result| A subgraph \"Controller\" W I S A RU end subgraph \"Application\" Controller R end I.e. basic flow. watcher s poll the Kubernetes api for changes to configured objects ( K and C ) stream of events from each watcher needs to be turned into a stream of the same type streams of non K type run through mappers ( C maps to K through relations ) initial queue created as the union of these streams The applier then polls this stream and merges it further with previous reconciliation results (requeues for a later time). This forces the need for a scheduler . Queue # The queue is the stream of inputs. It takes N main inputs (root object, related objects, external triggers), and it is our trigger_selector . Scheduler # The scheduler wraps the object to be reconciled in a future that will resolve when its associated timer is up. All reconcile requests go through this, but only requeues are delayed significantly. Applier # The applier is the most complicated component on the diagram because it is in charge of juggling the various input streams, invoking the scheduler and runner, and also somehow produce a stream of reconcile results at the same time. The flow of doing this is quite complicated in rust so the way this is done internally is likely subject to change in the long run. Please refer to the source of the applier for more details. It is possible to set up the controller machinery yourself by creating the queue yourself from watchers and then calling the applier with the queue injected, but this is not recommended.","title":"Internals"},{"location":"controllers/internals/#internals","text":"This is a brief overview of Controller internals. Suppose you have a Controller for a main object K which owns a child object C . I.e. if child_c is an Api<C> and main_k is an Api<K> , then the following code sets up this basic scenario: Controller :: new ( main_k , ListParams :: default ()) . owns ( child_c , ListParams :: default ()) . run ( reconcile , error_policy , context ) . for_each ( | _ | futures :: future :: ready (())) . await This Controller builder sets up a series of streams and links between them: graph TD K{{Kubernetes}} -->|Event K| W(watchers) K -->|Event C| W W -->|map C -> K| I(queue) W -->|owned K| I RU -->|run| R(reconciler) A(applier) -->|poll| I A -->|schedule| S(scheduler) A -->|poll| S A -->|next| RU(runner) R -->|Update| X{{World}} R -.->|result| A subgraph \"Controller\" W I S A RU end subgraph \"Application\" Controller R end I.e. basic flow. watcher s poll the Kubernetes api for changes to configured objects ( K and C ) stream of events from each watcher needs to be turned into a stream of the same type streams of non K type run through mappers ( C maps to K through relations ) initial queue created as the union of these streams The applier then polls this stream and merges it further with previous reconciliation results (requeues for a later time). This forces the need for a scheduler .","title":"Internals"},{"location":"controllers/internals/#queue","text":"The queue is the stream of inputs. It takes N main inputs (root object, related objects, external triggers), and it is our trigger_selector .","title":"Queue"},{"location":"controllers/internals/#scheduler","text":"The scheduler wraps the object to be reconciled in a future that will resolve when its associated timer is up. All reconcile requests go through this, but only requeues are delayed significantly.","title":"Scheduler"},{"location":"controllers/internals/#applier","text":"The applier is the most complicated component on the diagram because it is in charge of juggling the various input streams, invoking the scheduler and runner, and also somehow produce a stream of reconcile results at the same time. The flow of doing this is quite complicated in rust so the way this is done internally is likely subject to change in the long run. Please refer to the source of the applier for more details. It is possible to set up the controller machinery yourself by creating the queue yourself from watchers and then calling the applier with the queue injected, but this is not recommended.","title":"Applier"},{"location":"controllers/intro/","text":"Introduction # This is a larger guide to showcase how to build controllers, and is a WIP with a progress issue . Overview # A controller a long-running program that ensures the kubernetes state of an object, matches the state of the world. As users update the desired state, the controller sees the change and schedules a reconciliation, which will update the state of the world: flowchart TD A[User/CD] -- kubectl apply object.yaml --> K[Kubernetes Api] C[Controller] -- watch objects --> K C -- schedule object --> R[Reconciler] R -- result --> C R -- update state --> K any unsuccessful reconciliations are retried or requeued, so a controller should eventually apply the desired state to the world. Writing a controller requires three pieces: an object dictating what the world should see an reconciler function that ensures the state of one object is applied to the world an application living in kubernetes watching the object and related objects The Object # The main object is the source of truth for what the world should be like, and it takes the form of a Kubernetes object like a: Pod Deployment .. any native Kubernetes Resource a partially typed or dynamically typed Kubernetes Resource an object from api discovery a Custom Resource Kubernetes already has a core controller manager for the core native objects, so the most common use-case for controller writing is a Custom Resource , but many more fine-grained use-cases exist. See the object document for how to use the various types. The Reconciler # The reconconciler is the part of the controller that ensures the world is up to date. It takes the form of an async fn taking the object along with some context, and performs the alignment between the state of world and the object . In its simplest form, this is what a reconciler (that does nothing) looks like: async fn reconcile ( object : Arc < MyObject > , data : Arc < Data > ) -> Result < Action , Error > { // TODO: logic here Ok ( Action :: requeue ( Duration :: from_secs ( 3600 / 2 ))) } As a controller writer, your job is to complete the logic that align the world with what is inside the object . The core reconciler must at minimum contain mutating api calls to what your object is meant to manage, and in some situations, handle annotations management for ownership or garbage collection . Writing a good idempotent reconciler is the most difficult part of the whole affair, and its difficulty is the reason we generally provide diagnostics and observability: See the reconciler document for more information. The Application # The controller application is the part that watches for changes, determines what root object needs reconciliations, and then schedules reconciliations for those changes. It is the glue that turns what you want into something running in Kubernetes. In this guide; the application is written in rust , using the kube crate as a dependency with the runtime feature, compiled into a container , and deployed in Kubernetes as a Deployment . The core features inside the application are: an encoding of the main object + relevant objects an infinite watch loop around relevant objects a system that maps object changes to the relevant main object an idempotent reconciler acting on a main object The system must be fault-tolerant , and thus must be able to recover from crashes , downtime , and resuming even having missed messages . Setting up a blank controller in rust satisfying these constraints is fairly simple, and can be done with minimal boilerplate (no generated files need be inlined in your project). See the application document for the high-level details. Controllers and Operators # The terminology between controllers and operators are quite similar: Kubernetes uses the following controller terminology : In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed . Each controller tries to move the current cluster state closer to the desired state. The term operator , on the other hand, was originally introduced by CoreOS as: An Operator is an application-specific controller that extends the Kubernetes API to create, configure and manage instances of complex stateful applications on behalf of a Kubernetes user. It builds upon the basic Kubernetes resource and controller concepts, but also includes domain or application-specific knowledge to automate common tasks better managed by computers. Which is further reworded now under their new agglomerate banner . They key differences between the two is that operators generally a specific type of controller, sometimes more than one in a single application. A controller would at the very least need to: manage custom resource definition(s) maintain single app focus to be classified as an operator. The term operator is a flashier term that makes the common use-case for user-written CRD controllers more understandable. If you have a CRD, you likely want to write a controller for it ( otherwise why go through the effort of making a custom resource?). Guide Focus # Our goal is that with this guide, you will learn how to use and apply the various controller patterns, so that you can avoid scaffolding out a large / complex / underutilized structure. We will focus on all the patterns as to not betray the versatility of the Kubernetes API, because components found within complex controllers can generally be mixed and matched as you see fit. We will focus on how the various element composes so you can take advantage of any controller archetypes - operators included.","title":"Introduction"},{"location":"controllers/intro/#introduction","text":"This is a larger guide to showcase how to build controllers, and is a WIP with a progress issue .","title":"Introduction"},{"location":"controllers/intro/#overview","text":"A controller a long-running program that ensures the kubernetes state of an object, matches the state of the world. As users update the desired state, the controller sees the change and schedules a reconciliation, which will update the state of the world: flowchart TD A[User/CD] -- kubectl apply object.yaml --> K[Kubernetes Api] C[Controller] -- watch objects --> K C -- schedule object --> R[Reconciler] R -- result --> C R -- update state --> K any unsuccessful reconciliations are retried or requeued, so a controller should eventually apply the desired state to the world. Writing a controller requires three pieces: an object dictating what the world should see an reconciler function that ensures the state of one object is applied to the world an application living in kubernetes watching the object and related objects","title":"Overview"},{"location":"controllers/intro/#the-object","text":"The main object is the source of truth for what the world should be like, and it takes the form of a Kubernetes object like a: Pod Deployment .. any native Kubernetes Resource a partially typed or dynamically typed Kubernetes Resource an object from api discovery a Custom Resource Kubernetes already has a core controller manager for the core native objects, so the most common use-case for controller writing is a Custom Resource , but many more fine-grained use-cases exist. See the object document for how to use the various types.","title":"The Object"},{"location":"controllers/intro/#the-reconciler","text":"The reconconciler is the part of the controller that ensures the world is up to date. It takes the form of an async fn taking the object along with some context, and performs the alignment between the state of world and the object . In its simplest form, this is what a reconciler (that does nothing) looks like: async fn reconcile ( object : Arc < MyObject > , data : Arc < Data > ) -> Result < Action , Error > { // TODO: logic here Ok ( Action :: requeue ( Duration :: from_secs ( 3600 / 2 ))) } As a controller writer, your job is to complete the logic that align the world with what is inside the object . The core reconciler must at minimum contain mutating api calls to what your object is meant to manage, and in some situations, handle annotations management for ownership or garbage collection . Writing a good idempotent reconciler is the most difficult part of the whole affair, and its difficulty is the reason we generally provide diagnostics and observability: See the reconciler document for more information.","title":"The Reconciler"},{"location":"controllers/intro/#the-application","text":"The controller application is the part that watches for changes, determines what root object needs reconciliations, and then schedules reconciliations for those changes. It is the glue that turns what you want into something running in Kubernetes. In this guide; the application is written in rust , using the kube crate as a dependency with the runtime feature, compiled into a container , and deployed in Kubernetes as a Deployment . The core features inside the application are: an encoding of the main object + relevant objects an infinite watch loop around relevant objects a system that maps object changes to the relevant main object an idempotent reconciler acting on a main object The system must be fault-tolerant , and thus must be able to recover from crashes , downtime , and resuming even having missed messages . Setting up a blank controller in rust satisfying these constraints is fairly simple, and can be done with minimal boilerplate (no generated files need be inlined in your project). See the application document for the high-level details.","title":"The Application"},{"location":"controllers/intro/#controllers-and-operators","text":"The terminology between controllers and operators are quite similar: Kubernetes uses the following controller terminology : In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed . Each controller tries to move the current cluster state closer to the desired state. The term operator , on the other hand, was originally introduced by CoreOS as: An Operator is an application-specific controller that extends the Kubernetes API to create, configure and manage instances of complex stateful applications on behalf of a Kubernetes user. It builds upon the basic Kubernetes resource and controller concepts, but also includes domain or application-specific knowledge to automate common tasks better managed by computers. Which is further reworded now under their new agglomerate banner . They key differences between the two is that operators generally a specific type of controller, sometimes more than one in a single application. A controller would at the very least need to: manage custom resource definition(s) maintain single app focus to be classified as an operator. The term operator is a flashier term that makes the common use-case for user-written CRD controllers more understandable. If you have a CRD, you likely want to write a controller for it ( otherwise why go through the effort of making a custom resource?).","title":"Controllers and Operators"},{"location":"controllers/intro/#guide-focus","text":"Our goal is that with this guide, you will learn how to use and apply the various controller patterns, so that you can avoid scaffolding out a large / complex / underutilized structure. We will focus on all the patterns as to not betray the versatility of the Kubernetes API, because components found within complex controllers can generally be mixed and matched as you see fit. We will focus on how the various element composes so you can take advantage of any controller archetypes - operators included.","title":"Guide Focus"},{"location":"controllers/object/","text":"The Object # A controller always needs a source of truth for what the world should look like , and this object always lives inside kubernetes . Depending on how the object was created/imported or performance optimization reasons, you can pick one of the following object archetypes: typed Kubernetes native resource Derived Custom Resource for Kubernetes Imported Custom Resource already in Kubernetes untyped Kubernetes resource partially typed Kubernetes resource We will outline how they interact with controllers and the basics of how to set them up. Typed Resource # This is the most common, and simplest case. Your source of truth is an existing Kubernetes object found in the openapi spec . To use a typed Kubernetes resource as a source of truth in a Controller , import it from k8s-openapi , and create an Api from it, then pass it to the Controller . use k8s_openapi :: api :: core :: v1 :: Pod ; let pods = Api :: < Pod > :: all ( client ); Controller :: new ( pods , ListParams :: default ()) This is the simplest flow and works right out of the box because the openapi implementation ensures we have all the api information via the Resource traits. If you have a native Kubernetes type, you generally want to start with k8s-openapi . If will likely do exactly what you want without further issues. That said , if both your clusters and your chosen object are large, then you can consider optimizing further by changing to a partially typed resource for smaller memory profile. A separate k8s-pb repository for our future protobuf serialization structs also exists, and while it will slot into this category and should hotswappable with k8s-openapi , it is not yet usable here. Custom Resources # Derived Custom Resource # The operator use case is heavily based on you writing your own struct, and a schema, and extending the kuberntes api with it. This has historically required a lot of boilerplate for both the api information and the (now required) schema , but this is a lot simpler with kube thanks to the CustomResource derive proc_macro . /// Our Document custom resource spec #[derive(CustomResource, Deserialize, Serialize, Clone, Debug, JsonSchema)] #[kube(kind = \"Document\" , group = \"kube.rs\" , version = \"v1\" , namespaced)] #[kube(status = \"DocumentStatus\" )] pub struct DocumentSpec { name : String , author : String , } #[derive(Deserialize, Serialize, Clone, Debug, JsonSchema)] pub struct DocumentStatus { checksum : String , last_updated : Option < DateTime < Utc >> , } This will generate a pub struct Document in this scope which implements Resource . In other words, to use it with the a controller is at this point analogous to a fully typed resource: let docs = Api :: < Document > :: all ( client ); Controller :: new ( docs , ListParams :: default ()) Custom resources require schemas Since v1 of CustomResourceDefinition became the main variant ( v1beta1 was removed in Kubernetes 1.22 ), a schema is required . These schemas are generated using schemars by specifying the JsonSchema derive. See the schemas section (TODO) for further information on advanced usage. Installation # Before Kubernetes accepts api calls for a custom resource, we need to install it. This is the usual pattern for creating the yaml definition: # Cargo.toml [[bin]] name = \"crdgen\" path = \"src/crdgen.rs\" // crdgen.rs use kube :: CustomResourceExt ; fn main () { print! ( \"{}\" , serde_yaml :: to_string ( & mylib :: Document :: crd ()). unwrap ()) } Here, a separate crdgen bin entry would install your custom resource using cargo run --bin crdgen | kubectl -f - . Installation outside the controller While it is tempting to install a custom resource within your controller at startup, this is not advisable. The permissions needed to write to the cluster-level customresourcedefinition resource is almost always much higher than what your controller needs to run. It is thus advisable to generate the yaml out-of-band, and bundle it with the rest of the controller's installation yaml. Imported Custom Resource # In the case that a customresourcedefinition already exists in your cluster, but it was implemented in another language , then we can generate structs from the schema using kopium . Suppose you want to write some extra controller or replace the native controller for PrometheusRule : curl -sSL https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml \\ | kopium -Af - > prometheusrule.rs this will read the crd from the cluster, and generate rust-optimized structs for it: use kube :: CustomResource ; use schemars :: JsonSchema ; use serde :: { Serialize , Deserialize }; use std :: collections :: BTreeMap ; use k8s_openapi :: apimachinery :: pkg :: util :: intstr :: IntOrString ; /// Specification of desired alerting rule definitions for Prometheus. #[derive(CustomResource, Serialize, Deserialize, Clone, Debug, JsonSchema)] #[kube(group = \"monitoring.coreos.com\" , version = \"v1\" , kind = \"PrometheusRule\" , plural = \"prometheusrules\" )] #[kube(namespaced)] pub struct PrometheusRuleSpec { /// Content of Prometheus rule file #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub groups : Option < Vec < PrometheusRuleGroups >> , } /// RuleGroup is a list of sequentially evaluated recording and alerting rules. #[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)] pub struct PrometheusRuleGroups { #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub interval : Option < String > , pub name : String , #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub partial_response_strategy : Option < String > , pub rules : Vec < PrometheusRuleGroupsRules > , } /// Rule describes an alerting or recording rule #[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)] pub struct PrometheusRuleGroupsRules { #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub alert : Option < String > , #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub annotations : Option < BTreeMap < String , String >> , pub expr : IntOrString , #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub r # for : Option < String > , #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub labels : Option < BTreeMap < String , String >> , #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub record : Option < String > , } you typically would then import this file as a module and use it as follows: use prometheusrule :: PrometheusRule ; let prs : Api < PrometheusRule > = Api :: default_namespaced ( client ); Controller :: new ( prs , ListParams :: default ()) Kopium is unstable Kopium is a relatively new project and it is neither feature complete nor bug free at the moment . While feedback has been very positive, and people have so far contributed fixes for several major customresources; expect some snags . Dynamic Typing # Untyped Resources # Untyped resources are using DynamicObject ; an umbrella container for arbitrary Kubernetes resources. Hard to use with controllers This type is the most unergonomic variant available. You will have to operate on untyped json to grab data out of specifications and is best suited for general (non-controller) cases where you need to look at common metadata properties from ObjectMeta like labels and annotations across different object types. The DynamicObject consists of just the unavoidable properties like apiVersion , kind , and metadata , whereas the entire spec is loaded onto an arbitrary serde_json::Value via flattening . The benefits you get is that: you avoid having to write out fields manually you can achieve tolerance against multiple versions of your object it is compatible with api discovery but you do have to find out where the object lives on the api (its ApiResource ) manually: use kube :: { api :: { Api , DynamicObject }, discovery }; // Discover most stable version variant of `documents.kube.rs` let apigroup = discovery :: group ( & client , \"kube.rs\" ). await ? ; let ( ar , caps ) = apigroup . recommended_kind ( \"Document\" ). unwrap (); // Use the discovered kind in an Api, and Controller with the ApiResource as its DynamicType let api : Api < DynamicObject > = Api :: all_with ( client , & ar ); Controller :: new_with ( api , ListParams :: default (), & ar ) Other ways of doing discovery are also available. We are highlighting recommended_kind in particular here because it can be used to achieve version agnosticity. Multiple versions of an object Kubernetes supports specifying multiple versions of a specification , and using DynamicObject above can help solve that. There are other potential ways of achieving similar results, but it does require some work. Partially-typed Resource # A very special-case setup where we specify a subset of the normal typed information, and allows tighter control over memory characteristics, and deserialization cost of the program, but at the cost of more struct code. Better methods available for improving memory characteristics Because almost all methods on Kubernetes objects such as PodSpec are wrapped in Option s, as long as unnecessary properties are unset before passing them to a reflector , similar memory reductions can be achieved. One method is to use Event::modify chained onto the watcher stream. See the pod_reflector for details. Because of these advances, the partially-typed resource pattern is not recommended. It is similar to DynamicObject (above) in that Object is another umbrella container for arbitrary Kubernetes resources, and also requires you to discover or hard-code an ApiResource for extra type information to be queriable. Here is an example of handwriting a new implementation of Pod by overriding its spec and status and placing it inside Object , then stealing its type information: use kube :: api :: { Api , ApiResource , NotUsed , Object }; // Here we replace heavy type k8s_openapi::api::core::v1::PodSpec with #[derive(Clone, Deserialize, Debug)] struct PodSpecSimple { containers : Vec < ContainerSimple > , } #[derive(Clone, Deserialize, Debug)] struct ContainerSimple { #[allow(dead_code)] image : String , } // Pod replacement type PodSimple = Object < PodSpecSimple , NotUsed > ; // steal api resource information from k8s-openapi let ar = ApiResource :: erase :: < k8s_openapi :: api :: core :: v1 :: Pod > ( & ()); Controller :: new_with ( api , ListParams :: default (), & ar ) In the end, we end up with some extra lines to define our Pod , but we also drop every field inside spec + status except spec.container.image . If your cluster has thousands of pods and you want to do some kind of common operation on a small subset of fields, then this can give a very quick win in terms of memory use (a Controller will usually maintain a Store of all owned objects). Dynamic new_with constructors # Partial or dynamic typing always needs additional type information All usage of DynamicObject or Object require the use of alternate constructors for multiple interfaces such as Api and Controller . These constructors have an additional _with suffix to carry an associated type for the Resource trait. Summary # All the fully typed methods all have a consistent usage pattern once the types have been generated. The dynamic and partial objects have more niche use cases and require a little more work such as alternate constructors. typing Source Implementation full k8s-openapi use k8s-openapi::X full kube:: CustomResource #[derive(CustomResource)] full kopium kopium > gen.rs partial kube::core:: Object partial copy-paste none kube::core:: DynamicObject write nothing","title":"The Object"},{"location":"controllers/object/#the-object","text":"A controller always needs a source of truth for what the world should look like , and this object always lives inside kubernetes . Depending on how the object was created/imported or performance optimization reasons, you can pick one of the following object archetypes: typed Kubernetes native resource Derived Custom Resource for Kubernetes Imported Custom Resource already in Kubernetes untyped Kubernetes resource partially typed Kubernetes resource We will outline how they interact with controllers and the basics of how to set them up.","title":"The Object"},{"location":"controllers/object/#typed-resource","text":"This is the most common, and simplest case. Your source of truth is an existing Kubernetes object found in the openapi spec . To use a typed Kubernetes resource as a source of truth in a Controller , import it from k8s-openapi , and create an Api from it, then pass it to the Controller . use k8s_openapi :: api :: core :: v1 :: Pod ; let pods = Api :: < Pod > :: all ( client ); Controller :: new ( pods , ListParams :: default ()) This is the simplest flow and works right out of the box because the openapi implementation ensures we have all the api information via the Resource traits. If you have a native Kubernetes type, you generally want to start with k8s-openapi . If will likely do exactly what you want without further issues. That said , if both your clusters and your chosen object are large, then you can consider optimizing further by changing to a partially typed resource for smaller memory profile. A separate k8s-pb repository for our future protobuf serialization structs also exists, and while it will slot into this category and should hotswappable with k8s-openapi , it is not yet usable here.","title":"Typed Resource"},{"location":"controllers/object/#custom-resources","text":"","title":"Custom Resources"},{"location":"controllers/object/#derived-custom-resource","text":"The operator use case is heavily based on you writing your own struct, and a schema, and extending the kuberntes api with it. This has historically required a lot of boilerplate for both the api information and the (now required) schema , but this is a lot simpler with kube thanks to the CustomResource derive proc_macro . /// Our Document custom resource spec #[derive(CustomResource, Deserialize, Serialize, Clone, Debug, JsonSchema)] #[kube(kind = \"Document\" , group = \"kube.rs\" , version = \"v1\" , namespaced)] #[kube(status = \"DocumentStatus\" )] pub struct DocumentSpec { name : String , author : String , } #[derive(Deserialize, Serialize, Clone, Debug, JsonSchema)] pub struct DocumentStatus { checksum : String , last_updated : Option < DateTime < Utc >> , } This will generate a pub struct Document in this scope which implements Resource . In other words, to use it with the a controller is at this point analogous to a fully typed resource: let docs = Api :: < Document > :: all ( client ); Controller :: new ( docs , ListParams :: default ()) Custom resources require schemas Since v1 of CustomResourceDefinition became the main variant ( v1beta1 was removed in Kubernetes 1.22 ), a schema is required . These schemas are generated using schemars by specifying the JsonSchema derive. See the schemas section (TODO) for further information on advanced usage.","title":"Derived Custom Resource"},{"location":"controllers/object/#installation","text":"Before Kubernetes accepts api calls for a custom resource, we need to install it. This is the usual pattern for creating the yaml definition: # Cargo.toml [[bin]] name = \"crdgen\" path = \"src/crdgen.rs\" // crdgen.rs use kube :: CustomResourceExt ; fn main () { print! ( \"{}\" , serde_yaml :: to_string ( & mylib :: Document :: crd ()). unwrap ()) } Here, a separate crdgen bin entry would install your custom resource using cargo run --bin crdgen | kubectl -f - . Installation outside the controller While it is tempting to install a custom resource within your controller at startup, this is not advisable. The permissions needed to write to the cluster-level customresourcedefinition resource is almost always much higher than what your controller needs to run. It is thus advisable to generate the yaml out-of-band, and bundle it with the rest of the controller's installation yaml.","title":"Installation"},{"location":"controllers/object/#imported-custom-resource","text":"In the case that a customresourcedefinition already exists in your cluster, but it was implemented in another language , then we can generate structs from the schema using kopium . Suppose you want to write some extra controller or replace the native controller for PrometheusRule : curl -sSL https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml \\ | kopium -Af - > prometheusrule.rs this will read the crd from the cluster, and generate rust-optimized structs for it: use kube :: CustomResource ; use schemars :: JsonSchema ; use serde :: { Serialize , Deserialize }; use std :: collections :: BTreeMap ; use k8s_openapi :: apimachinery :: pkg :: util :: intstr :: IntOrString ; /// Specification of desired alerting rule definitions for Prometheus. #[derive(CustomResource, Serialize, Deserialize, Clone, Debug, JsonSchema)] #[kube(group = \"monitoring.coreos.com\" , version = \"v1\" , kind = \"PrometheusRule\" , plural = \"prometheusrules\" )] #[kube(namespaced)] pub struct PrometheusRuleSpec { /// Content of Prometheus rule file #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub groups : Option < Vec < PrometheusRuleGroups >> , } /// RuleGroup is a list of sequentially evaluated recording and alerting rules. #[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)] pub struct PrometheusRuleGroups { #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub interval : Option < String > , pub name : String , #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub partial_response_strategy : Option < String > , pub rules : Vec < PrometheusRuleGroupsRules > , } /// Rule describes an alerting or recording rule #[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)] pub struct PrometheusRuleGroupsRules { #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub alert : Option < String > , #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub annotations : Option < BTreeMap < String , String >> , pub expr : IntOrString , #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub r # for : Option < String > , #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub labels : Option < BTreeMap < String , String >> , #[serde(default, skip_serializing_if = \"Option::is_none\" )] pub record : Option < String > , } you typically would then import this file as a module and use it as follows: use prometheusrule :: PrometheusRule ; let prs : Api < PrometheusRule > = Api :: default_namespaced ( client ); Controller :: new ( prs , ListParams :: default ()) Kopium is unstable Kopium is a relatively new project and it is neither feature complete nor bug free at the moment . While feedback has been very positive, and people have so far contributed fixes for several major customresources; expect some snags .","title":"Imported Custom Resource"},{"location":"controllers/object/#dynamic-typing","text":"","title":"Dynamic Typing"},{"location":"controllers/object/#untyped-resources","text":"Untyped resources are using DynamicObject ; an umbrella container for arbitrary Kubernetes resources. Hard to use with controllers This type is the most unergonomic variant available. You will have to operate on untyped json to grab data out of specifications and is best suited for general (non-controller) cases where you need to look at common metadata properties from ObjectMeta like labels and annotations across different object types. The DynamicObject consists of just the unavoidable properties like apiVersion , kind , and metadata , whereas the entire spec is loaded onto an arbitrary serde_json::Value via flattening . The benefits you get is that: you avoid having to write out fields manually you can achieve tolerance against multiple versions of your object it is compatible with api discovery but you do have to find out where the object lives on the api (its ApiResource ) manually: use kube :: { api :: { Api , DynamicObject }, discovery }; // Discover most stable version variant of `documents.kube.rs` let apigroup = discovery :: group ( & client , \"kube.rs\" ). await ? ; let ( ar , caps ) = apigroup . recommended_kind ( \"Document\" ). unwrap (); // Use the discovered kind in an Api, and Controller with the ApiResource as its DynamicType let api : Api < DynamicObject > = Api :: all_with ( client , & ar ); Controller :: new_with ( api , ListParams :: default (), & ar ) Other ways of doing discovery are also available. We are highlighting recommended_kind in particular here because it can be used to achieve version agnosticity. Multiple versions of an object Kubernetes supports specifying multiple versions of a specification , and using DynamicObject above can help solve that. There are other potential ways of achieving similar results, but it does require some work.","title":"Untyped Resources"},{"location":"controllers/object/#partially-typed-resource","text":"A very special-case setup where we specify a subset of the normal typed information, and allows tighter control over memory characteristics, and deserialization cost of the program, but at the cost of more struct code. Better methods available for improving memory characteristics Because almost all methods on Kubernetes objects such as PodSpec are wrapped in Option s, as long as unnecessary properties are unset before passing them to a reflector , similar memory reductions can be achieved. One method is to use Event::modify chained onto the watcher stream. See the pod_reflector for details. Because of these advances, the partially-typed resource pattern is not recommended. It is similar to DynamicObject (above) in that Object is another umbrella container for arbitrary Kubernetes resources, and also requires you to discover or hard-code an ApiResource for extra type information to be queriable. Here is an example of handwriting a new implementation of Pod by overriding its spec and status and placing it inside Object , then stealing its type information: use kube :: api :: { Api , ApiResource , NotUsed , Object }; // Here we replace heavy type k8s_openapi::api::core::v1::PodSpec with #[derive(Clone, Deserialize, Debug)] struct PodSpecSimple { containers : Vec < ContainerSimple > , } #[derive(Clone, Deserialize, Debug)] struct ContainerSimple { #[allow(dead_code)] image : String , } // Pod replacement type PodSimple = Object < PodSpecSimple , NotUsed > ; // steal api resource information from k8s-openapi let ar = ApiResource :: erase :: < k8s_openapi :: api :: core :: v1 :: Pod > ( & ()); Controller :: new_with ( api , ListParams :: default (), & ar ) In the end, we end up with some extra lines to define our Pod , but we also drop every field inside spec + status except spec.container.image . If your cluster has thousands of pods and you want to do some kind of common operation on a small subset of fields, then this can give a very quick win in terms of memory use (a Controller will usually maintain a Store of all owned objects).","title":"Partially-typed Resource"},{"location":"controllers/object/#dynamic-new_with-constructors","text":"Partial or dynamic typing always needs additional type information All usage of DynamicObject or Object require the use of alternate constructors for multiple interfaces such as Api and Controller . These constructors have an additional _with suffix to carry an associated type for the Resource trait.","title":"Dynamic new_with constructors"},{"location":"controllers/object/#summary","text":"All the fully typed methods all have a consistent usage pattern once the types have been generated. The dynamic and partial objects have more niche use cases and require a little more work such as alternate constructors. typing Source Implementation full k8s-openapi use k8s-openapi::X full kube:: CustomResource #[derive(CustomResource)] full kopium kopium > gen.rs partial kube::core:: Object partial copy-paste none kube::core:: DynamicObject write nothing","title":"Summary"},{"location":"controllers/observability/","text":"Observability # This document showcases common techniques for instrumentation: logs (via tracing + tracing-subscriber + EnvFilter ) traces (via tracing + tracing-subscriber + opentelemetry-otlp + tonic ) metrics (via tikv/prometheus exposed via actix-web ) and follows the approach of controller-rs . Most of this logic happens in main , before any machinery starts, so it will liberally .unwrap() . Adding Logs # We will use the tracing library for logging because it allows us reusing the same system for tracing later. cargo add tracing cargo add tracing-subscriber --features = json,env-filter We will configure this in main by creating a json log layer with an EnvFilter picking up on the common RUST_LOG environment variable: let logger = tracing_subscriber :: fmt :: layer (). json (); let env_filter = EnvFilter :: try_from_default_env () . or_else ( | _ | EnvFilter :: try_new ( \"info\" )) . unwrap (); This can be set as the global collector using: let collector = Registry :: default (). with ( logger ). with ( env_filter ); tracing :: subscriber :: set_global_default ( collector ). unwrap (); We will change how the collector is built if using tracing , but for now, this is sufficient for adding logging. Adding Traces # Following on from logging section, we add extra dependencies to let us push traces to an opentelemetry collector (sending over gRPC with tonic ): cargo add opentelemetry --features = trace,rt-tokio cargo add opentelemetry-otlp --features = tokio cargo add tonic Setting up the layer and configuring the collector follows fundamentally the same process: let telemetry = tracing_opentelemetry :: layer (). with_tracer ( init_tracer (). await ); Note 3 layers now: let collector = Registry :: default (). with ( telemetry ). with ( logger ). with ( env_filter ); tracing :: subscriber :: set_global_default ( collector ). unwrap (); However, tracing requires us to have a configurable location of where to send spans , so creating the actual tracer requires a bit more work: async fn init_tracer () -> opentelemetry :: sdk :: trace :: Tracer { let otlp_endpoint = std :: env :: var ( \"OPENTELEMETRY_ENDPOINT_URL\" ) . expect ( \"Need a otel tracing collector configured\" ); let channel = tonic :: transport :: Channel :: from_shared ( otlp_endpoint ) . unwrap () . connect () . await . unwrap (); opentelemetry_otlp :: new_pipeline () . tracing () . with_exporter ( opentelemetry_otlp :: new_exporter (). tonic (). with_channel ( channel )) . with_trace_config ( opentelemetry :: sdk :: trace :: config (). with_resource ( opentelemetry :: sdk :: Resource :: new ( vec! [ opentelemetry :: KeyValue :: new ( \"service.name\" , \"ctlr\" , // TODO: change to controller name )]), )) . install_batch ( opentelemetry :: runtime :: Tokio ) . unwrap () } Note the gRPC address (e.g. OPENTELEMETRY_ENDPOINT_URL=https://0.0.0.0:55680 ) must be explicitly wrapped in a tonic::Channel , and this forces an explicit dependency on tonic . Instrumenting # At this point, you can start adding #[instrument] attributes onto functions you want, in particular reconcile : #[instrument(skip(ctx))] async fn reconcile ( foo : Arc < Foo > , ctx : Arc < Data > ) -> Result < Action , Error > Note that the reconcile span should be the root span in the context of a controller. A reconciliation starting is the root of the chain: nothing called into the controller to reconcile an object, this happens regularly automatically. Higher levels spans Do not #[instrument] any function that creates a Controller as this would create an unintentionally wide ( application lifecycle wide ) span being a parent to all reconcile spans. Such a span will be problematic to manage. Linking Logs and Traces # To link logs and traces we take advantage that tracing data is being outputted to both logs and our tracing collector, and attach the trace_id onto our root span: #[instrument(skip(ctx), fields(trace_id))] async fn reconcile ( foo : Arc < Foo > , ctx : Arc < Data > ) -> Result < Action , Error > { let trace_id = get_trace_id (); Span :: current (). record ( \"trace_id\" , & field :: display ( & trace_id )); todo! ( \"reconcile implementation\" ) } This part is useful for Loki or other logging systems as a way to cross-link from logs to traces. Extracting the trace_id requires a helper function atm: pub fn get_trace_id () -> opentelemetry :: trace :: TraceId { // opentelemetry::Context -> opentelemetry::trace::Span use opentelemetry :: trace :: TraceContextExt as _ ; // tracing::Span -> opentelemetry::Context use tracing_opentelemetry :: OpenTelemetrySpanExt as _ ; tracing :: Span :: current () . context () . span () . span_context () . trace_id () } and it is the only reason for needing to directly add opentelemetry as a dependency. Adding Metrics # This is the hardest part of the instrumentation because it introduces the need for a webserver , along with additional complexity of choice. We will use tikv's prometheus library as its the most battle tested library available: cargo add prometheus Limitations The prometheus crate outlined herein does not support exemplars nor the openmetrics standard, at current writing. For newer features we will likely look toward the new official client , or the metrics crate suite . Registering # We will start creating a basic Metrics struct to house two metrics, a histogram and a counter: /// Metrics exposed on /metrics #[derive(Clone)] pub struct Metrics { pub reconciliations : IntCounter , pub failures : IntCounter , pub reconcile_duration : HistogramVec , } impl Metrics { fn new () -> Self { let reconcile_histogram = register_histogram_vec ! ( \"foo_controller_reconcile_duration_seconds\" , \"The duration of reconcile to complete in seconds\" , & [], vec! [ 0.01 , 0.1 , 0.25 , 0.5 , 1. , 5. , 15. , 60. ] ) . unwrap (); Metrics { reconciliations : register_int_counter ! ( \"foo_controller_reconciliations_total\" , \"reconciliations\" ). unwrap (), failures : register_int_counter ! ( \"foo_controller_reconciliation_errors_total\" , \"reconciliation errors\" ). unwrap (), reconcile_duration : reconcile_histogram , } } } and as these metrics are measurable entirely from within reconcile or error_policy we can attach the struct to the context passed to the reconciler##using-context . Measuring # Measuring our metric values can then be done by extracting the metrics struct from the context and doing the necessary computation inside reconcile : async fn reconcile ( foo : Arc < Foo > , ctx : Arc < Data > ) -> Result < Action , Error > { ctx . metrics . reconciliations . inc (); // Start a timer let start = Instant :: now (); // ... // DO RECONCILE WORK HERE // ... // Measure time taken at the end and update counter let duration = start . elapsed (). as_millis () as f64 / 1000.0 ; ctx . metrics . reconcile_duration . with_label_values ( & []) . observe ( duration ); Ok ( .. .) // end of fn } and you can increment your failures metric inside the error_policy : fn error_policy ( error : & Error , ctx : Arc < Data > ) -> Action { warn ! ( \"reconcile failed: {:?}\" , error ); ctx . metrics . failures . inc (); Action :: requeue ( Duration :: from_secs ( 5 * 60 )) } Future exemplar work If we had exemplar support here, we could have attached our trace_id to the histogram metric to be able to cross-browse from grafana metric panels into a trace-viewer. Exposing # For prometheus to obtain our metrics, we require a web server. As per the webserver guide, we will assume actix-web . In our case, we will pass a Manager struct that contains the Metrics struct and attach it to the HttpServer in main : HttpServer :: new ( move || { App :: new () . app_data ( Data :: new ( manager . clone ())) // new state . service ( metrics ) // new endpoint }) the metrics service is the important one here, and its implementation is able to extract the Metrics struct from actix's web::Data : #[get( \"/metrics\" )] async fn metrics ( c : web :: Data < Manager > , _req : HttpRequest ) -> impl Responder { let metrics = c . metrics (); // grab out of actix data let encoder = TextEncoder :: new (); let mut buffer = vec! []; encoder . encode ( & metrics , & mut buffer ). unwrap (); HttpResponse :: Ok (). body ( buffer ) } What Metrics # The included metrics failures , reconciliations and a reconcile_duration histogram will be sufficient to have prometheus compute a wide array of details: reconcile amounts in last hour - sum(increase(reconciliations[1h])) hourly error rates - sum(rate(failures[1h]) / sum(rate(reconciliations[1h])) success rates - same rate setup but reconciliations / (reconciliations + failures) p90 reconcile duration - histogram_quantile(0.9, sum(rate(reconciliations[1h]))) and you could then create alerts on aberrant values (e.g. say 10% error rate, zero reconciliation rate, and maybe p90 durations >30s). The above metric setup should comprise the core need of a standard controller (although you may have more things to care about than our simple example). You will also want resource utilization metrics, but this is typically handled upstream. E.g. cpu/memory utilization metrics are generally available via kubelet's metrics and other utilization metrics can be gathered from node_exporter . tokio-metrics New experimental runtime metrics are also availble for the tokio runtime via tokio-metrics .","title":"Observability"},{"location":"controllers/observability/#observability","text":"This document showcases common techniques for instrumentation: logs (via tracing + tracing-subscriber + EnvFilter ) traces (via tracing + tracing-subscriber + opentelemetry-otlp + tonic ) metrics (via tikv/prometheus exposed via actix-web ) and follows the approach of controller-rs . Most of this logic happens in main , before any machinery starts, so it will liberally .unwrap() .","title":"Observability"},{"location":"controllers/observability/#adding-logs","text":"We will use the tracing library for logging because it allows us reusing the same system for tracing later. cargo add tracing cargo add tracing-subscriber --features = json,env-filter We will configure this in main by creating a json log layer with an EnvFilter picking up on the common RUST_LOG environment variable: let logger = tracing_subscriber :: fmt :: layer (). json (); let env_filter = EnvFilter :: try_from_default_env () . or_else ( | _ | EnvFilter :: try_new ( \"info\" )) . unwrap (); This can be set as the global collector using: let collector = Registry :: default (). with ( logger ). with ( env_filter ); tracing :: subscriber :: set_global_default ( collector ). unwrap (); We will change how the collector is built if using tracing , but for now, this is sufficient for adding logging.","title":"Adding Logs"},{"location":"controllers/observability/#adding-traces","text":"Following on from logging section, we add extra dependencies to let us push traces to an opentelemetry collector (sending over gRPC with tonic ): cargo add opentelemetry --features = trace,rt-tokio cargo add opentelemetry-otlp --features = tokio cargo add tonic Setting up the layer and configuring the collector follows fundamentally the same process: let telemetry = tracing_opentelemetry :: layer (). with_tracer ( init_tracer (). await ); Note 3 layers now: let collector = Registry :: default (). with ( telemetry ). with ( logger ). with ( env_filter ); tracing :: subscriber :: set_global_default ( collector ). unwrap (); However, tracing requires us to have a configurable location of where to send spans , so creating the actual tracer requires a bit more work: async fn init_tracer () -> opentelemetry :: sdk :: trace :: Tracer { let otlp_endpoint = std :: env :: var ( \"OPENTELEMETRY_ENDPOINT_URL\" ) . expect ( \"Need a otel tracing collector configured\" ); let channel = tonic :: transport :: Channel :: from_shared ( otlp_endpoint ) . unwrap () . connect () . await . unwrap (); opentelemetry_otlp :: new_pipeline () . tracing () . with_exporter ( opentelemetry_otlp :: new_exporter (). tonic (). with_channel ( channel )) . with_trace_config ( opentelemetry :: sdk :: trace :: config (). with_resource ( opentelemetry :: sdk :: Resource :: new ( vec! [ opentelemetry :: KeyValue :: new ( \"service.name\" , \"ctlr\" , // TODO: change to controller name )]), )) . install_batch ( opentelemetry :: runtime :: Tokio ) . unwrap () } Note the gRPC address (e.g. OPENTELEMETRY_ENDPOINT_URL=https://0.0.0.0:55680 ) must be explicitly wrapped in a tonic::Channel , and this forces an explicit dependency on tonic .","title":"Adding Traces"},{"location":"controllers/observability/#instrumenting","text":"At this point, you can start adding #[instrument] attributes onto functions you want, in particular reconcile : #[instrument(skip(ctx))] async fn reconcile ( foo : Arc < Foo > , ctx : Arc < Data > ) -> Result < Action , Error > Note that the reconcile span should be the root span in the context of a controller. A reconciliation starting is the root of the chain: nothing called into the controller to reconcile an object, this happens regularly automatically. Higher levels spans Do not #[instrument] any function that creates a Controller as this would create an unintentionally wide ( application lifecycle wide ) span being a parent to all reconcile spans. Such a span will be problematic to manage.","title":"Instrumenting"},{"location":"controllers/observability/#linking-logs-and-traces","text":"To link logs and traces we take advantage that tracing data is being outputted to both logs and our tracing collector, and attach the trace_id onto our root span: #[instrument(skip(ctx), fields(trace_id))] async fn reconcile ( foo : Arc < Foo > , ctx : Arc < Data > ) -> Result < Action , Error > { let trace_id = get_trace_id (); Span :: current (). record ( \"trace_id\" , & field :: display ( & trace_id )); todo! ( \"reconcile implementation\" ) } This part is useful for Loki or other logging systems as a way to cross-link from logs to traces. Extracting the trace_id requires a helper function atm: pub fn get_trace_id () -> opentelemetry :: trace :: TraceId { // opentelemetry::Context -> opentelemetry::trace::Span use opentelemetry :: trace :: TraceContextExt as _ ; // tracing::Span -> opentelemetry::Context use tracing_opentelemetry :: OpenTelemetrySpanExt as _ ; tracing :: Span :: current () . context () . span () . span_context () . trace_id () } and it is the only reason for needing to directly add opentelemetry as a dependency.","title":"Linking Logs and Traces"},{"location":"controllers/observability/#adding-metrics","text":"This is the hardest part of the instrumentation because it introduces the need for a webserver , along with additional complexity of choice. We will use tikv's prometheus library as its the most battle tested library available: cargo add prometheus Limitations The prometheus crate outlined herein does not support exemplars nor the openmetrics standard, at current writing. For newer features we will likely look toward the new official client , or the metrics crate suite .","title":"Adding Metrics"},{"location":"controllers/observability/#registering","text":"We will start creating a basic Metrics struct to house two metrics, a histogram and a counter: /// Metrics exposed on /metrics #[derive(Clone)] pub struct Metrics { pub reconciliations : IntCounter , pub failures : IntCounter , pub reconcile_duration : HistogramVec , } impl Metrics { fn new () -> Self { let reconcile_histogram = register_histogram_vec ! ( \"foo_controller_reconcile_duration_seconds\" , \"The duration of reconcile to complete in seconds\" , & [], vec! [ 0.01 , 0.1 , 0.25 , 0.5 , 1. , 5. , 15. , 60. ] ) . unwrap (); Metrics { reconciliations : register_int_counter ! ( \"foo_controller_reconciliations_total\" , \"reconciliations\" ). unwrap (), failures : register_int_counter ! ( \"foo_controller_reconciliation_errors_total\" , \"reconciliation errors\" ). unwrap (), reconcile_duration : reconcile_histogram , } } } and as these metrics are measurable entirely from within reconcile or error_policy we can attach the struct to the context passed to the reconciler##using-context .","title":"Registering"},{"location":"controllers/observability/#measuring","text":"Measuring our metric values can then be done by extracting the metrics struct from the context and doing the necessary computation inside reconcile : async fn reconcile ( foo : Arc < Foo > , ctx : Arc < Data > ) -> Result < Action , Error > { ctx . metrics . reconciliations . inc (); // Start a timer let start = Instant :: now (); // ... // DO RECONCILE WORK HERE // ... // Measure time taken at the end and update counter let duration = start . elapsed (). as_millis () as f64 / 1000.0 ; ctx . metrics . reconcile_duration . with_label_values ( & []) . observe ( duration ); Ok ( .. .) // end of fn } and you can increment your failures metric inside the error_policy : fn error_policy ( error : & Error , ctx : Arc < Data > ) -> Action { warn ! ( \"reconcile failed: {:?}\" , error ); ctx . metrics . failures . inc (); Action :: requeue ( Duration :: from_secs ( 5 * 60 )) } Future exemplar work If we had exemplar support here, we could have attached our trace_id to the histogram metric to be able to cross-browse from grafana metric panels into a trace-viewer.","title":"Measuring"},{"location":"controllers/observability/#exposing","text":"For prometheus to obtain our metrics, we require a web server. As per the webserver guide, we will assume actix-web . In our case, we will pass a Manager struct that contains the Metrics struct and attach it to the HttpServer in main : HttpServer :: new ( move || { App :: new () . app_data ( Data :: new ( manager . clone ())) // new state . service ( metrics ) // new endpoint }) the metrics service is the important one here, and its implementation is able to extract the Metrics struct from actix's web::Data : #[get( \"/metrics\" )] async fn metrics ( c : web :: Data < Manager > , _req : HttpRequest ) -> impl Responder { let metrics = c . metrics (); // grab out of actix data let encoder = TextEncoder :: new (); let mut buffer = vec! []; encoder . encode ( & metrics , & mut buffer ). unwrap (); HttpResponse :: Ok (). body ( buffer ) }","title":"Exposing"},{"location":"controllers/observability/#what-metrics","text":"The included metrics failures , reconciliations and a reconcile_duration histogram will be sufficient to have prometheus compute a wide array of details: reconcile amounts in last hour - sum(increase(reconciliations[1h])) hourly error rates - sum(rate(failures[1h]) / sum(rate(reconciliations[1h])) success rates - same rate setup but reconciliations / (reconciliations + failures) p90 reconcile duration - histogram_quantile(0.9, sum(rate(reconciliations[1h]))) and you could then create alerts on aberrant values (e.g. say 10% error rate, zero reconciliation rate, and maybe p90 durations >30s). The above metric setup should comprise the core need of a standard controller (although you may have more things to care about than our simple example). You will also want resource utilization metrics, but this is typically handled upstream. E.g. cpu/memory utilization metrics are generally available via kubelet's metrics and other utilization metrics can be gathered from node_exporter . tokio-metrics New experimental runtime metrics are also availble for the tokio runtime via tokio-metrics .","title":"What Metrics"},{"location":"controllers/reconciler/","text":"The Reconciler # The reconciler is the user-defined function in charge of reconciling the state of the world . async fn reconcile ( o : Arc < K > , ctx : Arc < T > ) -> Result < Action , Error > It is always called with the object type that you instantiate the Controller with, regardless of what auxillary objects you end up watching: graph TD K{{Kubernetes}} -->|changes| W(watchers) W -->|correlate| A(applier) A -->|run| R(reconciler) R -->|Update| X{{World}} R -.->|result| A subgraph \"Controller\" W A end subgraph \"Application\" Controller R end A Controller contains a machinery that will: watch api endpoints in Kubernetes (main object and related objects) map changes from those apis (via relations ) into your main object schedule and apply reconciliations observe the result of reconciliations to decide when to reschedule tolerate a wide class of failures We will largerly treat Controller as a black-box, but details are explored in internals and architecture . As a user of kube , you will just to have to instantiate a Controller (see application ) and define your reconcile fn. The World # The state of the world is your main Kubernetes object along with anything your reconciler touches. The World >= Kubernetes While your main object must reside within Kubernetes , it is possibly to manage/act on changes outside Kubernetes . You do not have to configure the world, as any side effect you perform implicitly becomes the world for your controller. It is, however, beneficial to specify any relations your object has with the world to ensure reconcile is correctly invoked: What triggers reconcile # The reconciler is invoked - for an instance of your object - if: that main object changed an owned object (with ownerReferences to the main object) changed a related object/api (pointing to the main object) changed the object had failed reconciliation before and was requeued earlier the object received an infrequent periodic reconcile request In other words; reconcile will be triggered periodically (infrequently), and immediately upon changes to the main object, or related objects. It is therefore beneficial to configure relations so the Controller will know what counts as a reconcile-worthy change. Typically, this is accomplished with a call to Controller::owns on any owned Api and ensuring ownerReferences are created in your reconciler. See relations for details. Reasons for reconciliation # Notice that the reason for why the reconciliation started is not included in the signature of reconcile ; you only get the object . The reason for this omission is fault-tolerance : Fault-tolerance against missed messages If your controller is down / crashed earlier, you might have missed messages . In fact, no matter how well you guard against downtime (e.g with multiple replicas, rolling upgrades, pdbs, leases), the Kubernetes watch api is not sufficiently safe to guarantee unmissed messages. It is unsafe to give you a reason for why you got a reconcile call, because it is sometimes impossible to know. We therefore have to hide this information from you, and you are forced to write a more defensive reconciler . We have to: assume nothing about why reconciliation started assume the reconciler could have failed at any point during the function check every property independently (you always start at the beginning) The type of defensive function writing described above is intended to grant a formal property called idempotency . Idempotency # A function is said to be idempotent if it can be applied multiple times without changing the result beyond the initial application. A reconciler must be idempotent If a reconciler is triggered twice for the same object, it must cause the same outcome. Care must be taken to ensure operations are not dependent on all-or-nothing approaches, and the flow of the reconciler must be able to recover from errors occurring in a previous reconcile runs. Let us create a reconciler for a custom PodManager resource that will: create an associated Pod with ownerReferences note its creation time on the status object of PodManager Both of these operations can be done in isolation in an idempotent manner (we will show below), but it is possible to compose the two operations in an erroneous way. Combining Idempotent Operations # A naive approach to the above problem might be to take a shortcut, and simply check if the work has been done , and if not, do it: if pod_missing { create_owned_pod () ? ; set_timestamp_on_owner () ? ; } Is this a good optimization? Can we avoid calling set timestamps over and over again? Unfortunately, this reasoning is flawed ; if the timestamp creation fails after the pod got created the first time, the second action will never get done! Reconciler interruptions If your reconciler errored half-way through a run; the only way you would know what failed , is if you check everything. Therefore the correct way to do these two actions it to do them independently: if pod_missing { create_owned_pod () ? ; } if is_timestamp_missing () { set_timestamp_on_owner () ? ; } In-depth Solution # Let's suppose we have access to an Api<Pod> and an Api<PodManager> (via a context ): let api : Api < PodManager > = ctx . get_ref (). api . clone (); let pods : Api < Pod > = Api :: default_namespaced ( ctx . get_ref (). client . clone ()); and let's define the Pod we want to create as: fn create_owned_pod ( source : & PodManager ) -> Pod { let oref = source . controller_owner_ref ( & ()); Pod { metadata : ObjectMeta { name : source . metadata . name . clone (), owner_references : Some ( vec! [ oref ]), .. ObjectMeta :: default () }, spec : my_pod_spec (), .. Default :: default () } } one approach of achieving idempotency is to check every property carefully:: // TODO: find using ownerReferences instead - has to be done using jsonpath... // {range .items[?(.metadata.ownerReferences.uid=262bab1a-1c79-11ea-8e23-42010a800016)]}{.metadata.name}{end} // make a helper for this? let podfilter = ListParams :: default () . labels ( format! ( \"owned-by/{}\" , obj . name ())); // if owned pod is not created, do the work to create it let pod : Pod = match & pods . list ( & podfilter ). await ? [ .. ] { [ p , .. ] => p , // return the first found pod [] => { let pod_data = create_owned_pod ( & obj ); pods . create ( pod_data ). await ? // return the new pod from the apiserver }, }; if obj . status . pod_created . is_none () { // update status object with the creation_timestamp of the owned Pod let status = json ! ({ \"status\" : PodManagerStatus { pod_created : pod . meta (). creation_timestamp } }); api . patch_status ( & obj . name (), & PatchParams :: default (), & Patch :: Merge ( & status )) . await ? ; } but we can actually simplify this significantly by taking advantage of idempotent Kubernetes apis: let pod_data = create_owned_pod ( & obj ); let pod = pods . create ( pod . name (), serversideapply , Patch :: Apply ( pod_data )). await ? // update status object with the creation_timestamp of the owned Pod let status = json ! ({ \"status\" : PodManagerStatus { pod_created : pod . meta (). creation_timestamp } }); api . patch_status ( & obj . name (), & PatchParams :: default (), & Patch :: Merge ( & status )) . await ? ; Here we are taking advantage of Server-Side Apply and deterministic naming of the owned pod to call the equivalent of kubectl apply on the pod_data . The patch_status is already idempotent, and does not technically need the pre-check. However, we might wish to keep the check, as this will lead to less networked requests. Using Context # To do anything useful inside the reconciler like persisting your changes, you typically need to inject some client in there. The way this is done is through the context parameter on Controller::run . It's whatever you want, packed in an Arc . // Context for our reconciler #[derive(Clone)] struct Data { /// kubernetes client client : Client , /// In memory state state : Arc < RwLock < State >> , } let context = Arc :: new ( Data { client : client . clone (), state : state . clone (), }); Controller :: new ( foos , ListParams :: default ()) . run ( reconcile , error_policy , context ) then you can pull out your user defined struct (here Data ) items inside reconcile : async fn reconcile ( object : Arc < MyObject > , data : Arc < Data > ) -> Result < Action , Error > { ctx . state . write (). await . last_event = Utc :: now (); let reporter = ctx . state . read (). await . reporter . clone (); let objs : Api < MyObject > = Api :: all ( ctx . client . clone ()); // ... Ok ( Action :: await_change ()) } Cleanup # Kubernetes provides two methods of cleanup of resources; the automatic ownerReferences , and the manual (but safe) finalizers . WIP . Separate document describing these. Instrumentation # The root reconcile function should be instrumented with logs, traces and metrics, and can also post diagnostic events to the Kubernetes api. See the observability document for how to add good instrumentation to your reconcile fn. Diagnostics # WIP . Separate document for posting diagnostic events to the events api + using the status object.","title":"The Reconciler"},{"location":"controllers/reconciler/#the-reconciler","text":"The reconciler is the user-defined function in charge of reconciling the state of the world . async fn reconcile ( o : Arc < K > , ctx : Arc < T > ) -> Result < Action , Error > It is always called with the object type that you instantiate the Controller with, regardless of what auxillary objects you end up watching: graph TD K{{Kubernetes}} -->|changes| W(watchers) W -->|correlate| A(applier) A -->|run| R(reconciler) R -->|Update| X{{World}} R -.->|result| A subgraph \"Controller\" W A end subgraph \"Application\" Controller R end A Controller contains a machinery that will: watch api endpoints in Kubernetes (main object and related objects) map changes from those apis (via relations ) into your main object schedule and apply reconciliations observe the result of reconciliations to decide when to reschedule tolerate a wide class of failures We will largerly treat Controller as a black-box, but details are explored in internals and architecture . As a user of kube , you will just to have to instantiate a Controller (see application ) and define your reconcile fn.","title":"The Reconciler"},{"location":"controllers/reconciler/#the-world","text":"The state of the world is your main Kubernetes object along with anything your reconciler touches. The World >= Kubernetes While your main object must reside within Kubernetes , it is possibly to manage/act on changes outside Kubernetes . You do not have to configure the world, as any side effect you perform implicitly becomes the world for your controller. It is, however, beneficial to specify any relations your object has with the world to ensure reconcile is correctly invoked:","title":"The World"},{"location":"controllers/reconciler/#what-triggers-reconcile","text":"The reconciler is invoked - for an instance of your object - if: that main object changed an owned object (with ownerReferences to the main object) changed a related object/api (pointing to the main object) changed the object had failed reconciliation before and was requeued earlier the object received an infrequent periodic reconcile request In other words; reconcile will be triggered periodically (infrequently), and immediately upon changes to the main object, or related objects. It is therefore beneficial to configure relations so the Controller will know what counts as a reconcile-worthy change. Typically, this is accomplished with a call to Controller::owns on any owned Api and ensuring ownerReferences are created in your reconciler. See relations for details.","title":"What triggers reconcile"},{"location":"controllers/reconciler/#reasons-for-reconciliation","text":"Notice that the reason for why the reconciliation started is not included in the signature of reconcile ; you only get the object . The reason for this omission is fault-tolerance : Fault-tolerance against missed messages If your controller is down / crashed earlier, you might have missed messages . In fact, no matter how well you guard against downtime (e.g with multiple replicas, rolling upgrades, pdbs, leases), the Kubernetes watch api is not sufficiently safe to guarantee unmissed messages. It is unsafe to give you a reason for why you got a reconcile call, because it is sometimes impossible to know. We therefore have to hide this information from you, and you are forced to write a more defensive reconciler . We have to: assume nothing about why reconciliation started assume the reconciler could have failed at any point during the function check every property independently (you always start at the beginning) The type of defensive function writing described above is intended to grant a formal property called idempotency .","title":"Reasons for reconciliation"},{"location":"controllers/reconciler/#idempotency","text":"A function is said to be idempotent if it can be applied multiple times without changing the result beyond the initial application. A reconciler must be idempotent If a reconciler is triggered twice for the same object, it must cause the same outcome. Care must be taken to ensure operations are not dependent on all-or-nothing approaches, and the flow of the reconciler must be able to recover from errors occurring in a previous reconcile runs. Let us create a reconciler for a custom PodManager resource that will: create an associated Pod with ownerReferences note its creation time on the status object of PodManager Both of these operations can be done in isolation in an idempotent manner (we will show below), but it is possible to compose the two operations in an erroneous way.","title":"Idempotency"},{"location":"controllers/reconciler/#combining-idempotent-operations","text":"A naive approach to the above problem might be to take a shortcut, and simply check if the work has been done , and if not, do it: if pod_missing { create_owned_pod () ? ; set_timestamp_on_owner () ? ; } Is this a good optimization? Can we avoid calling set timestamps over and over again? Unfortunately, this reasoning is flawed ; if the timestamp creation fails after the pod got created the first time, the second action will never get done! Reconciler interruptions If your reconciler errored half-way through a run; the only way you would know what failed , is if you check everything. Therefore the correct way to do these two actions it to do them independently: if pod_missing { create_owned_pod () ? ; } if is_timestamp_missing () { set_timestamp_on_owner () ? ; }","title":"Combining Idempotent Operations"},{"location":"controllers/reconciler/#in-depth-solution","text":"Let's suppose we have access to an Api<Pod> and an Api<PodManager> (via a context ): let api : Api < PodManager > = ctx . get_ref (). api . clone (); let pods : Api < Pod > = Api :: default_namespaced ( ctx . get_ref (). client . clone ()); and let's define the Pod we want to create as: fn create_owned_pod ( source : & PodManager ) -> Pod { let oref = source . controller_owner_ref ( & ()); Pod { metadata : ObjectMeta { name : source . metadata . name . clone (), owner_references : Some ( vec! [ oref ]), .. ObjectMeta :: default () }, spec : my_pod_spec (), .. Default :: default () } } one approach of achieving idempotency is to check every property carefully:: // TODO: find using ownerReferences instead - has to be done using jsonpath... // {range .items[?(.metadata.ownerReferences.uid=262bab1a-1c79-11ea-8e23-42010a800016)]}{.metadata.name}{end} // make a helper for this? let podfilter = ListParams :: default () . labels ( format! ( \"owned-by/{}\" , obj . name ())); // if owned pod is not created, do the work to create it let pod : Pod = match & pods . list ( & podfilter ). await ? [ .. ] { [ p , .. ] => p , // return the first found pod [] => { let pod_data = create_owned_pod ( & obj ); pods . create ( pod_data ). await ? // return the new pod from the apiserver }, }; if obj . status . pod_created . is_none () { // update status object with the creation_timestamp of the owned Pod let status = json ! ({ \"status\" : PodManagerStatus { pod_created : pod . meta (). creation_timestamp } }); api . patch_status ( & obj . name (), & PatchParams :: default (), & Patch :: Merge ( & status )) . await ? ; } but we can actually simplify this significantly by taking advantage of idempotent Kubernetes apis: let pod_data = create_owned_pod ( & obj ); let pod = pods . create ( pod . name (), serversideapply , Patch :: Apply ( pod_data )). await ? // update status object with the creation_timestamp of the owned Pod let status = json ! ({ \"status\" : PodManagerStatus { pod_created : pod . meta (). creation_timestamp } }); api . patch_status ( & obj . name (), & PatchParams :: default (), & Patch :: Merge ( & status )) . await ? ; Here we are taking advantage of Server-Side Apply and deterministic naming of the owned pod to call the equivalent of kubectl apply on the pod_data . The patch_status is already idempotent, and does not technically need the pre-check. However, we might wish to keep the check, as this will lead to less networked requests.","title":"In-depth Solution"},{"location":"controllers/reconciler/#using-context","text":"To do anything useful inside the reconciler like persisting your changes, you typically need to inject some client in there. The way this is done is through the context parameter on Controller::run . It's whatever you want, packed in an Arc . // Context for our reconciler #[derive(Clone)] struct Data { /// kubernetes client client : Client , /// In memory state state : Arc < RwLock < State >> , } let context = Arc :: new ( Data { client : client . clone (), state : state . clone (), }); Controller :: new ( foos , ListParams :: default ()) . run ( reconcile , error_policy , context ) then you can pull out your user defined struct (here Data ) items inside reconcile : async fn reconcile ( object : Arc < MyObject > , data : Arc < Data > ) -> Result < Action , Error > { ctx . state . write (). await . last_event = Utc :: now (); let reporter = ctx . state . read (). await . reporter . clone (); let objs : Api < MyObject > = Api :: all ( ctx . client . clone ()); // ... Ok ( Action :: await_change ()) }","title":"Using Context"},{"location":"controllers/reconciler/#cleanup","text":"Kubernetes provides two methods of cleanup of resources; the automatic ownerReferences , and the manual (but safe) finalizers . WIP . Separate document describing these.","title":"Cleanup"},{"location":"controllers/reconciler/#instrumentation","text":"The root reconcile function should be instrumented with logs, traces and metrics, and can also post diagnostic events to the Kubernetes api. See the observability document for how to add good instrumentation to your reconcile fn.","title":"Instrumentation"},{"location":"controllers/reconciler/#diagnostics","text":"WIP . Separate document for posting diagnostic events to the events api + using the status object.","title":"Diagnostics"},{"location":"controllers/relations/","text":"Related Objects # A Controller needs to specify related resources if changes to them is meant to trigger the reconciler . These relations are generally set up with Controller::owns , but we will go through the different variants below. Owned Relation # The Controller::owns relation is the most straight-forward and most ubiquitous one. One object controls the lifecycle of a child object, and cleanup happens automatically via ownerReferences . let cmgs = Api :: < ConfigMapGenerator > :: all ( client . clone ()); let cms = Api :: < ConfigMap > :: all ( client . clone ()); Controller :: new ( cmgs , ListParams :: default ()) . owns ( cms , ListParams :: default ()) This configmapgen example uses one custom resource ConfigMapGenerator whose controller is in charge of the lifecycle of the child ConfigMap . What happens if we delete a ConfigMapGenerator instance here? Well, there will be a ConfigMap with ownerReferences matching the ConfigMapGenerator so Kubernetes will automatically cleanup the associated ConfigMap . What happens if we modify the managed ConfigMap ? The Controller sees a change and associates the change with the owning ConfigMapGenerator , ultimately triggering a reconciliation of the root ConfigMapGenerator . This relation relies on ownerReferences being created on the managed/owned objects for Kubernetes automatic cleanup, and the Controller relies on it for association with its owner. Watched Relations # The Controller::watches relation is for related Kubernetes objects without ownerReferences , i.e. without a standard way for the controller to map the object to the root object. Thus, you need to define this mapper yourself: let main = Api :: < MainObj > :: all ( client ); let related = Api :: < RelatedObject > :: all ( client ); let mapper = | obj : RelatedObject | { obj . spec . object_ref . map ( | oref | { ReconcileRequest :: from ( oref ) }) }; Controller :: new ( main , ListParams :: default ()) . watches ( related , ListParams :: default (), mapper ) In this case we are extracing an object reference from the spec of our object. Regardless of how you get the information, your mapper must return an iterator of ObjectRef for the root object(s) that must be reconciled as a result of the change. As a theoretical example; every HPA object bundles a scale ref to the workload, so you could use this to build a Controller for Deployment using HPA as a watched object. External Relations # It is possible to be dependent on some external api that you have semantically linked to your cluster, either as a managed resource or a source of information. If you want to populate an external API from a custom resource, you will want to use finalizers to ensure the api gets cleaned up on CRD deletion. If you want changes to the external API to trigger reconciliations, then you need to write some custom logic. The current best way to do this is to inject reconciliation requests to the Controller using Controller::reconcile_all_on . Subsets # With owned and watched relations, it is not always necessary to watch the full space. Use ListParams to filter on the categories you want to reduce IO utilization: let myobjects = Api :: < MyObject > :: all ( client . clone ()); let pods = Api :: < Pod > :: all ( client . clone ()) Controller :: new ( myobjects , ListParams :: default ()) . owns ( pods , ListParams :: default (). labels ( \"managed-by=my-controller\" )) Summary # Depending on what type of child object and its relation with the main object , you will need the following setup and cleanup: Child Controller relation Setup Cleanup Kubernetes object Owned Controller::owns ownerReferences Kubernetes object Related Controller::watches n/a External API Managed custom finalizers External API Related custom n/a","title":"Related Objects"},{"location":"controllers/relations/#related-objects","text":"A Controller needs to specify related resources if changes to them is meant to trigger the reconciler . These relations are generally set up with Controller::owns , but we will go through the different variants below.","title":"Related Objects"},{"location":"controllers/relations/#owned-relation","text":"The Controller::owns relation is the most straight-forward and most ubiquitous one. One object controls the lifecycle of a child object, and cleanup happens automatically via ownerReferences . let cmgs = Api :: < ConfigMapGenerator > :: all ( client . clone ()); let cms = Api :: < ConfigMap > :: all ( client . clone ()); Controller :: new ( cmgs , ListParams :: default ()) . owns ( cms , ListParams :: default ()) This configmapgen example uses one custom resource ConfigMapGenerator whose controller is in charge of the lifecycle of the child ConfigMap . What happens if we delete a ConfigMapGenerator instance here? Well, there will be a ConfigMap with ownerReferences matching the ConfigMapGenerator so Kubernetes will automatically cleanup the associated ConfigMap . What happens if we modify the managed ConfigMap ? The Controller sees a change and associates the change with the owning ConfigMapGenerator , ultimately triggering a reconciliation of the root ConfigMapGenerator . This relation relies on ownerReferences being created on the managed/owned objects for Kubernetes automatic cleanup, and the Controller relies on it for association with its owner.","title":"Owned Relation"},{"location":"controllers/relations/#watched-relations","text":"The Controller::watches relation is for related Kubernetes objects without ownerReferences , i.e. without a standard way for the controller to map the object to the root object. Thus, you need to define this mapper yourself: let main = Api :: < MainObj > :: all ( client ); let related = Api :: < RelatedObject > :: all ( client ); let mapper = | obj : RelatedObject | { obj . spec . object_ref . map ( | oref | { ReconcileRequest :: from ( oref ) }) }; Controller :: new ( main , ListParams :: default ()) . watches ( related , ListParams :: default (), mapper ) In this case we are extracing an object reference from the spec of our object. Regardless of how you get the information, your mapper must return an iterator of ObjectRef for the root object(s) that must be reconciled as a result of the change. As a theoretical example; every HPA object bundles a scale ref to the workload, so you could use this to build a Controller for Deployment using HPA as a watched object.","title":"Watched Relations"},{"location":"controllers/relations/#external-relations","text":"It is possible to be dependent on some external api that you have semantically linked to your cluster, either as a managed resource or a source of information. If you want to populate an external API from a custom resource, you will want to use finalizers to ensure the api gets cleaned up on CRD deletion. If you want changes to the external API to trigger reconciliations, then you need to write some custom logic. The current best way to do this is to inject reconciliation requests to the Controller using Controller::reconcile_all_on .","title":"External Relations"},{"location":"controllers/relations/#subsets","text":"With owned and watched relations, it is not always necessary to watch the full space. Use ListParams to filter on the categories you want to reduce IO utilization: let myobjects = Api :: < MyObject > :: all ( client . clone ()); let pods = Api :: < Pod > :: all ( client . clone ()) Controller :: new ( myobjects , ListParams :: default ()) . owns ( pods , ListParams :: default (). labels ( \"managed-by=my-controller\" ))","title":"Subsets"},{"location":"controllers/relations/#summary","text":"Depending on what type of child object and its relation with the main object , you will need the following setup and cleanup: Child Controller relation Setup Cleanup Kubernetes object Owned Controller::owns ownerReferences Kubernetes object Related Controller::watches n/a External API Managed custom finalizers External API Related custom n/a","title":"Summary"},{"location":"controllers/webserver/","text":"Web Server # This is a WIP document. Actix-web # Now that we have a more stable release chain of actix-web (version 4 is out), it is easier to write guides, and will use this heavily battle tested web-framework. cargo add actix-web Heavy Weight Framework The actix-web crate is fairly heavy-weight for just exposing metrics. For a simpler web framework that we have partial support for, consider axum and our version-rs application using it. Usage # This document is unfinished so we refer to controller-rs which is a full-featured example of using actix-web with kube .","title":"Web Server"},{"location":"controllers/webserver/#web-server","text":"This is a WIP document.","title":"Web Server"},{"location":"controllers/webserver/#actix-web","text":"Now that we have a more stable release chain of actix-web (version 4 is out), it is easier to write guides, and will use this heavily battle tested web-framework. cargo add actix-web Heavy Weight Framework The actix-web crate is fairly heavy-weight for just exposing metrics. For a simpler web framework that we have partial support for, consider axum and our version-rs application using it.","title":"Actix-web"},{"location":"controllers/webserver/#usage","text":"This document is unfinished so we refer to controller-rs which is a full-featured example of using actix-web with kube .","title":"Usage"},{"location":"crates/kube-client/","text":"kube-client # kube-client is the client crate with config and client abstractions. It is re-exported from kube under the kube::client and kube::config modules. This crate has the most extensive documentation on docs.rs/kube/client docs.rs/kube/config","title":"kube-client"},{"location":"crates/kube-client/#kube-client","text":"kube-client is the client crate with config and client abstractions. It is re-exported from kube under the kube::client and kube::config modules. This crate has the most extensive documentation on docs.rs/kube/client docs.rs/kube/config","title":"kube-client"},{"location":"crates/kube-core/","text":"kube-core # kube-core is the core crate with the lowest level abstractions. It is re-exported from kube under kube::core . This crate has the most extensive documentation on docs.rs/kube/core . Contains # Core traits and types necessary for interacting with the kubernetes API. This crate is the rust counterpart to kubernetes/apimachinery .","title":"kube-core"},{"location":"crates/kube-core/#kube-core","text":"kube-core is the core crate with the lowest level abstractions. It is re-exported from kube under kube::core . This crate has the most extensive documentation on docs.rs/kube/core .","title":"kube-core"},{"location":"crates/kube-core/#contains","text":"Core traits and types necessary for interacting with the kubernetes API. This crate is the rust counterpart to kubernetes/apimachinery .","title":"Contains"},{"location":"crates/kube-derive/","text":"kube-derive # kube-derive is a procedural macro crate with helpers for managing Custom Resource Definitions. Its macros are re-exported from kube . The macros exported are heavily documented on: docs.rs/kube/CustomResource","title":"kube-derive"},{"location":"crates/kube-derive/#kube-derive","text":"kube-derive is a procedural macro crate with helpers for managing Custom Resource Definitions. Its macros are re-exported from kube . The macros exported are heavily documented on: docs.rs/kube/CustomResource","title":"kube-derive"},{"location":"crates/kube-runtime/","text":"kube-runtime # kube-runtime is the runtime crate with the highest level abstractions. It is re-exported from kube under kube::runtime . This crate has the most extensive documentation on docs.rs/kube/runtime .","title":"kube-runtime"},{"location":"crates/kube-runtime/#kube-runtime","text":"kube-runtime is the runtime crate with the highest level abstractions. It is re-exported from kube under kube::runtime . This crate has the most extensive documentation on docs.rs/kube/runtime .","title":"kube-runtime"},{"location":"crates/kube/","text":"kube # kube is the facade crate that re-exports all the other crates. This crate has the most extensive documentation on docs.rs/kube . Re-exports # kube re-exports kube-runtime under kube::runtime kube re-exports kube-core under kube::core kube re-exports kube-derive 's proc macros onto kube kube re-exports kube-client (flattened) under kube ( kube::client and kube::config )","title":"kube"},{"location":"crates/kube/#kube","text":"kube is the facade crate that re-exports all the other crates. This crate has the most extensive documentation on docs.rs/kube .","title":"kube"},{"location":"crates/kube/#re-exports","text":"kube re-exports kube-runtime under kube::runtime kube re-exports kube-core under kube::core kube re-exports kube-derive 's proc macros onto kube kube re-exports kube-client (flattened) under kube ( kube::client and kube::config )","title":"Re-exports"}]}